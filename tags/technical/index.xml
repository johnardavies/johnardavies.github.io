<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>technical on John&#39;s Site</title>
    <link>https://johnardavies.github.io/tags/technical/</link>
    <description>Recent content in technical on John&#39;s Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://johnardavies.github.io/tags/technical/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bayes 2: Bayesian inference with Markov Chain Monte Carlo</title>
      <link>https://johnardavies.github.io/technical/mcmc/</link>
      <pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/mcmc/</guid>
      <description>View from the Peggy Guggenheim Collection, Venice.
In the previous post we calculated the posterior distribution for a parameter we wanted to estimate. However, in practice, this is often not possible and numerical simulations are needed. Here we evaluate the distribution using a Markov Chain Monte Carlo (MCMC) technique.
The idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain evolves over time it has the properties of the posterior’s probability distribution.</description>
    </item>
    
    <item>
      <title>Bayes 1: Introduction to Bayesian Inference</title>
      <link>https://johnardavies.github.io/technical/bayes1/</link>
      <pubDate>Sat, 20 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/bayes1/</guid>
      <description>Ilke Sahin&amp;rsquo;s &amp;lsquo;Water Canals (Aquaducts)&amp;rsquo; at Goldsmiths degree show 2024
This note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.
1. Finding the most probable parameters given the data using Bayes’ theorem We want to find the best hypothesis or model parameter(s) that describes the data. Bayes’ theorem allows us to write down a general formula for this:</description>
    </item>
    
    <item>
      <title>Transformers 3: Building and training a Transformer</title>
      <link>https://johnardavies.github.io/technical/transformer3/</link>
      <pubDate>Sat, 18 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/transformer3/</guid>
      <description>Printworks, London
Transformers 3: Building and training a Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that translates German text into English. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&amp;rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.</description>
    </item>
    
    <item>
      <title>Cloud 6: Introduction to Infrastructure as Code using CloudFormation</title>
      <link>https://johnardavies.github.io/technical/infrastructure/</link>
      <pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/infrastructure/</guid>
      <description>Willis tower, Chicago, August 2023
Introduction to Infrastructure as Code - deploying a virtual machine In previous posts we spun up virtual machines from the consoles of cloud providers websites. However this has the disadvantage that it can be time consuming and the steps involved complicated and easy to forget. By contrast directly specifying the cloud computing that we want to access in code is more explicit and easier to reuse - it can also be version controlled.</description>
    </item>
    
    <item>
      <title>Transformers 2: The Transformer&#39;s Structure</title>
      <link>https://johnardavies.github.io/technical/transformer2/</link>
      <pubDate>Fri, 12 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/transformer2/</guid>
      <description>Helix, White City
Transformers 2: The structure of a Transformer Continuing the series of notes on Transformers post I’ll now look at the overall structure of Transformers and how attention is used within them in the context of language translation. I’ll cover:
1. The architecture of Transformers
2. How attention is included in the Transformer
The previous post discussed the attention mechanism that underlies how Transformers work.
1. The architecture of Transformers 1.</description>
    </item>
    
    <item>
      <title>Transformers 1: The Attention Mechanism</title>
      <link>https://johnardavies.github.io/technical/transformer1/</link>
      <pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/transformer1/</guid>
      <description>&amp;ldquo;Let me Unsee&amp;rdquo; by Asbestos, Belfast, August 2023.
The T in GPT In all the excitement around OpenAI’s ChatGPT two things are often not mentioned:
  the underlying architecture of Generative Pre-trained Transformer (GPT) models, the Transformer, was invented and made public by Google
  despite these models’ impressive capabilities, and their scale and complexity, the underlying mathematics involved (at least in terms of the components used) is relatively simple</description>
    </item>
    
    <item>
      <title>Cloud 5: Introduction to deploying an app with simple CI/CD</title>
      <link>https://johnardavies.github.io/technical/front_end/</link>
      <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/front_end/</guid>
      <description>Patrick Staff&amp;rsquo;s &amp;lsquo;On Venus&amp;rsquo; at the Serpentine gallery
Deploying a web app with Continuous Integration and Continuous Deployment (CI/CD) Introduction
The last post used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud.</description>
    </item>
    
    <item>
      <title>Cloud 4: Using Serverless</title>
      <link>https://johnardavies.github.io/technical/serverless/</link>
      <pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/serverless/</guid>
      <description>La Cabane Éclatée aux 4 Salles, Daniel Buren at the Gori collection
What is Serverless? In previous posts we spun up virtual machines to work in the cloud, but the cloud can also perform specific functions for us without us having to run any servers, so called serverless computing. The servers providing the services still exist, but they are managed by the cloud provider while we deal with the services directly.</description>
    </item>
    
    <item>
      <title>Cloud 3: Docker and Jupyter notebooks in the Cloud</title>
      <link>https://johnardavies.github.io/technical/docker_use/</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/docker_use/</guid>
      <description>Kings Cross underpass rotated, London
In the last post we covered some basic operations with cloud machines, in this post we discuss using Docker containers to replicate environments in the cloud and how to containerise a Jupyter notebook.
Docker has become a standard way to package applications enabling something that runs on one machine to run on another. It is a container which holds a virtualisation of an operating system within which applications can be packaged to allow them to move between machines.</description>
    </item>
    
    <item>
      <title>Cloud 2: Getting started with using a Virtual Machine in the Cloud</title>
      <link>https://johnardavies.github.io/technical/cloud_use/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/cloud_use/</guid>
      <description>Clouds over the Lea valley, London
In the last post we covered setting up a virtual machine in the cloud, logging into it via ssh and securing it. However, this gets us a virtual machine running in the cloud costing us money, it isn&amp;rsquo;t doing anything useful. To work with the virtual machine there are some other basic things that it&amp;rsquo;s helpful to be able to do.
1. Getting things on and off the cloud machine</description>
    </item>
    
    <item>
      <title>Cloud 1: Introduction to launching a Virtual Machine in the Cloud</title>
      <link>https://johnardavies.github.io/technical/cloud_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://johnardavies.github.io/technical/cloud_intro/</guid>
      <description>Es Devlin&amp;rsquo;s &amp;ldquo;BLUEWHITESKY&amp;rdquo; at the 2021 LUX exhibition
More and more of the digital services we use involve the cloud, where the computing is not done on our laptop or mobile, but accessed over the internet. However, coming across its technical details for the first time, the cloud can be slightly opaque to someone whose background is not computing - I certainly found it that way. From a data science perspective working with it involves things that are not related to statistics, machine learning or even programming narrowly defined.</description>
    </item>
    
  </channel>
</rss>
