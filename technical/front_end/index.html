<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Cloud 5: Introduction to deploying an app with simple CI/CD | John&#39;s Site</title>
<meta name="keywords" content="technical, digital, cloud" />
<meta name="description" content="Patrick Staff&rsquo;s &lsquo;On Venus&rsquo; at the Serpentine gallery
Deploying a web app with Continuous Integration and Continuous Deployment (CI/CD) Introduction
The last post used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/front_end/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Cloud 5: Introduction to deploying an app with simple CI/CD" />
<meta property="og:description" content="Patrick Staff&rsquo;s &lsquo;On Venus&rsquo; at the Serpentine gallery
Deploying a web app with Continuous Integration and Continuous Deployment (CI/CD) Introduction
The last post used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/front_end/" />
<meta property="og:image" content="https://johnardavies.github.io/on_venus.jpg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2023-02-05T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-02-05T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/on_venus.jpg" />
<meta name="twitter:title" content="Cloud 5: Introduction to deploying an app with simple CI/CD"/>
<meta name="twitter:description" content="Patrick Staff&rsquo;s &lsquo;On Venus&rsquo; at the Serpentine gallery
Deploying a web app with Continuous Integration and Continuous Deployment (CI/CD) Introduction
The last post used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Cloud 5: Introduction to deploying an app with simple CI/CD",
      "item": "https://johnardavies.github.io/technical/front_end/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Cloud 5: Introduction to deploying an app with simple CI/CD",
  "name": "Cloud 5: Introduction to deploying an app with simple CI\/CD",
  "description": "Patrick Staff\u0026rsquo;s \u0026lsquo;On Venus\u0026rsquo; at the Serpentine gallery\nDeploying a web app with Continuous Integration and Continuous Deployment (CI/CD) Introduction\nThe last post used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud.",
  "keywords": [
    "technical", "digital", "cloud"
  ],
  "articleBody": "Patrick Staff’s ‘On Venus’ at the Serpentine gallery\nDeploying a web app with Continuous Integration and Continuous Deployment (CI/CD) Introduction\nThe last post used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud. To do this we use many of the tools from previous posts combined with some new ones. I wrote this to learn more about:\nContinuous Integration (CI)\nThe process where before integration of code changes into the main project codebase automated testing is run.\nContinuous Deployment (CD)\nThe process where, once testing has happened, and the changes integrated into the main code branch, the changes are automatically released to the public product.\nAt a high level deploying the app involves:\n1. Building a Docker container image to hold the app\n2. Deploying the container to a virtual machine to make the app available\nIt is possible to do these stages manually (as the post on using Jupyter notebooks in virtual machines does, here we automate this using GitHub Actions.\nWe have a very simple form of continuous integration - when we want to merge changes on GitHub from the working branch into the app’s main branch we run a code formatter over the app code to check for any formatting errors. Once this has been done we automatically deploy the new app to the virtual machine running in the cloud (continuous deployment). The schematic below gives a high-level summary of the pipeline.\nCloud warning: The activities in this and the previous post it builds on will incur costs from running cloud services. Costs should always be monitored and services turned off to avoid incurring unwanted expense. In addition this post involves linking multiple cloud services which can potentially cause security issues. Care should be taken not to commit keys and secrets in git and to avoid giving any unnecessary access to services.\nThe following sections cover:\n1. Creating a Streamlit app that accesses the cloud database\n2. Developing a Docker container to hold the app in\n3. Producing a basic CI system that automatically formats new code changes\n4. Having a basic CD system that when the app’s code changes rebuilds the Docker container image and runs it on the virtual machine\n1. Creating a Streamlit app that accesses the cloud database The python script below generates a very simple app, which we will deploy to the cloud. It accesses the Twitter data that is stored in an Amazon Web Services (AWS) DynamoDB, runs the Vader sentiment analyser over the collected tweets and calculates the average sentiment per day. It then generates a chart of this information with ggplot and converts this to a webpage with Streamlit.\nimport boto3 import pandas as pd from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer from plotnine import * import streamlit as st # Imports the AWS key and secret to access the database. # Delete this direct import before the script is deployed See discussion later. from aws_auth import * # Gives a streamlit app title st.title(\"Twitter sentiment monitor\") st.button(\"Update\") # Connect to DynamoDB with the boto3 library # Delete the aws key and secret lines before deploying dynamodb = boto3.resource( \"dynamodb\" , aws_access_key_id=APP_KEY , aws_secret_access_key=APP_SECRET ) # Connect to the dynamodb table apicallstore dynamodb = boto3.resource(\"dynamodb\") table = dynamodb.Table(\"apicallstore\") # Read in the table and extract the tweets response = table.scan() data = response[\"Items\"] # Takes the data from DynamoDB and turn it into a data frame df = pd.DataFrame(data) # # Sentiment analysis analyser = SentimentIntensityAnalyzer() def print_sentiment_scores(sentence): try: snt = analyser.polarity_scores(sentence) return snt except: \"none\" df[\"polarity_score\"] = df[\"text\"].map(print_sentiment_scores) # Extracts the compound sentiment coefficient. This is the sum of positive, negative \u0026 neutral scores # normalised between -1 (the most negative sentiment) and +1 (most positive sentiment). df[\"compound\"] = df[\"polarity_score\"].apply(lambda x: x[\"compound\"]) df[\"date\"] = pd.to_datetime(df[\"created_at\"]).dt.date # Calculates the average daily sentiment sent_date = df[[\"date\", \"compound\"]].groupby(\"date\").mean() sent_date = sent_date.reset_index() # Create a chart of the average daily sentiment daily_average = ( ggplot(sent_date, aes(x=\"date\", y=\"compound\", group=1)) + geom_line() + ylab(\"Average sentiment\") + theme(axis_text_x=element_text(rotation=90, hjust=1)) + ylim(-1, 1) ) # Make the chart available through Streamlit and add a header st.header(\"Comparing daily average sentiment\") st.pyplot(ggplot.draw(daily_average)) In this example the serverless pipeline was set up to find tweets with the word London, so the script (which is called London_tweet_monitor.py) will calculate average daily sentiment of a sample of regularly collected tweets that in some way relate to “London”. The sentiment can range between -1 (most negative) and 1 (most positive).\nIn order for the script to run locally it needs access to the AWS key and secret credentials which give permission to read from the database (Here imported in the aws_auth.py file) and a virtual environment which has the relevant dependencies. The key and secret credentials should be obtained from an Identity Access Management (IAM) role that gives just read-only access to the data, as we want to limit the permissions to use AWS services. For instructions on how to set up a read-only IAM role for DynamodDB, see the two tutorials on this subject linked at the bottom of this post. The script can be deployed locally from the command line using the command below.\n$ streamlit run London_tweet_monitor.py This produces a simple app running locally on http://localhost:xxxx that shows the average daily sentiment (where xxxx is the port number, in this example we will use port 8009). 2. Developing a Docker container to hold the app in To ensure the app can run on another machine we create the Dockerfile below which will generate a container image to hold the app. This container image can then be deployed to a virtual machine in the cloud with Docker and run as it has all the dependencies needed. The requirements.txt that is referenced has been generated from the virtual environment used to run the app script locally.\n# app/Dockerfile # Start with a base python image FROM python:3.9-slim # Open port 8009 EXPOSE 8009 # Create a directory called app RUN mkdir app # Copy the python file for the app and the requirements file to the app folder in the container # These should be in the same folder locally with the Dockerfile COPY London_tweet_monitor.py requirements.txt /app # Switch the working directory to the app folder WORKDIR /app # Run some basic installations RUN apt-get update \u0026\u0026 apt-get install -y \\ build-essential \\ software-properties-common \\ git \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* #Install the latest version of pip and packages specified in the requirements file such as Streamlit RUN pip install --upgrade pip RUN pip3 install -r requirements.txt # Run the streamlit app ENTRYPOINT [\"streamlit\", \"run\", \"London_tweet_monitor.py\", \"--server.port=8009\", \"--server.address=0.0.0.0\"] In what follows we assume that file for the Streamlit app London_tweet_monitor.py along with the Dockerfile and requirements.txt have been committed and pushed to the main branch of the GitHub repo that has been set up for the project. Prior to deployment the lines that were used to import the credentials locally should be removed e.g. from aws_auth import *, aws_access_key_id=APP_KEY and the aws_secret_access_key=APP_SECRET. A different approach is used to obtain the credentials needed to access DynamoDB in the cloud which is covered later.\n3. Producing a basic CI system that automatically formats new code changes We use GitHub Actions to trigger various activities based on changes made in GitHub. A new action can be generated by clicking the GitHub repo’s Actions tab and then the New workflow tab. This will prompt you with a screen showing some of the workflows that are available. Here we will write a new Action so click on set up a workflow yourself. Copying the code below into the GitHub editor and saving will create a file with this text in the directory .github/workflows where GitHub Actions are defined. Here we do a basic test to the code using the Black formatter. To link this to your GitHub add your user name and associated email into the script in the relevant lines. The structure of the Action is as follows:\n name: What the Action is called. Here Black. on: The event that triggers the action. Here the event is a pull request on the main branch of the GitHub repo. jobs: The workflow comprises a series of jobs. runs-on: specifies where the Action is run. Here an ubuntu virtual machine that GitHub is providing to the Action. uses: Other Actions that the Action uses. Here:  checkout@v3 which checks out the GitHub repo so the Action can work with it. setup-python@v2 which allows the Action to use a version of python, so we can install Black and run it.   run: The commands that are run. Here a series of python and Git commands to install and run the Black formatter and then commit the changes that have been made.  name: Black on: pull_request: branches: - main jobs: format: runs-on: ubuntu-latest steps: # Download the code to the environment with checkout@v2 the reference (ref) is needed to allow the commit created by the Action to be added to the branch - uses: actions/checkout@v3 with: fetch-depth: 0 ref: ${{ github.event.pull_request.head.ref }} - uses: actions/setup-python@v2 with: python-version: 3.8 - run: | python -m pip install black # Runs black on the python files black $(git ls-files '*.py') git config --global user.email \"GitHub user email\" git config --global user.name \"GitHub user name\" # Adds and commits with a message with the -am flag. The code after || is used if first part fails git commit -am \"Format code with Black\" || echo \"No changes to commit\" # pushes it git push When working on a separate branch from the main branch and opening a pull request the Black formatter will now be applied to the code in the branch. When the pull request is created, as shown below, after a few seconds a GitHub Actions in progress symbol will appear showing that the Black formatter is running. Black will then create another commit that fixes formatting issues that it has identified so that the pull requests into the main branch are always formatted to the Black standard. Below is an example of how the GitHub Actions commit has automatically fixed the formatting errors for the chart in the python script. If you click on the Actions tab you will be able to track the Action’s operations as it runs.\n4. Having a basic CD system that when the app’s code changes rebuilds the Docker container image and runs it on the virtual machine Having the Streamlit App and the Docker file we now use GitHub Actions to take the code on GitHub and automatically deploy it. This involves when the code is changed on the main GitHub branch:\n1. building a new Docker container image for it\n2. running it on the virtual machine to make the changes available\nWe cover these in section 4.1 and section 4.2. To do this we set up a new GitHub Action with the following code. The parts labelled as Part 1 and Part 2 in the code below correspond to the different stages. Here we store the container image that the Action creates on Docker Hub (container image storage options are a standard offer from cloud providers so one could use another option such as the AWS Elastic Container Registry or the DigitalOcean Container Registry).\nname: Build and Publish Docker container on: push: branches: - main jobs: # define job to build and publish docker image build-and-push-docker-image: name: Build Docker image and push to repositories runs-on: ubuntu-latest steps: # Part 1: Rebuilding the container image on DockerHub - name: Checkout uses: actions/checkout@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v2 - name: Login to DockerHub uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build and push uses: docker/build-push-action@v4 with: context: ./ file: ./Dockerfile push: true tags: ${{secrets.DOCKERHUB_USERNAME}}/sentiment_monitor:latest # Part 2: Deploying the container image to the EC2 instance - name: executing remote ssh commands using ssh key # Action to ssh into the EC2 uses: appleboy/ssh-action@v0.1.7 with: host: ${{ secrets.HOST }} username: ${{ secrets.USERNAME }} key: ${{ secrets.KEY }} port: ${{ secrets.PORT }} script: | # Stop any running containers docker stop $(docker ps -a -q) # Delete the stopped containers docker rm $(docker ps -a -q) # Login to Docker Hub docker login -u ${{ secrets.DOCKERHUB_USERNAME }} -p ${{ secrets.DOCKERHUB_TOKEN }} # Pull down the container image from Docker Hub docker pull ${{ secrets.DOCKERHUB_USERNAME }}/sentiment_monitor:latest # Run the image in detached mode sending the container's output port 8009 to external port 8009 docker run --detach -p 8009:8009 ${{ secrets.DOCKERHUB_USERNAME }}/sentiment_monitor:latest 4.1 Updating the container on Docker Hub when the code on the main branch on GitHub changes 4.1.1 Giving GitHub access to the Docker Hub container repository\nGoing to Docker Hub we select Create repository and start a new private repository called sentiment_monitor and create a DockerHub access token. The access token can be generated by going to the Account Settings button and then the Security tab.\nWe need to use security credentials to connect all the pieces of the pipeline together e.g. to allow the GitHub Action to access Docker Hub and login to the EC2 virtual machine. We don’t want to write any of this information directly in the code as it poses a security risk and is hard to remove once committed to Git.\nFor the Docker Hub and EC2 login information we use the Secrets store of GitHub Actions. This can be found by going to the Settings tab in the project’s GitHub repo and scrolling down to the Security tab and selecting the Actions tab under Secrets and variables as shown in the image below. The secrets are added in here and are then injected through the curly brackets in the Actions script with the secrets prefix e.g. ${{ secrets.DOCKERHUB_TOKEN }} inserts the access token we have created and added as a secret called DOCKERHUB_TOKEN into the script.\nIn the secrets tab of GitHub Actions we add two pieces of information DOCKERHUB_TOKEN, DOCKERHUB_USERNAME that will allow the GitHub Action to access the Docker Hub and the repository where the container image is stored.\n4.1.2 Having the Action rebuild the container image that is available on DockerHub\nThe action code uses a succession of pre-existing actions in Part 1 to rebuild the container image on Docker Hub following a push to the main code branch on GitHub:\n actions/checkout@v3 checks out the existing code which the push has changed docker/setup-buildx-action@v2 an action that allows containers to be built docker/login-action@v2 logs into Docker Hub using the credentials that added as secrets docker/build-push-action@v4 builds and pushes the container image to Docker Hub  Now when a change is made to the main branch of the repo the GitHub Actions can log into Docker Hub and rebuild the container image on the Hub based on the latest version of the app code.\n4.2 Deploy the container from GitHub to EC2 after a change to the code running in the container After the Action has rebuilt the container image (Part 1) we now have it deploy the container to EC2 (Part 2). For simplicity we have the EC2 already running (for a guide on setting up an EC2 on AWS see this tutorial from Kostas Stathoulopoulos - in this case as the application is not computationally intensive a GPU is not needed). To allow the GitHub Action to pull down the container image from Docker Hub and for the image to be run successfully we need to enable permissions to allow 1. the GitHub Action to login to the virtual machine and 2. allow the virtual machine to read from the DynamoDB database where the data is.\n4.2.1 Giving GitHub Actions the ability to log into the virtual machine\nAn EC2 instance will have a name along the lines of ubuntu@ec2-xx-xxx-xx-xxx.region.compute.amazonaws.com (Where xxs are numbers and region is an AWS region e.g. eu-west-2). The part before the @ is the username and the part after is the host (This is the default setting for an ubuntu virtual machine, with another operating system or created user account the user name will be different. To allow us to ssh into the virtual machine from GitHub in the GitHub Actions secrets we add HOST (In this example ec2-18-170-44-111.eu-west-2.compute.amazonaws.com) and USERNAME as ubuntu. The SSH key is also copied into the GitHub secrets (here labelled KEY) to allow the Action to login (this should be the full key including all the text in the key file). The port that is used for SSH is also added as PORT (Typically it is 22). Assuming the EC2 instance is accessible using SSH keys only (without a password) the GitHub Actions will then be able to log into the EC2 instance.\n4.2.2 Giving the EC2 instance the ability to read data from the DynamoDB database\nTo give the EC2 instance access to the DynamoDB database we attach the IAM role to the instance that gives read only access to the DynamoDB database. Once the IAM role has been added to the instance, then it will automatically be able to access the credentials to read data from DynamoDB - there is no need to reference or import credentials in the script which is why they were removed previously.\n4.2.3 Deploying the container\nWhen the code has been pushed to GitHub, after having:\n Tested the branched code with the Black formatter before branches are merged into the main branch (The first Action) Updated the Docker container image on DockerHub using the changed code (Part 1 of the second Action)  Part 2 of the second action uses the pre-existing action appleboy/ssh-action@v0.1.7 which allows the Action to SSH into the EC2 using the credentials that were added. The GitHub Action will then stop the existing container which is running on the EC2 and delete it. The Action will then pull down the new container image from Docker Hub and run it. This takes a few minutes. In this example the app will be available at http://ec2-18-170-44-111.eu-west-2.compute.amazonaws.com:8009/ i.e. port 8009 of the EC2 instance it is being run on. The app will be up and running using the code in the latest container image. This could all be done manually, but is now automated with GitHub Actions. As an example if we add the following code which generates average hourly sentiment into the python file from another branch.\n# # Sentiment by hour sent_hour=df[[\"hour\", \"compound\"]].groupby([\"hour\"]).mean() sent_hour=sent_hour.reset_index() days_by_hour=ggplot(sent_hour , aes(x='hour', y='compound')) + geom_line()+ylab(\"Average sentiment\")+ylim(-1,1) st.header('Average hourly sentiment') st.pyplot(ggplot.draw(days_by_hour)) Then it will automatically be reformatted by our action running the Black formatter as:\n# # Sentiment by hour sent_hour = df[[\"hour\", \"compound\"]].groupby([\"hour\"]).mean() sent_hour = sent_hour.reset_index() days_by_hour = ( ggplot(sent_hour, aes(x=\"hour\", y=\"compound\")) + glom_line() + ylab(\"Average sentiment\") + ylim(-1, 1) ) st.header(\"Average hourly sentiment\") st.pyplot(ggplot.draw(days_by_hour)) When this change is pulled into the main branch the container image on Docker Hub will be updated, pulled down to the EC2 instance and run, and the served page updated automatically with the changes to the below. Extensions Although this example illustrates some basic CI/CD for learning purposes, it does not pretend to be fully optimised. Things to improve include:\nCaching the data As it is currently the app is reading in all the data from DynamoDB and does so each time it is reloaded. This is inefficient and will pose issues as the amount of data increases so it would probably be better to have the app cache the processed data it has loaded to date and only import the latest raw data that has been collected.\nhttps and not http\nThis currently serves the app with http and not https, converting it to https would require setting up certificates.\nManaging the images\nThe container images that have been run are all stored on the EC2 instance when they are pulled down. On a small EC2 instance, which is what it should be deployed on, this will eventually fill up the memory. If we want to retain them the container images should be moved to a lower cost storage solution with more capacity or deleted if no longer needed.\nOther cloud posts: Cloud 1: Introduction to launching a Virtual Machine in the Cloud\nCloud 2: Getting started with using a Virtual Machine in the Cloud\nCloud 3: Docker and Jupyter notebooks in the Cloud\nCloud 4: Using Serverless\nReferences Deploying Streamlit on Docker\nStreamlit doc on Docker deployment\nGiving IAM roles access to DynamoDB\nJennelle Cosby, ‘Using AWS IAM roles with EC2 and DynamoDB’\nOmar Egal, ‘How to create a DynamoDB table and grant AWS EC2 Read-Only Access’\nLaunching EC2\nKostas Stathoulopoulos, ‘How to set up a GPU instance for machine learning on AWS’\nCI/CD and Docker\nDocker Docs, Introduction to GitHub Actions\nMinho Jang, ‘CI/CD Hands-On : Github Actions+DockerHub+AWS EC2’\nTruong Huu Thao, ‘Full CI/CD with Docker + GitHub Actions + DigitalOcean’\nQuestion on deployment Using GitHub Actions on Digital Ocean\n",
  "wordCount" : "3502",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/on_venus.jpg","datePublished": "2023-02-05T00:00:00Z",
  "dateModified": "2023-02-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/front_end/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical">
                    <span>technical</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/contact/" title="contact">
                    <span>contact</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Cloud 5: Introduction to deploying an app with simple CI/CD
    </h1>
    <div class="post-meta">February 5, 2023&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/on_venus.jpg" alt="">
        
</figure>
  <div class="post-content"><p><em><a href="https://www.serpentinegalleries.org/whats-on/p-staff-venus/">Patrick Staff&rsquo;s &lsquo;On Venus&rsquo;</a> at the Serpentine gallery</em></p>
<h3 id="deploying-a-web-app-with-continuous-integration-and-continuous-deployment-cicd">Deploying a web app with Continuous Integration and Continuous Deployment (CI/CD)<a hidden class="anchor" aria-hidden="true" href="#deploying-a-web-app-with-continuous-integration-and-continuous-deployment-cicd">#</a></h3>
<p><strong>Introduction</strong><br>
The <a href="https://johnardavies.github.io/technical/serverless/">last post</a> used a Serverless approach to regularly search Twitter for particular words and then stored the corresponding Tweet data in a cloud database. Here we develop a simple Streamlit app that accesses the database and creates a page showing average daily sentiment on the topic. We then deploy the app in the cloud. To do this we use many of the tools from previous posts combined with some new ones.
I wrote this to learn more about:</p>
<p><strong>Continuous Integration (CI)</strong><br>
The process where before integration of code changes into the main project codebase automated testing is run.</p>
<p><strong>Continuous Deployment (CD)</strong><br>
The process where, once testing has happened, and the changes integrated into the main code branch, the changes are automatically released to the public product.</p>
<p>At a high level deploying the app involves:</p>
<p><strong>1. Building a Docker container image to hold the app</strong><br>
<strong>2. Deploying the container to a virtual machine to make the app available</strong></p>
<p>It is possible to do these stages manually (as the post on using <a href="https://johnardavies.github.io/technical/docker_use/">Jupyter notebooks in virtual machines</a> does, <strong>here we automate this using <a href="https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions%60">GitHub Actions</a>.</strong></p>
<p>We have a very simple form of continuous integration - when we want to merge changes on GitHub from the working branch into the app&rsquo;s main branch we run a code formatter over the app code to check for any formatting errors. Once this has been done we automatically deploy the new app to the virtual machine running in the cloud (continuous deployment). The schematic below gives a high-level summary of the pipeline.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/pipeline.png" alt="Pipeline"  />
</p>
<p><strong>Cloud warning:</strong> <em>The activities in this and the previous post it builds on will incur costs from running cloud services. Costs should always be monitored and services turned off to avoid incurring unwanted expense. In addition this post involves linking multiple cloud services which can potentially cause security issues. Care should be taken not to commit keys and secrets in git and to avoid giving any unnecessary access to services.</em></p>
<p>The following sections cover:</p>
<p><strong>1. Creating a Streamlit app that accesses the cloud database</strong><br>
<strong>2. Developing a Docker container to hold the app in</strong><br>
<strong>3. Producing a basic CI system that automatically formats new code changes</strong><br>
<strong>4. Having a basic CD system that when the app&rsquo;s code changes rebuilds the Docker container image and runs it on the virtual machine</strong></p>
<h3 id="1-creating-a-streamlit-app-that-accesses-the-cloud-database">1. Creating a Streamlit app that accesses the cloud database<a hidden class="anchor" aria-hidden="true" href="#1-creating-a-streamlit-app-that-accesses-the-cloud-database">#</a></h3>
<p>The python script below generates a very simple app, which we will deploy to the cloud. It accesses the Twitter data that is stored in an Amazon Web Services (AWS) DynamoDB, runs the Vader sentiment analyser over the collected tweets and calculates the average sentiment per day. It then generates a chart of this information with ggplot and converts this to a webpage with Streamlit.</p>
<pre tabindex="0"><code>import boto3
import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from plotnine import *
import streamlit as st

# Imports the AWS key and secret to access the database. 
# Delete this direct import before the script is deployed See discussion later.
from aws_auth import *

# Gives a streamlit app title
st.title(&quot;Twitter sentiment monitor&quot;)

st.button(&quot;Update&quot;)

# Connect to DynamoDB with the boto3 library
# Delete the aws key and secret lines before deploying
dynamodb = boto3.resource(
    &quot;dynamodb&quot;
    , aws_access_key_id=APP_KEY
    , aws_secret_access_key=APP_SECRET
)

# Connect to the dynamodb table apicallstore
dynamodb = boto3.resource(&quot;dynamodb&quot;)
table = dynamodb.Table(&quot;apicallstore&quot;)

# Read in the table and extract the tweets
response = table.scan()
data = response[&quot;Items&quot;]

# Takes the data from DynamoDB and turn it into a data frame
df = pd.DataFrame(data)


# # Sentiment analysis

analyser = SentimentIntensityAnalyzer()


def print_sentiment_scores(sentence):
        try:
            snt = analyser.polarity_scores(sentence)
            return snt  
        except:
            &quot;none&quot;


df[&quot;polarity_score&quot;] = df[&quot;text&quot;].map(print_sentiment_scores)

# Extracts the compound sentiment coefficient. This is the sum of positive, negative &amp; neutral scores
# normalised between -1 (the most negative sentiment) and +1 (most positive sentiment).
df[&quot;compound&quot;] = df[&quot;polarity_score&quot;].apply(lambda x: x[&quot;compound&quot;])


df[&quot;date&quot;] = pd.to_datetime(df[&quot;created_at&quot;]).dt.date


# Calculates the average daily sentiment
sent_date = df[[&quot;date&quot;, &quot;compound&quot;]].groupby(&quot;date&quot;).mean()
sent_date = sent_date.reset_index()

# Create a chart of the average daily sentiment
daily_average = (
    ggplot(sent_date, aes(x=&quot;date&quot;, y=&quot;compound&quot;, group=1))
    + geom_line()
    + ylab(&quot;Average sentiment&quot;)
    + theme(axis_text_x=element_text(rotation=90, hjust=1))
    + ylim(-1, 1)
)

# Make the chart available through Streamlit and add a header
st.header(&quot;Comparing daily average sentiment&quot;)
st.pyplot(ggplot.draw(daily_average))

</code></pre><p>In this example the serverless pipeline was set up to find tweets with the word London, so the script (which is called <code>London_tweet_monitor.py</code>) will calculate average daily sentiment of a sample of regularly collected tweets that in some way relate to &ldquo;London&rdquo;. The sentiment can range between -1 (most negative) and 1 (most positive).</p>
<p>In order for the script to run locally it needs access to the AWS key and secret credentials which give permission to read from the database (Here imported in the aws_auth.py file) and a virtual environment which has the relevant dependencies. The key and secret credentials should be obtained from an Identity Access Management (IAM) role that gives just read-only access to the data, as we want to limit the permissions to use AWS services. For instructions on how to set up a read-only IAM role for DynamodDB, see the two tutorials on this subject linked at the bottom of this post. The script can be deployed locally from the command line using the command below.</p>
<pre tabindex="0"><code>$ streamlit run London_tweet_monitor.py
</code></pre><p>This produces a simple app running locally on  http://localhost:xxxx that shows the average daily sentiment (where xxxx is the port number, in this example we will use port 8009).
<img loading="lazy" src="https://johnardavies.github.io/streamlit_example.png" alt="Streamlit"  />
</p>
<h3 id="2-developing-a-docker-container-to-hold-the-app-in">2. Developing a Docker container to hold the app in<a hidden class="anchor" aria-hidden="true" href="#2-developing-a-docker-container-to-hold-the-app-in">#</a></h3>
<p>To ensure the app can run on another machine we create the Dockerfile below which will generate a container image to hold the app. This container image can then be deployed to a virtual machine in the cloud with Docker and run as it has all the dependencies needed. The <code>requirements.txt</code> that is referenced has been generated from the virtual environment used to run the app script locally.</p>
<pre tabindex="0"><code># app/Dockerfile

# Start with a base python image
FROM python:3.9-slim

# Open port 8009
EXPOSE 8009

# Create a directory called app
RUN mkdir app

# Copy the python file for the app and the requirements file to the app folder in the container 
# These should be in the same folder locally with the Dockerfile
COPY  London_tweet_monitor.py requirements.txt /app

# Switch the working directory to the app folder
WORKDIR /app

# Run some basic installations
RUN apt-get update &amp;&amp; apt-get install -y \
    build-essential \
    software-properties-common \
    git \
    &amp;&amp; rm -rf /var/lib/apt/lists/*


#Install the latest version of pip and packages specified in the requirements file such as Streamlit
RUN pip install --upgrade pip
RUN pip3  install -r  requirements.txt

# Run the streamlit app
ENTRYPOINT [&quot;streamlit&quot;, &quot;run&quot;, &quot;London_tweet_monitor.py&quot;, &quot;--server.port=8009&quot;, &quot;--server.address=0.0.0.0&quot;]

</code></pre><p>In what follows we assume that file for the Streamlit app <code>London_tweet_monitor.py</code> along with the <code>Dockerfile</code> and <code>requirements.txt</code> have been committed and pushed to the main branch of the GitHub repo that has been set up for the project. Prior to deployment the lines that were used to import the credentials locally should be removed e.g. <code>from aws_auth import *</code>, <code>aws_access_key_id=APP_KEY</code> and the <code>aws_secret_access_key=APP_SECRET</code>. A different approach is used to obtain the credentials needed to access DynamoDB in the cloud which is covered later.</p>
<h3 id="3-producing-a-basic-ci-system-that-automatically-formats-new-code-changes">3. Producing a basic CI system that automatically formats new code changes<a hidden class="anchor" aria-hidden="true" href="#3-producing-a-basic-ci-system-that-automatically-formats-new-code-changes">#</a></h3>
<p>We use GitHub Actions to trigger various activities based on changes made in GitHub. A new action can be generated by clicking the GitHub repo&rsquo;s Actions tab and then the <code>New workflow</code> tab.
<img loading="lazy" src="https://johnardavies.github.io/github_actions.png" alt="GitHub Actions"  />

This will prompt you with a screen showing some of the workflows that are available. Here we will write a new Action so click on <code>set up a workflow yourself</code>. Copying the code below into the GitHub editor and saving will create a file with this text in the directory <code>.github/workflows</code> where GitHub Actions are defined. Here we do a basic test to the code using <a href="https://black.readthedocs.io/en/stable/">the Black formatter</a>. To link this to your GitHub add your user name and associated email into the script in the relevant lines. The structure of the Action is as follows:</p>
<ol>
<li><strong>name:</strong> What the Action is called. Here Black.</li>
<li><strong>on:</strong> The event that triggers the action. Here the event is a pull request on the main branch of the GitHub repo.</li>
<li><strong>jobs:</strong> The workflow comprises a series of jobs.</li>
<li><strong>runs-on:</strong> specifies where the Action is run. Here an ubuntu virtual machine that GitHub is providing to the Action.</li>
<li><strong>uses:</strong> Other Actions that the Action uses. Here:
<ul>
<li><code>checkout@v3</code> which checks out the GitHub repo so the Action can work with it.</li>
<li><code>setup-python@v2</code> which allows the Action to use a version of python, so we can install Black and run it.</li>
</ul>
</li>
<li><strong>run:</strong> The commands that are run. Here a series of python and Git commands to install and run the Black formatter and then commit the changes that have been made.</li>
</ol>
<pre tabindex="0"><code>name: Black

on:
 pull_request:
    branches:
      - main
      
jobs:
  format:
    runs-on: ubuntu-latest
    steps:
     # Download the code to the environment with checkout@v2 the reference (ref) is needed to allow the commit created by the Action to be added to the branch
      - uses: actions/checkout@v3
        with:
           fetch-depth: 0
           ref: ${{ github.event.pull_request.head.ref }}
      - uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - run: |
          python -m pip install black
          # Runs black on the python files
          black $(git ls-files '*.py')
          git config --global user.email &quot;GitHub user email&quot;
          git config --global user.name &quot;GitHub user name&quot;
          # Adds and commits with a message with the -am flag. The code after || is used if first part fails
          git commit -am &quot;Format code with Black&quot; || echo &quot;No changes to commit&quot;
          # pushes it
          git push
    
</code></pre><p>When working on a separate branch from the main branch and opening a pull request the Black formatter will now be applied to the code in the branch. When the pull request is created, as shown below, after a few seconds a GitHub Actions in progress symbol will appear showing that the Black formatter is running.
<img loading="lazy" src="https://johnardavies.github.io/black_formatting.png" alt="Black formatter"  />
 Black will then create another commit that fixes formatting issues that it has identified so that the pull requests into the main branch are always formatted to the Black standard. Below is an example of how the GitHub Actions commit has automatically fixed the formatting errors for the chart in the python script.
<img loading="lazy" src="https://johnardavies.github.io/code_fixing.png" alt="Code fixing"  />
 If you click on the Actions tab you will be able to track the Action&rsquo;s operations as it runs.</p>
<h3 id="4--having-a-basic-cd-system-that-when-the-apps-code-changes-rebuilds-the-docker-container-image-and-runs-it-on-the-virtual-machine">4.  Having a basic CD system that when the app&rsquo;s code changes rebuilds the Docker container image and runs it on the virtual machine<a hidden class="anchor" aria-hidden="true" href="#4--having-a-basic-cd-system-that-when-the-apps-code-changes-rebuilds-the-docker-container-image-and-runs-it-on-the-virtual-machine">#</a></h3>
<p>Having the Streamlit App and the Docker file we now use GitHub Actions to take the code on GitHub and automatically deploy it.
This involves when the code is changed on the main GitHub branch:</p>
<p><strong>1. building a new Docker container image for it</strong><br>
<strong>2. running it on the virtual machine to make the changes available</strong></p>
<p>We cover these in section 4.1 and section 4.2. To do this we set up a new GitHub Action with the following code. The parts labelled as Part 1 and Part 2 in the code below correspond to the different stages. Here we store the container image that the Action creates on Docker Hub (container image storage options are a standard offer from cloud providers so one could use another option such as the AWS Elastic Container Registry or the DigitalOcean Container Registry).</p>
<pre tabindex="0"><code>name: Build and Publish Docker container
on:
  push:
    branches:
      - main
          
jobs:
  # define job to build and publish docker image
  build-and-push-docker-image:
    name: Build Docker image and push to repositories
    runs-on: ubuntu-latest
    
    steps: 
      # Part 1: Rebuilding the container image on DockerHub
      - name: Checkout
        uses: actions/checkout@v3
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2 
        
      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: ./
          file: ./Dockerfile
          push: true
          tags:  ${{secrets.DOCKERHUB_USERNAME}}/sentiment_monitor:latest
          
      # Part 2: Deploying the container image to the EC2 instance     
      - name: executing remote ssh commands using ssh key
        # Action to ssh into the EC2
        uses: appleboy/ssh-action@v0.1.7
        with:
         host: ${{ secrets.HOST }}
         username: ${{ secrets.USERNAME }}
         key: ${{ secrets.KEY }}
         port: ${{ secrets.PORT }}
         script: |
           # Stop any running containers
           docker stop $(docker ps -a -q)
           # Delete the stopped containers 
           docker rm $(docker ps -a -q)
           # Login to Docker Hub
           docker login -u ${{ secrets.DOCKERHUB_USERNAME }} -p ${{ secrets.DOCKERHUB_TOKEN }} 
           # Pull down the container image from Docker Hub
           docker pull  ${{ secrets.DOCKERHUB_USERNAME }}/sentiment_monitor:latest
           # Run the image in detached mode sending the container's output port 8009 to external port 8009 
           docker run --detach -p 8009:8009  ${{ secrets.DOCKERHUB_USERNAME }}/sentiment_monitor:latest 
 
</code></pre><h3 id="41-updating-the-container-on-docker-hub-when-the-code-on-the-main-branch-on-github-changes">4.1 Updating the container on Docker Hub when the code on the main branch on GitHub changes<a hidden class="anchor" aria-hidden="true" href="#41-updating-the-container-on-docker-hub-when-the-code-on-the-main-branch-on-github-changes">#</a></h3>
<p><strong>4.1.1 Giving GitHub access to the Docker Hub container repository</strong></p>
<p>Going to Docker Hub we select <code>Create repository</code> and start a new private repository called <code>sentiment_monitor</code> and create a DockerHub access token. The access token can be generated by going to the <code>Account Settings</code> button and then the <code>Security</code> tab.</p>
<p>We need to use security credentials to connect all the pieces of the pipeline together e.g. to allow the GitHub Action to access Docker Hub and login to the EC2 virtual machine. We don’t want to write any of this information directly in the code as it poses a security risk and is hard to remove once committed to Git.</p>
<p>For the Docker Hub and EC2 login information we use the Secrets store of GitHub Actions. This can be found by going to the <code>Settings</code> tab in the project&rsquo;s GitHub repo and scrolling down to the Security tab and selecting the <code>Actions</code> tab under <code>Secrets and variables</code> as shown in the image below. The secrets are added in here
<img loading="lazy" src="https://johnardavies.github.io/github_secrets.png" alt="Github secrets"  />
 and are then injected through the curly brackets in the Actions script with the secrets prefix e.g. ${{ secrets.DOCKERHUB_TOKEN }} inserts the access token we have created and added as a secret called DOCKERHUB_TOKEN into the script.</p>
<p>In the secrets tab of GitHub Actions we add two pieces of information <code>DOCKERHUB_TOKEN</code>, <code>DOCKERHUB_USERNAME</code> that will allow the GitHub Action to access the Docker Hub and the repository where the container image is stored.</p>
<p><strong>4.1.2 Having the Action rebuild the container image that is available on DockerHub</strong></p>
<p>The action code uses a succession of pre-existing actions in Part 1 to rebuild the container image on Docker Hub following a push to the main code branch on GitHub:</p>
<ol>
<li><strong>actions/checkout@v3</strong> checks out the existing code which the push has changed</li>
<li><strong>docker/setup-buildx-action@v2</strong> an action that allows containers to be built</li>
<li><strong>docker/login-action@v2</strong> logs into Docker Hub using the credentials that added as secrets</li>
<li><strong>docker/build-push-action@v4</strong> builds and pushes the container image to Docker Hub</li>
</ol>
<p>Now when a change is made to the main branch of the repo the GitHub Actions can log into Docker Hub and rebuild the container image on the Hub based on the latest version of the app code.</p>
<h3 id="42-deploy-the-container-from-github-to-ec2-after-a-change-to-the-code-running-in-the-container">4.2 Deploy the container from GitHub to EC2 after a change to the code running in the container<a hidden class="anchor" aria-hidden="true" href="#42-deploy-the-container-from-github-to-ec2-after-a-change-to-the-code-running-in-the-container">#</a></h3>
<p>After the Action has rebuilt the container image (Part 1) we now have it deploy the container to EC2 (Part 2). For simplicity we have the EC2 already running (for a guide on setting up an EC2 on AWS see <a href="https://kstathou.medium.com/how-to-set-up-a-gpu-instance-for-machine-learning-on-aws-b4fb8ba51a7c">this tutorial</a> from Kostas Stathoulopoulos - in this case as the application is not computationally intensive a GPU is not needed). To allow the GitHub Action to pull down the container image from Docker Hub and for the image to be run successfully we need to enable permissions to allow 1. the GitHub Action to login to the virtual machine and 2. allow the virtual machine to read from the DynamoDB database where the data is.</p>
<p><strong>4.2.1 Giving GitHub Actions the ability to log into the virtual machine</strong></p>
<p>An EC2 instance will have a name along the lines of <code>ubuntu@ec2-xx-xxx-xx-xxx.region.compute.amazonaws.com</code> (Where xxs are numbers and region is an AWS region e.g. eu-west-2). The part before the @ is the username and the part after is the host (This is the default setting for an ubuntu virtual machine, with another operating system or created user account the user name will be different. To allow us to ssh into the virtual machine from GitHub in the GitHub Actions secrets we add HOST (In this example <code>ec2-18-170-44-111.eu-west-2.compute.amazonaws.com</code>) and USERNAME as <code>ubuntu</code>. The SSH key is also copied into the GitHub secrets (here labelled <code>KEY</code>) to allow the Action to login (this should be the full key including all the text in the key file). The port that is used for SSH is also added as <code>PORT</code> (Typically it is 22). Assuming the EC2 instance is accessible using SSH keys only (without a password) the GitHub Actions will then be able to log into the EC2 instance.</p>
<p><strong>4.2.2 Giving the EC2 instance the ability to read data from the DynamoDB database</strong></p>
<p>To give the EC2 instance access to the DynamoDB database we attach the IAM role to the instance that gives read only access to the DynamoDB database. Once the IAM role has been added to the instance, then it will automatically be able to access the credentials to read data from DynamoDB - there is no need to reference or import credentials in the script which is why they were removed previously.</p>
<p><strong>4.2.3 Deploying the container</strong></p>
<p>When the code has been pushed to GitHub, after having:</p>
<ol>
<li>Tested the branched code with the Black formatter before branches are merged into the main branch (The first Action)</li>
<li>Updated the Docker container image on DockerHub using the changed code (Part 1 of the second Action)</li>
</ol>
<p>Part 2 of the second action uses the pre-existing action <code>appleboy/ssh-action@v0.1.7</code> which allows the Action to SSH into the EC2 using the credentials that were added. The GitHub Action will then stop the existing container which is running on the EC2 and delete it. The Action will then pull down the new container image from Docker Hub and run it. This takes a few minutes. In this example the app will be available at <code>http://ec2-18-170-44-111.eu-west-2.compute.amazonaws.com:8009/</code> i.e. port 8009 of the EC2 instance it is being run on. The app will be up and running using the code in the latest container image. This could all be done manually, but is now automated with GitHub Actions. As an example if we add the following code which generates average hourly sentiment into the python file from another branch.</p>
<pre tabindex="0"><code># # Sentiment by hour

sent_hour=df[[&quot;hour&quot;, &quot;compound&quot;]].groupby([&quot;hour&quot;]).mean()
sent_hour=sent_hour.reset_index()


days_by_hour=ggplot(sent_hour , aes(x='hour', y='compound')) + geom_line()+ylab(&quot;Average sentiment&quot;)+ylim(-1,1)

st.header('Average hourly sentiment')
st.pyplot(ggplot.draw(days_by_hour))
</code></pre><p>Then it will automatically be reformatted by our action running the Black formatter as:</p>
<pre tabindex="0"><code># # Sentiment by hour

sent_hour = df[[&quot;hour&quot;, &quot;compound&quot;]].groupby([&quot;hour&quot;]).mean()
sent_hour = sent_hour.reset_index()


days_by_hour = (
    ggplot(sent_hour, aes(x=&quot;hour&quot;, y=&quot;compound&quot;))
    + glom_line()
    + ylab(&quot;Average sentiment&quot;)
    + ylim(-1, 1)
)

st.header(&quot;Average hourly sentiment&quot;)
st.pyplot(ggplot.draw(days_by_hour))
</code></pre><p>When this change is pulled into the main branch the container image on Docker Hub will be updated, pulled down to the EC2 instance and run, and the served page updated automatically with the changes to the below.
<img loading="lazy" src="https://johnardavies.github.io/post_change.png" alt="Updated"  />
</p>
<h3 id="extensions">Extensions<a hidden class="anchor" aria-hidden="true" href="#extensions">#</a></h3>
<p>Although this example illustrates some basic CI/CD for learning purposes, it does not pretend to be fully optimised. Things to improve include:</p>
<p><strong>Caching the data</strong>
As it is currently the app is reading in all the data from DynamoDB and does so each time it is reloaded.
This is inefficient and will pose issues as the amount of data increases so it would probably be better to have the app cache the processed data it has loaded to date and only import the latest raw data that has been collected.</p>
<p><strong>https and not http</strong><br>
This currently serves the app with http and not https, converting it to https would require setting up certificates.</p>
<p><strong>Managing the images</strong><br>
The container images that have been run are all stored on the EC2 instance when they are pulled down. On a small EC2 instance, which is what it should be deployed on, this will eventually fill up the memory. If we want to retain them the container images should be moved to a lower cost storage solution with more capacity or deleted if no longer needed.</p>
<h3 id="other-cloud-posts">Other cloud posts:<a hidden class="anchor" aria-hidden="true" href="#other-cloud-posts">#</a></h3>
<p><em><strong><a href="https://johnardavies.github.io/technical/cloud_intro/">Cloud 1: Introduction to launching a Virtual Machine in the Cloud</a></strong></em></p>
<p><em><strong><a href="https://johnardavies.github.io/technical/cloud_use/">Cloud 2: Getting started with using a Virtual Machine in the Cloud</a></strong></em></p>
<p><em><strong><a href="https://johnardavies.github.io/technical/docker_use/">Cloud 3: Docker and Jupyter notebooks in the Cloud</a></strong></em></p>
<p><em><strong><a href="https://johnardavies.github.io/technical/serverless/">Cloud 4: Using Serverless</a></strong></em></p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<p><strong>Deploying Streamlit on Docker</strong></p>
<p><a href="https://docs.streamlit.io/knowledge-base/tutorials/deploy/docker">Streamlit doc on Docker deployment</a></p>
<p><strong>Giving IAM roles access to DynamoDB</strong></p>
<p>Jennelle Cosby, <a href="https://awstip.com/using-aws-iam-roles-with-ec2-and-dynamodb-7beb09af31b9/">&lsquo;Using AWS IAM roles with EC2 and DynamoDB&rsquo;</a><br>
Omar Egal, <a href="https://medium.com/@omar.egal/how-to-create-a-dynamodb-table-and-grant-aws-ec2-read-only-access-f1b2049b8324">&lsquo;How to create a DynamoDB table and grant AWS EC2 Read-Only Access&rsquo;</a></p>
<p><strong>Launching EC2</strong></p>
<p>Kostas Stathoulopoulos, <a href="https://kstathou.medium.com/how-to-set-up-a-gpu-instance-for-machine-learning-on-aws-b4fb8ba51a7c">&lsquo;How to set up a GPU instance for machine learning on AWS&rsquo;</a></p>
<p><strong>CI/CD and Docker</strong></p>
<p>Docker Docs, <a href="https://docs.docker.com/build/ci/github-actions/">Introduction to GitHub Actions</a><br>
Minho Jang, <a href="https://medium.com/ryanjang-devnotes/ci-cd-hands-on-github-actions-docker-hub-aws-ec2-ba09f80297e1">&lsquo;CI/CD Hands-On : Github Actions+DockerHub+AWS EC2&rsquo;</a><br>
Truong Huu Thao, <a href="https://thaoth.dev/Full-CI-CD-with-Docker-GitHub-Actions-DigitalOcean-Droplets-Container-Registry/">&lsquo;Full CI/CD with Docker + GitHub Actions + DigitalOcean&rsquo;</a><br>
<a href="https://www.digitalocean.com/community/questions/automatic-deployment-using-github-actions-digital-ocean-registry-into-a-droplet/">Question on deployment Using GitHub Actions on Digital Ocean</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
      <li><a href="https://johnardavies.github.io/tags/digital/">digital</a></li>
      <li><a href="https://johnardavies.github.io/tags/cloud/">cloud</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
