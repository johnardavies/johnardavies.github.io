<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers 1: The Attention Mechanism | John&#39;s Site</title>
<meta name="keywords" content="technical, digital" />
<meta name="description" content="&ldquo;Let me Unsee&rdquo; by Asbestos, Belfast, August 2023.
The T in GPT In all the excitement around OpenAI’s ChatGPT two things are often not mentioned:
 the underlying architecture of Generative Pre-trained Transformer (GPT) models is an approach called the Transformer which was invented and made public by Google despite these models’ impressive capabilities, and their large scale and complexity, the underlying mathematics involved (at least in terms of the components used) is relatively simple  As these models are already important and likely to be more important in future it felt worth learning more about them.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/transformer1/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Transformers 1: The Attention Mechanism" />
<meta property="og:description" content="&ldquo;Let me Unsee&rdquo; by Asbestos, Belfast, August 2023.
The T in GPT In all the excitement around OpenAI’s ChatGPT two things are often not mentioned:
 the underlying architecture of Generative Pre-trained Transformer (GPT) models is an approach called the Transformer which was invented and made public by Google despite these models’ impressive capabilities, and their large scale and complexity, the underlying mathematics involved (at least in terms of the components used) is relatively simple  As these models are already important and likely to be more important in future it felt worth learning more about them." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/transformer1/" />
<meta property="og:image" content="https://johnardavies.github.io/attention.jpg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2023-12-26T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-12-26T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/attention.jpg" />
<meta name="twitter:title" content="Transformers 1: The Attention Mechanism"/>
<meta name="twitter:description" content="&ldquo;Let me Unsee&rdquo; by Asbestos, Belfast, August 2023.
The T in GPT In all the excitement around OpenAI’s ChatGPT two things are often not mentioned:
 the underlying architecture of Generative Pre-trained Transformer (GPT) models is an approach called the Transformer which was invented and made public by Google despite these models’ impressive capabilities, and their large scale and complexity, the underlying mathematics involved (at least in terms of the components used) is relatively simple  As these models are already important and likely to be more important in future it felt worth learning more about them."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers 1: The Attention Mechanism",
      "item": "https://johnardavies.github.io/technical/transformer1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers 1: The Attention Mechanism",
  "name": "Transformers 1: The Attention Mechanism",
  "description": "\u0026ldquo;Let me Unsee\u0026rdquo; by Asbestos, Belfast, August 2023.\nThe T in GPT In all the excitement around OpenAI’s ChatGPT two things are often not mentioned:\n the underlying architecture of Generative Pre-trained Transformer (GPT) models is an approach called the Transformer which was invented and made public by Google despite these models’ impressive capabilities, and their large scale and complexity, the underlying mathematics involved (at least in terms of the components used) is relatively simple  As these models are already important and likely to be more important in future it felt worth learning more about them.",
  "keywords": [
    "technical", "digital"
  ],
  "articleBody": "“Let me Unsee” by Asbestos, Belfast, August 2023.\nThe T in GPT In all the excitement around OpenAI’s ChatGPT two things are often not mentioned:\n the underlying architecture of Generative Pre-trained Transformer (GPT) models is an approach called the Transformer which was invented and made public by Google despite these models’ impressive capabilities, and their large scale and complexity, the underlying mathematics involved (at least in terms of the components used) is relatively simple  As these models are already important and likely to be more important in future it felt worth learning more about them. This is the first in a series of notes to help me understand the area better. This post covers Attention, the key mechanism at the heart of how Transformers work. Subsequent posts will then show how attention is implemented in Transformers to translate languages and generate text.\nWhat follows implicitly assumes some knowledge of vectors, matrices and how neural networks work. The linear algebra involved is though pretty minimal. The main thing used without explanation is that if we have an n dimensional vector v and want to transform it into an m dimensional vector v*, then we need to multiply it by a matrix with dimensions m x n (where m is the number of rows and n the number of columns) i.e. v*=Mv.\n1. Processing and embedding (converting words into vectors) In order to feed text into a neural network we take the text and split it into tokens which are the components that the text is made of. These tokens can be words and for simplicity I’ll call them that, but they can also be parts of speech. These tokens are then encoded into numbers to keep track of them e.g. the word ‘cat’ might be given the number 240, the word ‘dog’ 241 etc. These numbers are converted into vectors representing the words, one vector per word, using a technique called embedding. In an embedding words with a similar meaning have vectors that are closer together. An embedding vector is typically a few tens to a few hundred numbers long. We can represent text as a series of vectors, which we process in the Transformer.\nEmbeddings are created roughly speaking as follows. In the simplest kind of embedding we have a neural network which consists of a matrix of weights of dimensions (dimension of the embedding we want x the number of distinct tokens/words in the text). We then have another matrix of weights that goes from the embedding dimension back to the number of tokens/words and then a layer which from that outputs a probability distribution over all of the words. This layer has the functional form below and is known as a softmax. It takes a set of n inputs (xi, where i = 1 to n) and produces n outputs that sum to 1 i.e. it gives a probability distribution.\nWe then input sentences into the network as a series of vectors which have the dimension of the number of different words used in text. These vectors are 1 for that word’s allocated number and 0 everywhere else. The size of the vector corresponds to the number of distinct words so if there were 15,000 words, the vector would have 15,000 elements (have a 15,000 dimensionality). In the word dog in the example the corresponding vector would be 1 for its 241’th element and the rest of the 15,000 elements would be 0.\nThe vectors representing words are fed into the network and multiplied by the matrices of weights before outputting the probability distribution of different words being the next word in the sentence. We then adjust the weights of the two matrices to minimise the errors that the network makes in predicting the next word, when different bits of text are fed in. After this training the elements (vectors) of the first matrix have the property that each vector represents a word: a vector embedding. If you multiply your vector with 15,000 elements representing a word (which will be 1 for the element that corresponds to the word), then you get another vector embedding which has the dimension of the embedding chosen (say 300) which is smaller than the original vector.\nThe distance between these vectors, to some extent, corresponds to similarities in the semantic meaning they have e.g. vectors of synonyms should be closer together.\n2. The Attention mechanism (enriching how vectors represent relations between words) Transformer allow us to include more sophisticated information in the ways that vectors are representing words. Central to this is a mechanism known as ‘attention’ which is used to enrich words’ vectors with information on the relationship to other words in the text i.e. it tries to enable the vector that represents the word to include the meaning of the word given the context of the words in the text, For example in the sentence ‘The dog wagged its tail’ the word ‘its’ (technically a pronoun) has a special relationship to ‘dog’ and also ‘tail’ in the sentence, but ‘its’ and ‘dog’ are not necessarily connected in sentences in general and so this relationship is not necessarily included in a standard word embedding.\n2.1 The value, key and query vectors that are used to calculate attention from the embedding vectors\nIn the attention mechanism we start with a piece of text with m words which are mapped into m embedding vectors, one for each word.\nThese vectors are then multiplied by 3 separate weight matrices, to create three vectors for each word e.g. for the first embedding vector: to produce three distinct sets of vectors:\n Value vectors   Key vectors\n  Query vectors\n￼\n  The reason why these three groups of vectors are called key, query and value respectively will become clearer from the description of the attention mechanism in the next section.\nThe weights in the three matrices are originally created randomly, but are updated during the model training to optimise its ability to predict the next word. Although we are describing the process in terms of three matrices, an approach to implement this, as used in Karpathy’s nanoGPT, is to multiply the embedding vectors by one larger matrix to produce a longer vector and then split the vector into query, key and value vectors.\nThe vectors that the embeddings are transformed into are typically of lower dimensionality than the original embedding so the weight matrices have the dimensions of (new_vectors_dimensions x embedding_vectors_dimension). The query and the key vectors need to have the same dimensions to calculate attention, but the value vector can have a different dimensionality.\n2.2 How attention is calculated for a single word\nThe attention of a word in a piece of text, such as a sentence, is calculated as a weighted average of the value vectors of all the words in a sentence (Written in the example below as the attention for the first word). Where the query vector of the word we are calculating attention for and the key vectors of all the words in the piece of text are used to calculate the weights.\nIn the original Transformers paper the weights for a word (for a given value vector) when the attention mechanism is applied are created by the dot product of the word’s query vector with the corresponding key vectors (for the value vectors) of the words in the piece of text fed in (including the key vector of the word itself). The resulting number is then divided by a scaling factor (the square root of the dimensionality of the key vector). The mechanism is therefore called scaled dot product attention. The standard vector dot product between two vectors being:\n￼\nIf the query vector q is not pointing in a similar direction as the key vector k then the dot product will tend to 0 (e.g. if phi is 90 degrees (pi/2 radians) then cos(phi)=0) and the vector will be down-weighted in the calculation of attention, while words with more similar key and query vectors will be given larger weights in the sum of vectors (e.g. in the dot product of a vector with itself phi=0 and so cos(phi)=1 its maximum value).\nThe resulting dot products are then divided by the square root of the dimension of the key vector and run through a softmax function to ensure that the weights all sum to one. The calculated vector after the attention mechanism is applied is:\n￼ The terms value, key and query vectors come from an analogy with search. We are searching with a query, values are identified by keys and if a query is similar to a key the corresponding value is returned. Where the query and key vectors are less similar, the query is in some sense less similar than the key, the dot product is smaller and the corresponding value vector is returned less strongly as it has a lower weight. In GPTs where a single language is used the same word vector is used to create both key, query and value vectors. In language translation the key and query vectors are sometimes created in the source language and attention calculated in combination with value vectors from the target language.\n3. Attention in matrix form (calculating the attention for all the words in a piece of text at one go) By grouping the three sets of value, key and query vectors into corresponding matrices we can form a query matrix (Q), a key matrix (K) and a values matrix (V), where the rows of these matrices correspond to individual query, key and value vectors. The attention calculation for a piece of text can then be represented by matrix multiplication of Q and the transpose K to get the dot product used to calculate the weights as shown in the simple three word/vector example below: ￼\nThe resulting values are then divided by the square root of the dimensionality of the key vector, taking the softmax of the matrix element across the rows and then multiplying by the values matrix. This is mathematically equivalent to doing the same attention calculation for each word simultaneously.\nIn the example below we have a values matrix on the right and weights calculated previously on the left. Each row of the values matrix corresponds to a different values vector. The superscript of an element of the values matrix shows which vector it corresponds to. The subscript identifies which vector element is involved. The values vectors here have 3 elements, but this is just for illustrative purposes; they could have another dimensionality.\n￼\nMultiplying out the first row of the weights matrix by the columns of the values matrix and transposing we see that the attention calculation of the first vector is recovered.\n￼\nIf we multiplied out the second row of the weights matrix we would obtain the attention calculation for the second vector etc The calculation of attention for all the words in a piece of text can therefore be represented in matrix form:\n￼\nThis results in a matrix of dimension (number of words in the text x dimension of the values vector), where each row represents the individual words after the attention mechanism has been applied.\n4. Multi-headed attention (Encoding multiple relationships between words in a text) What has just been discussed is a single attention head. The kind of calculation can be done in parallel in a number of independent heads (so called multi-headed attention) for the same piece of text. The resulting vectors are then concatenated, to produce one long vector per word which has the dimension of the value vector multiplied by the number of heads.\nEach head has separate weights to generate its query, key and value vectors. This enables each head to potentially learn a different representation of the relationships that exist between the words in the text. For example the relationships between nouns and adjectives in the sentence, or between prepositions (e.g. in, and, on, of) and other words in the sentence.\nIf we had 5 heads and a model dimension embedding that is say 200, then when concatenating the vectors produced by the three heads we would end up with a vector 1000 elements long for a given word.\nTo get from the long vector produced by multi-head attention down to one of lower dimension we then multiply this vector by another matrix of weights which has the dimensions of (the models standard embedding dimension x the dimension of multi-head attention). In this example this matrix would have the dimensions of (200 x 1000), which again are a set of weights that are (along with all the other matrix weights) adjusted in training to predict the probability of the next word.\n5. How Attention is included in neural networks (and some things that are missing from the earlier discussion) Having started with our embeddings vectors with each vector representing a word, we then generated a set of query, key and value vectors. These three vectors are generated separately for however many attention heads we have. In each head we take the query, key and value vectors produced for that head and calculate a new set of vectors for each word using the attention mechanism. We then stick the vector from each head for each word together to create one long vector per word. We then multiply by the weights matrix to bring the dimensionality down resulting in a new set of vectors representing each word in the text, but one where the vectors represent the relations between the words better than they did in the original embedding vectors. We can then start all over again and feed these vectors into a new attention layer and calculate attention again, then take the resulting vectors and calculate it again, and again etc. Calculating multiple layers of attention one after another is considered to be one of the factors behind GPT models' success. GPT3 is reported to have 96 attention layers.\nIn GPTs and Transformers multiple layers of Attention are applied to build up a rich representation of the text that the Transformer is trained on i.e. the vectors produced by an attention layer are then passed to another attention layer and attention is calculated again. Although this post has discussed how attention has calculated it has omitted some things to keep things simple:\n  How the position of a word in a sentence is captured As described above nothing in the calculation of attention depends on the position of a word in the sentence. In practice an additional vector which represents a word’s position in the sentence is added to the word’s original embedding vector at the beginning to capture this. There is more than one technique to do this.\n  How attention deals with causality As described above, attention is calculated between all the words in the sentence, but sometimes that is not appropriate. For example if you are trying to get the network to generate a new sentence it should not have direct access in its inputs to the words used to complete a given sentence, you want the network to predict the words to use, not copy an input. To address this attention is often supplemented with filters (called masks) to change which words are included in its calculation.\n  How attention mechanisms are implemented in code and put together to create GPTs or translate from one language to another This post has covered how attention is calculated, but the discussion of attention has been conceptual, rather than how it is calculated in practice and then used to do things. For example there are some further processing stages that are used when the attention mechanism discussed here is included within a neural network.\n  These will be covered in subsequent posts.\nReferences:\nI found the following sources very helpful in writing this:\nThe original Transformers paper ‘Attention is All You Need’, Vaswani et al. (2017)\nThe repo for Andrej Karpathy’s nanoGPT\nJay Alammar’s ‘The illustrated transformer’\nFrançois Chollet’s book ‘Deep Learning with Python (2nd edition)’\nOpenAI’s, ‘Language Models are Few-Shot Learners’\n",
  "wordCount" : "2673",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/attention.jpg","datePublished": "2023-12-26T00:00:00Z",
  "dateModified": "2023-12-26T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/transformer1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Transformers 1: The Attention Mechanism
    </h1>
    <div class="post-meta">December 26, 2023&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/attention.jpg" alt="">
        
</figure>
  <div class="post-content"><p>&ldquo;Let me Unsee&rdquo; by Asbestos, Belfast, August 2023.</p>
<h3 id="the-t-in-gpt">The T in GPT<a hidden class="anchor" aria-hidden="true" href="#the-t-in-gpt">#</a></h3>
<p>In all the excitement around OpenAI’s ChatGPT two things are often
not mentioned:</p>
<ol>
<li>the underlying architecture of Generative Pre-trained Transformer (GPT)
models is an approach called the Transformer which was invented and made
public by Google</li>
<li>despite these models’ impressive capabilities, and their large scale and complexity, the underlying mathematics
involved (at least in terms of the components used) is relatively simple</li>
</ol>
<p>As these models are already important and likely to be more important in future
it felt worth learning more about them. This is the first in a series of
notes to help me understand the area better. This post covers Attention,
the key mechanism at the heart of how Transformers work. Subsequent posts
will then show how attention is implemented in Transformers to translate languages and generate text.</p>
<p>What follows implicitly assumes some knowledge of vectors, matrices and how
neural networks work. The linear algebra involved is though pretty minimal.
The main thing used without explanation is that if we have an n dimensional vector v and want to
transform it into an m dimensional vector v*, then we need to multiply it by
a matrix with dimensions m x n (where m is the number of rows and n the
number of columns) i.e. <code>v*=Mv</code>.</p>
<h3 id="1-processing-and-embedding--converting-words-into-vectors">1. Processing and embedding  (converting words into vectors)<a hidden class="anchor" aria-hidden="true" href="#1-processing-and-embedding--converting-words-into-vectors">#</a></h3>
<p>In order to feed text into a neural network we take the text and split it into tokens which are the
components that the text is made of. These tokens can be words and for
simplicity I’ll call them that, but they can also be parts of speech.
These tokens are then encoded into numbers to keep track of them e.g. the
word ‘cat’ might be given the number 240, the word ‘dog’ 241 etc. These
numbers are converted into vectors representing the words, one vector per
word, using a technique called embedding. In an embedding words with a
similar meaning have vectors that are closer together. An embedding vector
is typically a few tens to a few hundred numbers long. We can represent
text as a series of vectors, which we process in the Transformer.</p>
<p>Embeddings are created roughly speaking as follows. In the simplest kind
of embedding we have a neural network which consists of a matrix of
weights of dimensions (dimension of the embedding we want x the number of
distinct tokens/words in the text). We then have another matrix of
weights that goes from the embedding dimension back to the number of
tokens/words and then a layer which from that outputs a probability distribution
over all of the words. This layer has the functional form below and is
known as a softmax. It takes a set of n inputs (xi, where i = 1 to n) and produces n outputs
that sum to 1 i.e. it gives a probability distribution.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/Softmax2.png" alt="softmaxs"  />
</p>
<p>We then input sentences into the network as a series of vectors which have
the dimension of the number of different words used in text. These vectors are 1 for that word’s
allocated number and 0 everywhere else. The size of the vector corresponds to
the number of distinct words so if there were 15,000 words, the vector would have 15,000 elements (have a 15,000
dimensionality). In the word dog in the example the corresponding vector
would be 1 for its 241&rsquo;th element and the rest of the 15,000 elements would be 0.</p>
<p>The vectors representing words are fed into the network and multiplied by the
matrices of weights before outputting the probability distribution of different words
being the next word in the sentence. We then adjust the weights of the two matrices to
minimise the errors that the network makes in predicting the next word, when different bits
of text are fed in. After this training the elements (vectors) of the first matrix have the property that
each vector represents a word: a vector embedding. If you multiply your vector with 15,000 elements representing a word
(which will be 1 for the element that corresponds to the word), then you get another vector embedding
which has the dimension of the embedding chosen (say 300) which is smaller
than the original vector.</p>
<p>The distance between these vectors, to some extent, corresponds to similarities in the semantic
meaning they have e.g. vectors of synonyms should be closer together.</p>
<h3 id="2-the-attention-mechanism-enriching-how-vectors-represent-relations-between-words">2. The Attention mechanism (enriching how vectors represent relations between words)<a hidden class="anchor" aria-hidden="true" href="#2-the-attention-mechanism-enriching-how-vectors-represent-relations-between-words">#</a></h3>
<p>Transformer allow us to include more sophisticated
information in the ways that vectors are representing words. Central to
this is a mechanism known as ‘attention’ which is used to enrich words’
vectors with information on the relationship to other words in the text
i.e. it tries to enable the vector that represents the word to include the
meaning of the word given the context of the words in the text, For example in the
sentence ‘The dog wagged its tail’ the word ‘its’ (technically a pronoun)
has a special relationship to ‘dog’ and also ‘tail’ in the sentence, but
‘its’ and ‘dog’ are not necessarily connected in sentences in general and
so this relationship is not necessarily included in a standard word
embedding.</p>
<p><strong>2.1 The value, key and query vectors that are used to calculate attention from the embedding vectors</strong></p>
<p>In the attention mechanism we start with a piece of text with m words
which are mapped into m embedding vectors, one for each word.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/embeddings.png" alt="embeddings_vectors"  />
</p>
<p>These vectors are then multiplied by 3 separate weight matrices, to create
three vectors for each word e.g. for the first embedding vector:
<img loading="lazy" src="https://johnardavies.github.io/three_matrices.png" alt="three_matrices"  />
</p>
<p>to produce three distinct sets of vectors:</p>
<ol>
<li>Value vectors</li>
</ol>
<p><img loading="lazy" src="https://johnardavies.github.io/value_vectors2.png" alt="value_vectors"  />
</p>
<ol start="2">
<li>
<p>Key vectors</p>
<p><img loading="lazy" src="https://johnardavies.github.io/key_vectors.png" alt="key_vectors"  />
</p>
</li>
<li>
<p>Query vectors</p>
<p><img loading="lazy" src="https://johnardavies.github.io/query_vectors.png" alt="query_vectors"  />
￼</p>
</li>
</ol>
<p>The reason why these three groups of vectors are called key, query
and value respectively will become clearer from the description of the attention
mechanism in the next section.</p>
<p>The weights in the three matrices are originally created randomly, but are updated
during the model training to optimise its ability to predict the next word. Although
we are describing the process in terms of three matrices, an approach to implement this,
as used in Karpathy&rsquo;s nanoGPT, is to multiply the embedding vectors by one larger matrix to produce
a longer vector and then split the vector into query, key and value vectors.</p>
<p>The vectors that the embeddings are transformed into are typically of
lower dimensionality than the original embedding so the weight matrices
have the dimensions of (new_vectors_dimensions x
embedding_vectors_dimension). The query and the key vectors need to have the
same dimensions to calculate attention, but the value vector can have a different dimensionality.</p>
<p><strong>2.2 How attention is calculated for a single word</strong></p>
<p>The attention of a word in a piece of text, such as a sentence, is calculated as a weighted average of the value vectors
of all the words in a sentence (Written in the example below as the attention for the first word). Where the query vector of
the word we are calculating attention for and the key vectors of all the words in the piece of text are used to calculate
the weights.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/vector_weights2.png" alt="vector_weights"  />
</p>
<p>In the original Transformers paper the weights for a word (for a given value vector) when the attention mechanism is applied
are created by the dot product of the word&rsquo;s query vector with the corresponding key vectors (for the value vectors) of the
words in the piece of text fed in (including the key vector of the word itself). The resulting number is then divided by a
scaling factor (the square root of the dimensionality of the key vector). The mechanism is therefore called
scaled dot product attention. The standard vector dot product between two
vectors being:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/dot_product2.png" alt="dot_product"  />
￼</p>
<p>If the query vector q is not pointing in a similar direction as the key vector k then the dot product will tend to 0 (e.g.
if phi is 90 degrees (pi/2 radians) then cos(phi)=0) and the
vector will be down-weighted in the calculation of attention, while words with more
similar key and query vectors will be given larger weights in the sum of
vectors (e.g. in the dot product of a vector with itself phi=0 and so cos(phi)=1 its maximum value).</p>
<p>The resulting dot products are then divided
by the square root of the dimension of the key vector and run through a softmax function to
ensure that the weights all sum to one. The calculated vector after the attention mechanism is applied is:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/attention_vector2.png" alt="attention_vectors"  />

￼
The terms value, key and query vectors come from an analogy with search.
We are searching with a query, values are identified by keys and if a
query is similar to a key the corresponding value is returned. Where the
query and key vectors are less similar, the query is in some sense less similar than the
key, the dot product is smaller and the corresponding value vector is
returned less strongly as it has a lower weight. In GPTs where a single language is used the same word
vector is used to create both key, query and value vectors. In language translation
the key and query vectors are sometimes created in the source language
and attention calculated in combination with value vectors from the target language.</p>
<h3 id="3-attention-in-matrix-form-calculating-the-attention-for-all-the-words-in-a-piece-of-text-at-one-go">3. Attention in matrix form (calculating the attention for all the words in a piece of text at one go)<a hidden class="anchor" aria-hidden="true" href="#3-attention-in-matrix-form-calculating-the-attention-for-all-the-words-in-a-piece-of-text-at-one-go">#</a></h3>
<p>By grouping the three sets of value, key and query vectors into
corresponding matrices we can form a query matrix (Q), a key matrix (K) and
a values matrix (V), where the rows of these matrices correspond to individual query, key and value vectors. The attention
calculation for a piece of text can then be represented by matrix multiplication of Q and the transpose K to get the dot product used to calculate the
weights as shown in the simple three word/vector example below:
<img loading="lazy" src="https://johnardavies.github.io/qk_example4.png" alt="qk_example"  />
￼</p>
<p>The resulting values are then divided by the square root of the dimensionality of the key
vector, taking the softmax of the matrix element across the rows and then multiplying by the values matrix. This
is mathematically equivalent to doing the same attention calculation for each word
simultaneously.</p>
<p>In the example below we have a values matrix on the right and weights calculated previously on the left. Each row of the
values matrix corresponds to a different values vector. The superscript of an element of the values matrix shows
which vector it corresponds to. The subscript identifies which vector element is involved. The values vectors here have 3
elements, but this is just for illustrative purposes; they
could have another dimensionality.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/softmax_weight.png" alt="softmax_weight"  />
￼</p>
<p>Multiplying out the first row of the weights matrix by the columns of the values matrix and transposing we see
that the attention calculation of the first vector is recovered.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/matrix_mult4.png" alt="matrix_mult4"  />
￼</p>
<p>If we multiplied out the second row of the weights matrix
we would obtain the attention calculation for the second vector etc
The calculation of attention for all the words in a piece of
text can therefore be represented in matrix form:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/attention_matrix.png" alt="attention_matrix"  />
￼</p>
<p>This results in a matrix of dimension (number of words in
the text x dimension of the values vector), where each row represents the individual words after the attention
mechanism has been applied.</p>
<h3 id="4-multi-headed-attention-encoding-multiple-relationships-between-words-in-a-text">4. Multi-headed attention (Encoding multiple relationships between words in a text)<a hidden class="anchor" aria-hidden="true" href="#4-multi-headed-attention-encoding-multiple-relationships-between-words-in-a-text">#</a></h3>
<p>What has just been discussed is a single attention head. The kind of
calculation can be done in parallel in a number of independent heads (so
called multi-headed attention) for the same piece of text. The resulting vectors
are then concatenated, to produce one long vector per word which has the
dimension of the value vector multiplied by the number of heads.</p>
<p>Each head has separate weights to generate its query, key and value vectors.
This enables each head to potentially learn a different representation of the
relationships that exist between the words in the text. For example the relationships between nouns and adjectives in the
sentence, or between prepositions (e.g. in, and, on, of) and other words in the sentence.</p>
<p>If we had 5 heads and a model dimension embedding that is say 200, then when concatenating the vectors produced by the three
heads we would end up with a vector 1000 elements long for a given word.</p>
<p>To get from the long vector produced by multi-head attention down to one of lower dimension we then
multiply this vector by another matrix of weights which has the dimensions
of (the models standard embedding dimension x the dimension of multi-head attention).
In this example this matrix would have the dimensions of (200 x 1000), which again are
a set of weights that are (along with all the other matrix weights) adjusted in training to predict
the probability of the next word.</p>
<h3 id="5-how-attention-is-included-in-neural-networks-and-some-things-that-are-missing-from-the-earlier-discussion">5. How Attention is included in neural networks (and some things that are missing from the earlier discussion)<a hidden class="anchor" aria-hidden="true" href="#5-how-attention-is-included-in-neural-networks-and-some-things-that-are-missing-from-the-earlier-discussion">#</a></h3>
<p>Having started with our embeddings vectors with each vector representing a word, we then generated a set of query, key and
value vectors. These three vectors are generated separately for however many attention heads we have. In each head we take
the query, key and value vectors produced for that head and calculate a new set of vectors for each word using the attention mechanism. We then stick the vector from each head for
each word together to create one long vector per word. We then multiply by the weights matrix to bring the dimensionality
down resulting in a new set of vectors representing each word in the text, but one where the vectors
represent the relations between the words better than they did in the original embedding vectors. We can then start all
over again and feed these vectors into a new attention layer and calculate attention again, then take the resulting vectors
and calculate it again, and again etc. Calculating multiple layers of attention one after another
is considered to be one of the factors behind GPT models' success. GPT3 is reported to have 96 attention layers.</p>
<p>In GPTs and Transformers multiple layers of Attention are applied to build up a rich
representation of the text that the Transformer is trained on i.e. the vectors
produced by an attention layer are then passed to another attention layer and
attention is calculated again. Although this post has discussed how attention has
calculated it has omitted some things to keep things simple:</p>
<ol>
<li>
<p><strong>How the position of a word in a sentence is captured</strong> As described above nothing
in the calculation of attention depends on the position of a word in the sentence.
In practice an additional vector which represents a word&rsquo;s position in the sentence
is added to the word&rsquo;s original embedding vector at the beginning to capture this. There is more than one
technique to do this.</p>
</li>
<li>
<p><strong>How attention deals with causality</strong> As described above, attention is calculated
between all the words in the sentence, but sometimes that is not appropriate. For
example if you are trying to get the network to generate a new sentence it should not have direct access in its inputs to
the words used to complete a given sentence, you want the network to predict the words to use, not copy an input.
To address this attention is often supplemented with filters (called masks) to change which words
are included in its calculation.</p>
</li>
<li>
<p><strong>How attention mechanisms are implemented in code and put together to create GPTs or translate from one language to
another</strong> This post has covered how attention is calculated, but the discussion of attention has
been conceptual, rather than how it is calculated in practice and then used to do things. For example there are some further
processing stages that are used when the attention mechanism discussed here is included within a neural network.</p>
</li>
</ol>
<p>These will be covered in subsequent posts.</p>
<p><strong>References:</strong></p>
<p>I found the following sources very helpful in writing this:</p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">The original Transformers paper &lsquo;Attention is All You Need&rsquo;,
Vaswani et al. (2017)</a></p>
<p><a href="https://github.com/karpathy/nanoGPT/">The repo for Andrej Karpathy&rsquo;s nanoGPT</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s ‘The illustrated transformer’</a></p>
<p>François Chollet’s book ‘Deep Learning with Python (2nd edition)’</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">OpenAI&rsquo;s, &lsquo;Language Models are Few-Shot Learners&rsquo;</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
      <li><a href="https://johnardavies.github.io/tags/digital/">digital</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
