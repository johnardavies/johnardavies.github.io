<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayes 2: Bayesian inference with Markov Chain Monte Carlo | John&#39;s Site</title>
<meta name="keywords" content="technical" />
<meta name="description" content="View from the Peggy Guggenheim Collection, Venice.
In the previous post we calculated the posterior distribution for a parameter we wanted to estimate. However, in practice, this is often not analytically possible and numerical simulations are needed. Here we evaluate the distribution using a Markov Chain Monte Carlo (MCMC) technique.
The idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain evolves over time the values it generates have the properties of the posterior’s probability distribution.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/mcmc/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Bayes 2: Bayesian inference with Markov Chain Monte Carlo" />
<meta property="og:description" content="View from the Peggy Guggenheim Collection, Venice.
In the previous post we calculated the posterior distribution for a parameter we wanted to estimate. However, in practice, this is often not analytically possible and numerical simulations are needed. Here we evaluate the distribution using a Markov Chain Monte Carlo (MCMC) technique.
The idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain evolves over time the values it generates have the properties of the posterior’s probability distribution." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/mcmc/" />
<meta property="og:image" content="https://johnardavies.github.io/mcmc_title.png" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2024-07-30T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-07-30T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/mcmc_title.png" />
<meta name="twitter:title" content="Bayes 2: Bayesian inference with Markov Chain Monte Carlo"/>
<meta name="twitter:description" content="View from the Peggy Guggenheim Collection, Venice.
In the previous post we calculated the posterior distribution for a parameter we wanted to estimate. However, in practice, this is often not analytically possible and numerical simulations are needed. Here we evaluate the distribution using a Markov Chain Monte Carlo (MCMC) technique.
The idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain evolves over time the values it generates have the properties of the posterior’s probability distribution."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayes 2: Bayesian inference with Markov Chain Monte Carlo",
      "item": "https://johnardavies.github.io/technical/mcmc/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayes 2: Bayesian inference with Markov Chain Monte Carlo",
  "name": "Bayes 2: Bayesian inference with Markov Chain Monte Carlo",
  "description": "View from the Peggy Guggenheim Collection, Venice.\nIn the previous post we calculated the posterior distribution for a parameter we wanted to estimate. However, in practice, this is often not analytically possible and numerical simulations are needed. Here we evaluate the distribution using a Markov Chain Monte Carlo (MCMC) technique.\nThe idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain evolves over time the values it generates have the properties of the posterior’s probability distribution.",
  "keywords": [
    "technical"
  ],
  "articleBody": "View from the Peggy Guggenheim Collection, Venice.\nIn the previous post we calculated the posterior distribution for a parameter we wanted to estimate. However, in practice, this is often not analytically possible and numerical simulations are needed. Here we evaluate the distribution using a Markov Chain Monte Carlo (MCMC) technique.\nThe idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain evolves over time the values it generates have the properties of the posterior’s probability distribution. These techniques have become important in Bayesian inference as they allow complicated probability distributions to be simulated.\nThis post covers:\n1. The Metropolis Hastings MCMC algorithm\n2. Why this works\n3. Evaluating a Bayesian posterior distribution with MCMC\n1.\tThe Metropolis Hastings MCMC algorithm Here we look at the Metropolis Hastings algorithm. The distribution we want to simulate is labelled as p. We have a state x and from this generate a proposed new state x* with the probability q(x*|x). q is a probability distribution for x* which depends on x, known as a proposal distribution. As the chain evolves over time the distribution of x values it generates has the properties of the distribution p. x is indexed by the time period s: x(s).\nThe algorithm to simulate the probability distribution is:\n Randomly select a starting value of x Generate a proposed next value for x: x* from the proposal distribution q where the probability of the proposed x* depends on the existing value: x*=q(x*|x) Compute the probability of accepting the proposal using:\n Calculate the minimum of 1 and alpha: r = min(1,alpha) Decide what the next state is according to the rule:\nif alpha = r reject proposal, stick with exiting state x(s+1) = x(s)\ni.e. switch to the new state with probability alpha or stay with the existing state with probability 1-alpha. Return to 2. and generate a new proposed value from x(s+1)  2.\tWhy this works That if you run this several thousand times it generates a set of x values that have the probability distribution p(x) is, at least to me, not immediately obvious.\nWhat follows is a plausibility argument that the way the probabilities are constructed means that the chain will approach p(x) over time. Here we assume there is a finite number of M states and the probability of state i is labelled p(i). Although the argument is based on a finite number of states in practice a continuous distribution can be approximated by a large number of states.\nIn a Markov chain there is an equilibrium where for any given state the probability of outflow from the state is equal to the probability of inflow. In this situation the distribution over the states tends to be stable over time as the probability of the state growing or shrinking is the same. We want to show that the equilibrium of the Markov chain we have constructed corresponds to the probability distribution p we want to simulate.\nBetween two states i and j the probability of switching from state i to state j under the rules is:\n(the probability state j is proposed) x (probability state j is accepted) i.e: These probabilities summed over all possible states that can be reached from i (including staying in state i) sum to 1: The probability of being in state i and then switching to state j is\n(probability of being in state i) x (probability of switching from state i to j): Similarly, the probability of being in state j and switching to state i is: min(a,b)=min(b,a) and so the probability of entering state i from state j or vice versa is the same and we have an equilibrium for the probability distribution p(i).\nSuppose we start in state x(1)=i generated from the probability distribution with p(i) and calculate the probability of the next state being in state i from one iteration of the chain to the next: Inserting the equilibrium condition and rewriting to look at the second draw from the chain we get: The probability distribution of the second draw having state i, has the desired probability p(i), but this equation looks the same for each iteration of the chain so the third, and the fourth draw etc will also have the probability p(i) and similarly for the M-1 other states. p(i) is therefore the equilibrium distribution of the chain. In small samples the chain will not necessarily give the target probability distribution due to random variation, but as we draw more and more the distribution of the chain’s values should approach the probability distribution we want to simulate.\n3.\tEvaluating a Bayesian posterior distribution with MCMC To see the MCMC algorithm in action we use it to evaluate a simple example: the Gamma posterior produced by the Poisson likelihood and Gamma prior in the last post. This does not need to be simulated as it can be solved analytically, but gives a clear benchmark that we can compare the MCMC simulation against. For other functional forms and complicated situations with many parameters and multi-dimensional priors and likelihoods the posterior’s functional form (or the simpler likelihood multiplied the prior) cannot be directly calculated and therefore the distribution needs to be simulated to evaluate it. Even if you know the posterior distribution, quantities of interest such as its average may not always be readily calculable, but are easy to infer if you have a numerical approximation. The python code to simulate this and its output are show below.\nIn the simulation we use a proposal distribution of a Normal distribution with a mean equal to the current state to generate a proposal for the next one. As the lambda which parameterises the posterior can only take positive values, negative proposal values are converted to positive by multiplying by -1.\nThe charts show the posterior for dfferent sample sizes of simulated data, the corresponding MCMC simulations and the Gamma distribution that corresponds to the MCMC simulation results. The mean and the variance of a Gamma distribution are a function of the two parameters alpha and beta that specify it. This relationship can be reversed to estimate these parameters from the mean and variance of the simulations and hence their associated Gamma distributions. As can be seen in the charts the simulated distribution of the chain’s values (and its corresponding Beta function) match the posterior distribution very closely. This is slightly artificial as we know what the true posterior distribution is, but demonstrates that the technique works.\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy.stats import norm, lognorm from scipy.special import gamma from scipy.stats import gamma as gammadist # Number of samples drawn sample_number = 20000 # Functions that we are using def posterior(lambda_val, alpha, beta, data): \"\"\"function that gives the values of the gamma posterior for different values of alpha, beta and data\"\"\" count_no = len(data) sum_val = sum(data) new_beta = beta + count_no new_alpha = alpha + sum_val numerator = ( (new_beta**new_alpha) * np.exp(-lambda_val * new_beta) * (lambda_val ** (new_alpha - 1)) ) denominator = gamma(new_alpha) val = numerator / denominator return val def mcmc_threshold(x, y, data_sample): \"\"\"Function that generates the MCMC probability threshold for accepting the proposed state x is the current state y is the proposed new state norm.pdf(x,y,100) is the function x of a normal distribution with mean y and variance 100 \"\"\" ratio = (posterior(y, 0.3, 0.1, data_sample) * norm.pdf(x, y, 10)) / ( posterior(x, 0.3, 0.1, data_sample) * norm.pdf(y, x, 10) ) return min(ratio, 1) # The sample size of the datasets data_sets = [0, 3, 5, 25] # Sets up four subplots and the x axis the data will be evaluated over fig, axs = plt.subplots(4, figsize=(12, 15)) x = np.arange(0, 15, 0.1) # For each of the data_sets do a MCMC simulation of the posterior for index, sample_size in enumerate(data_sets): # Create a data frame to hold the results data_store = pd.DataFrame(index=range(sample_number), columns=[\"state_of_chain\"]) # Generate the artifical data to update the likelihood for the different sample sizes if index == 0: values = [] else: values = list(np.random.poisson(6, sample_size)) # Create an initial value to start the chain initial_value = np.random.normal(10, 10) state = initial_value # Run the samples for i in range(0, sample_number): # Generate the proposed state prop_value = np.random.normal(state, 10) # Generate a number between 1 and 0 to simulate probability of acceptance prob_sample = np.random.uniform(0, 1) # Set the proposed value to be positive if it is negative by multiplying by -1 if prop_value References:\nMurphy, K. ‘Machine Learning A probabilistic perspective’, Monte Carlo Inference\nDonovan, T. and Mickey, R. ‘Bayesian Statistics for Beginners’\nEfron, B. and Hastie, T., ‘Computer Age Statistical Inference’\nWilliams, D., ‘A course in Probability and Statistics’\nPrevious posts Bayes 1: Introduction to Bayesian inference\n",
  "wordCount" : "1612",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/mcmc_title.png","datePublished": "2024-07-30T00:00:00Z",
  "dateModified": "2024-07-30T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/mcmc/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Bayes 2: Bayesian inference with Markov Chain Monte Carlo
    </h1>
    <div class="post-meta">July 30, 2024&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/mcmc_title.png" alt="">
        
</figure>
  <div class="post-content"><p>View from the Peggy Guggenheim Collection, Venice.</p>
<p>In the <a href="https://johnardavies.github.io/technical/bayes1/">previous post</a> we calculated the posterior distribution for a parameter we wanted to
estimate. However, in practice, this is often not analytically possible and numerical simulations are needed. Here we evaluate the distribution
using a <strong>Markov Chain Monte Carlo (MCMC)</strong> technique.</p>
<p>The idea is to use a Markov chain to generate samples from the distribution we want to evaluate where as the chain
evolves over time the values it generates have the properties of the posterior’s probability distribution. These techniques have become important in
Bayesian inference as they allow complicated probability distributions to be simulated.</p>
<p>This post covers:</p>
<p><strong>1.  The Metropolis Hastings MCMC algorithm</strong><br>
<strong>2.  Why this works</strong><br>
<strong>3.  Evaluating a Bayesian posterior distribution with MCMC</strong></p>
<h3 id="1the-metropolis-hastings-mcmc-algorithm">1.	The Metropolis Hastings MCMC algorithm<a hidden class="anchor" aria-hidden="true" href="#1the-metropolis-hastings-mcmc-algorithm">#</a></h3>
<p>Here we look at the Metropolis Hastings algorithm. The distribution we want to simulate is labelled as p. We have a state x and from this
generate a proposed new state x* with the probability q(x*|x). q is a probability distribution for x* which depends on x, known as a proposal
distribution. As the chain evolves over time the distribution of x values it generates has the properties of the distribution p. x is indexed
by the time period s: x(s).</p>
<p>The algorithm to simulate the probability distribution is:</p>
<ol>
<li>Randomly select a starting value of x</li>
<li>Generate a proposed next value for x: x* from the proposal distribution q where the probability of the proposed x* depends on the
existing value: x*=q(x*|x)</li>
<li>Compute the probability of accepting the proposal using:<br>
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg1.png" alt="mcmc_alg1"  />
</li>
<li>Calculate the minimum of 1 and alpha: r = min(1,alpha)</li>
<li>Decide what the next state is according to the rule:<br>
if  alpha  &lt;  r  accept the proposal  x(s+1)  =  x*<br>
if  alpha  &gt;= r  reject proposal, stick with exiting state x(s+1)  =  x(s)<br>
i.e. switch to the new state with probability alpha or stay with the existing state with probability 1-alpha.</li>
<li>Return to 2. and generate a new proposed value from x(s+1)</li>
</ol>
<h3 id="2why-this-works">2.	Why this works<a hidden class="anchor" aria-hidden="true" href="#2why-this-works">#</a></h3>
<p>That if you run this several thousand times it generates a set of x values that have the probability distribution p(x) is, at least to me,
not immediately obvious.</p>
<p>What follows is a plausibility argument that the way the probabilities are constructed means that
the chain will approach p(x) over time. Here we assume there is a finite number of M states and the probability of state i is labelled p(i).
Although the argument is based on a finite number of states in practice a continuous distribution can be approximated by a large number of states.</p>
<p>In a Markov chain there is an equilibrium where for any given state the probability of outflow from the state is equal to the probability of
inflow. In this situation the distribution over the states tends to be stable over time as the probability of the state growing or shrinking is the
same. We want to show that the equilibrium of the Markov chain we have constructed corresponds to the probability distribution p we want to simulate.</p>
<p>Between two states i and j the probability of switching from state i to state j under the rules is:<br>
(the probability state j is proposed) x (probability state j is accepted) i.e:
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg2.png" alt="mcmc_alg2"  />

These probabilities summed over all possible states that can be reached from i (including staying in state i) sum to 1:
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg3.png" alt="mcmc_alg3"  />
</p>
<p>The probability of being in state i and then switching to state j is<br>
(probability of being in state i) x (probability of switching from state i to j):
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg4.png" alt="mcmc_alg4"  />
<br>
Similarly, the probability of being in state j and switching to state i is:
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg5.png" alt="mcmc_alg5"  />

min(a,b)=min(b,a) and so the probability of entering state i from state j or vice versa is the same and we have an equilibrium for the probability
distribution p(i).<br>
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg6.png" alt="mcmc_alg6"  />

Suppose we start in state x(1)=i generated from the probability distribution with p(i) and calculate the probability of the next state being in state i from one iteration of the chain to the next:
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg7.png" alt="mcmc_alg7"  />

Inserting the equilibrium condition and rewriting to look at the second draw from the chain we get:
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg8.png" alt="mcmc_alg8"  />

The probability distribution of the second draw having state i, has the desired probability p(i), but this equation looks the same for each iteration
of the chain so the third,
and the fourth draw etc will also have the probability p(i) and similarly for the M-1 other states. p(i) is therefore the equilibrium distribution of the chain. In small samples the chain will not
necessarily give the target probability distribution due to random variation, but as we draw more and more the distribution of the chain&rsquo;s values should approach the probability distribution we want to
simulate.</p>
<h3 id="3evaluating-a-bayesian-posterior-distribution-with-mcmc">3.	Evaluating a Bayesian posterior distribution with MCMC<a hidden class="anchor" aria-hidden="true" href="#3evaluating-a-bayesian-posterior-distribution-with-mcmc">#</a></h3>
<p>To see the MCMC algorithm in action we use it to evaluate a simple example: the Gamma posterior produced by the Poisson likelihood and Gamma prior in
the <a href="https://johnardavies.github.io/technical/bayes1/">last post</a>. This does not need to be simulated as it can be solved analytically, but gives a
clear benchmark that we can compare the MCMC simulation against. For other functional forms and complicated situations with many parameters
and multi-dimensional priors and likelihoods the posterior’s functional form (or the simpler likelihood multiplied the prior) cannot be
directly calculated and therefore the distribution needs to be simulated to evaluate it. Even if you know the posterior distribution, quantities of
interest such as its average may not always be readily calculable, but are easy to infer if you have a numerical approximation. The python code to
simulate this and its output are show below.</p>
<p>In the simulation we use a proposal distribution of a Normal distribution with a mean equal to the current state to generate a proposal for the
next one. As the lambda which parameterises the posterior can only take positive values, negative proposal values are converted
to positive by multiplying by -1.</p>
<p>The charts show the posterior for dfferent sample sizes of simulated data, the corresponding MCMC simulations and the Gamma distribution that
corresponds to the MCMC simulation results. The mean and the variance of a Gamma distribution are a function of the two parameters alpha and beta that specify it. This relationship can be reversed to
estimate these parameters from the mean and variance of the simulations and hence their associated Gamma distributions.
<img loading="lazy" src="https://johnardavies.github.io/mcmc_alg9.png" alt="mcmc_alg9"  />

As can be seen in the charts the simulated distribution of the chain&rsquo;s values (and its corresponding Beta function) match the posterior distribution very closely. This is slightly
artificial as we know what the true posterior distribution is, but demonstrates that the technique works.</p>
<pre tabindex="0"><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm, lognorm
from scipy.special import gamma
from scipy.stats import gamma as gammadist


# Number of samples drawn
sample_number = 20000

# Functions that we are using

def posterior(lambda_val, alpha, beta, data):
    &quot;&quot;&quot;function that gives the values of the gamma posterior for different values of alpha, beta and data&quot;&quot;&quot;

    count_no = len(data)
    sum_val = sum(data)
    new_beta = beta + count_no
    new_alpha = alpha + sum_val

    numerator = (
        (new_beta**new_alpha)
        * np.exp(-lambda_val * new_beta)
        * (lambda_val ** (new_alpha - 1))
    )
    denominator = gamma(new_alpha)

    val = numerator / denominator

    return val


def mcmc_threshold(x, y, data_sample):
    &quot;&quot;&quot;Function that generates the MCMC probability threshold for accepting the proposed state
    x is the current state
    y is the proposed new state
    norm.pdf(x,y,100) is the function x of a normal distribution with mean y and variance 100
    &quot;&quot;&quot;
    ratio = (posterior(y, 0.3, 0.1, data_sample) * norm.pdf(x, y, 10)) / (
        posterior(x, 0.3, 0.1, data_sample) * norm.pdf(y, x, 10)
    )

    return min(ratio, 1)

# The sample size of the datasets
data_sets = [0, 3, 5, 25]

# Sets up four subplots and the x axis the data will be evaluated over
fig, axs = plt.subplots(4, figsize=(12, 15))
x = np.arange(0, 15, 0.1)

# For each of the data_sets do a MCMC simulation of the posterior
for index, sample_size in enumerate(data_sets):

    # Create a data frame to hold the results
    data_store = pd.DataFrame(index=range(sample_number), columns=[&quot;state_of_chain&quot;])
    # Generate the artifical data to update the likelihood for the different sample sizes
    if index == 0:
        values = []
    else:
        values = list(np.random.poisson(6, sample_size))
    # Create an initial value to start the chain
    initial_value = np.random.normal(10, 10)

    state = initial_value
    

    # Run the samples
    for i in range(0, sample_number):
        # Generate the proposed state
        prop_value = np.random.normal(state, 10)

        # Generate a number between 1 and 0 to simulate probability of acceptance
        prob_sample = np.random.uniform(0, 1)
        
        # Set the proposed value to be positive if it is negative by multiplying by -1
        if prop_value &lt;  0:
           prop_value = -prop_value

        # Condition for accepting or rejecting the proposed state
        if prob_sample &lt; mcmc_threshold(state, prop_value, values):
            state = prop_value
        else:
            state = state
        # Storing the value in the dataframe
        data_store[&quot;state_of_chain&quot;].loc[i] = state

    # calculate the mean and the variance of the simulations and from that calculate alpha and beta
    data_mean = data_store[&quot;state_of_chain&quot;].mean()

    data_var = data_store[&quot;state_of_chain&quot;].var()

    alpha_est = (data_mean**2) / data_var
    beta_est = (data_mean) / data_var


    # chart the distribution of the simulations
    axs[index].hist(
        data_store[&quot;state_of_chain&quot;],
        density=True,
        bins=50,
        label=f&quot;mcmc distribution mean={data_mean:.2f}&quot;,
    )
    
    axs[index].set_title(
        f&quot;Distribution of {sample_number} MCMC chain runs based on {sample_size} datapoints&quot;,
        weight=&quot;bold&quot;,
    )
    # Plots the posterior distribution we are simulating 
    axs[index].plot(
        x, posterior(x, 0.3, 0.1, values), color=&quot;yellow&quot;, label=&quot;posterior&quot;
    )
    # Calculate the fitted distribution
    axs[index].plot(
        x,
        gammadist.pdf(x, alpha_est, loc=0, scale=1/beta_est),
        color=&quot;blue&quot;,
        label=&quot;fitted distribution to MCMC&quot;,
    )
    axs[index].legend()

fig.savefig(&quot;mcmc_chart.png&quot;)
</code></pre><p><img loading="lazy" src="https://johnardavies.github.io/mcmc_example.png" alt="mcmc_example"  />
</p>
<p><strong>References:</strong></p>
<p>Murphy, K. ‘Machine Learning A probabilistic perspective’, Monte Carlo Inference</p>
<p>Donovan, T. and Mickey, R. ‘Bayesian Statistics for Beginners’</p>
<p>Efron, B. and Hastie, T., ‘Computer Age Statistical Inference’</p>
<p>Williams, D., ‘A course in Probability and Statistics’</p>
<h3 id="previous-posts">Previous posts<a hidden class="anchor" aria-hidden="true" href="#previous-posts">#</a></h3>
<p><a href="https://johnardavies.github.io/technical/bayes1/">Bayes 1: Introduction to Bayesian inference</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
