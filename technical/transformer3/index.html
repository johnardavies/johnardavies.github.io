<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers 3: Building and training the Transformer | John&#39;s Site</title>
<meta name="keywords" content="technical, digital" />
<meta name="description" content="Printworks
Transformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/transformer3/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Transformers 3: Building and training the Transformer" />
<meta property="og:description" content="Printworks
Transformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/transformer3/" />
<meta property="og:image" content="https://johnardavies.github.io/printworks.jpg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2024-05-18T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-05-18T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/printworks.jpg" />
<meta name="twitter:title" content="Transformers 3: Building and training the Transformer"/>
<meta name="twitter:description" content="Printworks
Transformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers 3: Building and training the Transformer",
      "item": "https://johnardavies.github.io/technical/transformer3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers 3: Building and training the Transformer",
  "name": "Transformers 3: Building and training the Transformer",
  "description": "Printworks\nTransformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet\u0026rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.",
  "keywords": [
    "technical", "digital"
  ],
  "articleBody": "Printworks\nTransformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet’s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow. This note will cover:\n1. Processing the text data\n2. The implementation of attention\n3. The implementation of other Transformer components\n3. Putting all the components together to create a Transformer\n4. Training\n5. Generation of translations\nTo start we will initialise the model with a PyTorch dataclass (config.py) that sets out the model’s key parameters:\nfrom dataclasses import dataclass @dataclass class TransformerConfig: \"\"\"With a dataclass we don't need to write an init function, just specify class attributes and their types\"\"\" block_size: int = 20 # Was 20 vocab_size: int = 15000 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency dim_embedding: int = 600 dim_inner_layer: int = 2000 n_head: int = 6 dropout: float = 0.2 bias: bool = False epoch: int = 30 batch_size: int = 30 config = TransformerConfig() The model will have an embedding dimension of 600 and a vocabulary size of 15000 words for each language. Attention will be calculated over 6 heads in parallel, which due to how it is implemented involves 100 dimensional query, key and value vectors. During training German text and a corresponding English translation will be input in batches of 30 text-pairs. Both the German text and English translation text are capped at 20 tokens in length. In training we’ll loop through the training data 30 times (30 epochs). 20% dropout is implemented at a number of stages of the network.\n1. Processing the text data\nTo get some text data to train and test the Transformer we download the english-german translation pairs from anki and unzip the file:\n$ wget https://www.manythings.org/anki/deu-eng.zip \u0026\u0026 unzip deu-eng.zip This produces the text file (deu.txt) which consists of pairs of English and German text, for example:\nBless you.\tGesundheit\nBuy a gun.\tLeg dir eine Schusswaffe\nWe want to encode these into numbers (for both English and German). We will process this with the python script text_processing.py. To do this we’ll start by processing the pairs by removing punctuation, setting the text to lowercase and adding [start] and [end] tags at the beginning and end of all of the English texts.\n# Strip punctuation def strip_punctuation(s): \"\"\"Function that removes punctuation\"\"\" return str(''.join(c for c in s if c not in punctuation)) # Split the text file into german and english # We start each line of the english text with start and end it with end text_file = \"lang/deu.txt\" with open(text_file) as language_file: lines = language_file.read().split(\"\\n\") print(\"There are \" + str(len(lines)) + \" lines\") text_pairs = [] for i, line in enumerate(lines): try: english, german = line.split(\"\\t\")[0:2] english = \"[start] \" + strip_punctuation(english.lower()) + \" [end]\" text_pairs.append([strip_punctuation(german.lower()), english]) except: print(\"failed to load %s\" % (i)) We then calculate the distinct words used in all the German and all the English terms. We then create a class EncodeDecode which is initiated for each language separately and has two methods that are dictionaries allowing the most common 15,000 words in each language to be encoded into numbers and decoded back. More sophisticated encoding based on parts of speech or combinations of words are also possible.\n# To get the tokens used in English and German text we create # two separate lists and from these create a single german # text and a single english text german_list = [item[0] for item in text_pairs] english_list = [item[1] for item in text_pairs] german_text=' '.join(german_list) english_text=' '.join(english_list) class EncodeDecode(): \"\"\"Class that produces two dictionaries from input text, one of which maps words to numbers and the other one maps numbers to words.\"\"\" def __init__(self, text, vocab): self.text = text # Get the tokens self.tokens_list = list(set(self.text.split())) self.vocab_size=vocab # Get the most common tokens self.tokens_counter=Counter(self.text.split()).most_common(self.vocab_size) self.tokens_vocab=[item for item, count in self.tokens_counter] def decode(self): # Create a decoder from the numbers to the tokens decoder = {i+1: token for i, token in enumerate(self.tokens_vocab)} return decoder def encode(self): # Create an encoder of the tokens to numbers encoder = {token: i+1 for i, token in enumerate(self.tokens_vocab)} return encoder # Initialise the english and german classes english_tokens=EncodeDecode(english_text, 14999) german_tokens=EncodeDecode(german_text, 14999) We then apply the dictionaries to encode the English and German texts into numbers. The english translation text is then processed into two copies. One which has the [end] token removed which will be input for training and one which has the start token removed which is used as the target text the model is trying to predict. Each German and English text is padded with 0s to get to 20 tokens. The encoder and decoder calculations above start at 1 in order to keep 0 for the padding.\n %%time # Encode the tokens if they are in the 15000 most common tokens text_pairs_encoded = [ [ [ german_encoded[element] for element in pair[0].split() if element in german_tokens.tokens_vocab ], [ english_encoded[element] for element in pair[1].split() if element in english_tokens.tokens_vocab ] ] for pair in text_pairs ] # Take the encoded text pairs and move the english translation one element to the right text_pairs_encoded_split = [ (elem[0], elem[1][:-1], elem[1][1:]) for elem in text_pairs_encoded ] # We pad the data to feed the data into a network in a consistent size def tensor_pad(x): \"\"\"converts list to tensor and pads to length 20\"\"\" return torch.nn.functional.pad(torch.tensor(x, dtype=torch.int64), (0, 20)) # pads all of the tensors so that they are of size 20 text_pairs_encoded_padded = [ [ tensor_pad(item1)[:config.block_size], tensor_pad(item2)[:config.block_size], tensor_pad(item3)[:config.block_size], ] for item1, item2, item3 in text_pairs_encoded_split ] Having done that we split the data into a 20% test and 80% training sample.\n# Calculate how many observations are needed for a 20% test sample test_len = round(len(text_pairs_encoded_padded) * 0.2) # Calculate the number of training observations as residual train_len = round(len(text_pairs_encoded_padded)) - test_len # Split the data into training and test datasets train_dataset, test_dataset = random_split( text_pairs_encoded_padded, [train_len, test_len] ) # Lastly we create a dataloader to iterate through the data config.batch_size train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True) 2. The implementation of attention in PyTorch The following code forms part of the transformers script Transformer.py which is the core model script. Below is the implementation of attention that Andrej Karpathy used in nanoGPT. As with all PyTorch layers it sets up the parts that are used to construct the network when it is initiated. The forward method then details what happens when the data is input into an initialised attention layer. All layer classes inherit from the nn.Module class which is the class for all neural network modules.\nclass CausalSelfAttention(nn.Module): def __init__(self, config): super().__init__() assert config.dim_embedding % config.n_head == 0 # key, query, value projections for all heads, but in a batc # nn.Linear applies a linear y=Ax +b transformation to the input # the input dimension is the first argument # the output dimension is the second argument # the last argument is the b (bias) term self.c_attn = nn.Linear(config.dim_embedding, 3 * config.dim_embedding, bias=config.bias) # output projection self.c_proj = nn.Linear(config.dim_embedding, config.dim_embedding, bias=config.bias) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.dropout = config.dropout # flash attention make GPU go brrrrr but support is only in PyTorch = 2.0 self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') if not self.flash: print(\"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\") # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)) .view(1, 1, config.block_size, config.block_size)) def forward(self, x): # Split the input tensor dimension down into the batch size B , the sequence length T and the emnedding dimensionality C B, T, C = x.size() # batch size, sequence length, embedding dimensionality (dim_embedding) # print(B, T, C) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False) else: # manual implementation of attention # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Calculate the masked attention # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # Apply the softmax layer so that everything sums to 1 att = F.softmax(att, dim=-1) att = self.attn_dropout(att) # Multiply the attention results by the value vectors y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) # Change the shape of the tensor back to B, T, C removing the heads y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) return y This shows creation of key, query and value vectors from the embeddings, which is implemented by passing the embedding vector into c_attn(x) which generates these three vectors. These vectors are then split again by the number of heads to perform the attention calculation. The vectors for each of the query key and value vectors have 600 dimesions and we have 6 heads, so we end up with 6 sets of 100 dimensional vectors for each of the original query, key and value vectors.\nq, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs The method contains an optimised attention mechanism that uses flash attention to process these collections of vectors. Here we have is_causal=True as this is the encoder side and in translation we assume that the Transformer has access to the full German text before starting translation. By contrast on the encoder side is_causal=False when processing the English translations as in generating the prediction this is done recursively with the prediction for the next word of the translation based on the predicted words before. In the training data thereefore, to predict the n’th word of the translation access is given to the n-1 English words before. For the very first translated word the [start] token is used to initalise the translation.\n y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True) If flash attention is not available there is also a version that implements attention directly in Python. This illustrates more explicitly the components that make up attention. The matrix multiplication of query and the transposed key matrix and then normalising the results by the square root of the key vector’s dimensionality:\natt = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) The application of the softmax layer:\natt = F.softmax(att, dim=-1) The multiplication of the attention weights by the values vectors:\n y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) The concatenation of the head outputs and then the projection of resulting vector down to the model’s embedding dimension. In the Karpathy implementation he then adds a dropout layer.\n y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) 3. The implementation of other Transformer components\nThe Embedding class below takes in the encoded text and then passes each word to two embedding layers. There is the word embedding layer (wte) and the position embedding layer (wtp). These map the word and its position into two vectors of the chosen embedding dimensionality. These two vectors are then added together encoding both the word and its position in the text in a single vector. This is done for all of the 20 words/tokens in a piece of text fed into the network, and their relative position in the text.\nclass Embedding(nn.Module): def __init__(self, config): super().__init__() # Creates the text embedding and the position embedding # The embedding layer automatically does the one hot encoding so this # does not need to be created directly self.config = config self.wte = nn.Embedding(15000, config.dim_embedding) self.wtp = nn.Embedding(20, config.dim_embedding) def forward(self, x): # Applies the word embedding and then adds it to the position embedding x = self.wte(x) # The position embedding is applied over a tensor that ranges 0 to 20 representing each word's position x = x + self.wtp(torch.arange(0, 20)) return x The LayerNorm class applies layer normalisation, calculating the mean and standard deviation across the batch for each position in the text and elements of the vector per word and then subtracting these means and dividing by the standard deviation from the corresponding elements across the batch.\nclass LayerNorm(nn.Module): \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\" def __init__(self, ndim, bias): super().__init__() self.weight = nn.Parameter(torch.ones(ndim)) self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None def forward(self, input): return Fun.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5) A series of layers that implements a layer norm, a dense layer with a skip connection and then a dense layer again:\nclass Processing_layer(nn.Module): def __init__(self, config): super().__init__() self.denselayer1 = nn.Linear(config.dim_embedding, config.dim_embedding) self.denselayer2 = nn.Linear(config.dim_embedding, config.dim_embedding) self.layernorm1 = LayerNorm(config.dim_embedding, bias=config.bias) self.layernorm2 = LayerNorm(config.dim_embedding, bias=config.bias) def forward(self, x): # A layer norm and then two dense layers\\n\", x_in = self.layernorm1(x) x = self.denselayer1(x_in) x = self.denselayer2(x)+x_in x = self.layernorm2(x) return x There is also a class that implements masked causal self attention:\nclass CausalSelfAttentionMasked(nn.Module): def __init__(self, config): super().__init__() assert config.dim_embedding % config.n_head == 0 # key, query, value projections for all heads, but in a batc # nn.Linear applies a linear y=Ax +b transformation to the input # the input dimension is the first argument # the output dimension is the second argument # the last argument is the b (bias) term self.c_attn = nn.Linear(config.dim_embedding, 3 * config.dim_embedding, bias=config.bias) # output projection self.c_proj = nn.Linear(config.dim_embedding, config.dim_embedding, bias=config.bias) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.dropout = config.dropout # flash attention make GPU go brrrrr but support is only in PyTorch = 2.0 self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') if not self.flash: print(\"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\") # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)) .view(1, 1, config.block_size, config.block_size)) def forward(self, x): # Split the input tensor dimension down into the batch size B , the sequence length T and the emnedding dimensionality G B, T, C = x.size() # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True) else: # manual implementation of attention # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Calculate the masked attention att = att.masked_fill(attn_mask == 0, float('-inf')) # Apply the softmax layer so that everything sums to 1 att = F.softmax(att, dim=-1) att = self.attn_dropout(att) # Multiply the attention results by the value vectors y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) # Change the shape of the tensor back to B, T, C removing the heads y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) return y The Decoder has two inputs: the output from the encoder and the output from the previous layer of the decoder’s masked self-attention class. The encoder output is split into key and value vectors. These vectors are then combined with the query vectors created from the vectors output from the decoder’s previous layer in an attention calculation.\nclass EncoderDecoderAttention(nn.Module): def __init__(self, config): super().__init__() assert config.dim_embedding % config.n_head == 0 # key, query, value projections for all heads, but in a batch self.c_attn_en = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.dropout = config.dropout # flash attention make GPU go brrrrr but support is only in PyTorch = 2.0 self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer( \"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view( 1, 1, config.block_size, config.block_size ), ) def forward(self, x, e): # Generating the query and key vectors from the vectors from the encoder ( B, T, C, ) = ( e.size() ) # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension _, k, v = self.c_attn_en(e).split(self.dim_embedding, dim=2) k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # print(q.size(), k.size()) # Generating the value vector from the decoder input ( B, T, C, ) = ( x.size() ) # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate values for all heads in batch and move head forward to be the batch dim q, _, _ = self.c_attn(x).split(self.dim_embedding, dim=2) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) - (B, nh, T, T) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False, ) else: # manual implementation of attention att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = att.masked_fill(attn_mask == 0, float(\"-inf\")) att = F.softmax(att, dim=-1) att = self.attn_dropout(att) y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) return y 3. Putting all the components together to create a Transformer\nWe now combine all of these components together into a single Transformer class which we will initialise and then train.\nclass Encoder(nn.Module): def __init__(self, config): super().__init__() assert config.vocab_size is not None assert config.block_size is not None self.config = config # Create embeddings for both the encoder and the decoder self.encoder_embed = Embedding(config) # Create an attention mechanism for the encoder self.attention_encoder = CausalSelfAttention(config) # Set up a processing layer self.encoder_processing_layer = Processing_layer(config) def forward(self, x): # Encode the language we want to translate from x = self.encoder_embed(x) # Apply the attention mechanism and add the input x = self.attention_encoder(x) + x # apply layer norm, two dense layers and a layer norm again x = self.encoder_processing_layer(x) return x class Decoder(nn.Module): def __init__(self, config): super().__init__() assert config.vocab_size is not None assert config.block_size is not None self.config = config # Create embeddings for both the encoder and the decoder self.decoder_embed = Embedding(config) # Create an attention mechanism for the decoder self.attention_decoder = CausalSelfAttentionMasked(config) # Create a layernorm layer self.layernorm = LayerNorm(config.dim_embedding, bias=config.bias) # Create the encoder decoder attention self.decoder_attn = EncoderDecoderAttention(config) # Set up a processing sections one for the encoder and the other for the decoder self.decoder_processing_layer = Processing_layer(config) # The final layer which maps the models embedding dimension batch to the vocab size self.finallayer = nn.Linear(config.dim_embedding, config.vocab_size+1) def forward(self, x, y): # Encode the language we want to translate into y = self.decoder_embed(y) # Apply the attention mechanism and add the input y = self.attention_decoder(y) + y y = self.layernorm(y) # Take the ouput from the encoder and last layer of decoder and calculate attention again then add the input y = self.decoder_attn(y, x) + y # apply layer norm, two dense layers and a layer norm again y = self.decoder_processing_layer(y) y = self.finallayer(y) return y class Transformer(nn.Module): def __init__(self, config): super().__init__() assert config.vocab_size is not None assert config.block_size is not None self.config = config # Create embeddings for both the encoder and the decoder self.encoder = Encoder(config) self.decoder = Decoder(config) def forward(self, x, y): # Encode the language we want to translate from encoder_out = self.encoder(x) # Apply the attention mechanism and add the input y = self.decoder(encoder_out,y) return y # instantiate the Transformer with the config file model = Transformer(config) 4. Training The code below is from the python script train.py trains the model for 30 epochs. It minimises the cross entropy between the predicted translations and the actual translations.\n%%time losses = [] torch.set_grad_enabled(True) #optimizer = optim.Adam(model2.parameters(), lr=1e-3) optimizer = optim.SGD(model4.parameters(), lr=0.01, momentum=0.9) for epoch in range(0, 30): for i, (german, english, output) in enumerate(train_dataloader): model.train() yhat = model(german, english) loss = Fun.cross_entropy( yhat.view(-1, yhat.size(-1)), output.view(-1), ignore_index=-1 ) loss.backward() optimizer.step() # clear the gradients optimizer.zero_grad() losses.append(loss) if i % 400 == 0: print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\") 5. Generation of translations\nThe function below generates a translation from the input text. This is included in the script translate.py. It takes as an input the sentence we want to translate, it then:\n encodes that text into integers using the coding developed in the text processing phase. adds an extra dimension with the unsqueeze(0) so that the model which is trained on batches of 30, can read a single sentence rather than needing a batched input of multiple texts. it then generates the translation one token at a time. it starts with a blank decoding sentence and the encoded text we want the trained model to translate. The model takes these inputs and generates a probability distribution for its prediction of the first word of the translation. the model then samples from the probability distribution of the next word i.e. the word with highest predicted probability is most likely to be chosen. the predicted word is then added to the decoded sentence as its first word. the original sentence and the updated decoded sentence is then input into the model to get the next word. this is done recursively until we predict the token [end] and the translation ends.  def prediction(x,y): logits=model(x,y) logits=logits.squeeze(0) # Will remove the first dimension if it is set to 0 # returns a tensor with all specified dimensions of input of size 1 removed - in this case the first dimension # The dim = -1 applies softmax over the last dimension of the tensor return Fun.softmax(logits, dim=-1) Functions that encode strings of text as integers according to the encoding of the languages developed earlier:\ndef target_vectorization(x): return [english_encoded[element] for element in x.split() if element in english_tokens.tokens_vocab] def source_vectorization(x): return [german_encoded[element] for element in x.split() if element in german_tokens.tokens_vocab] def decode_sequence_no_print(input_sentence): # Unsqueezing adds an extra dimension so that the tensor is of size torch.Size([1, 20]) tokenized_input_sentence = tensor_pad(source_vectorization(input_sentence))[ :20 ].unsqueeze(0) decoded_sentence = \"[start]\" # Loop through the sentence word by word for i in range(config.block_size): tokenized_target_sentence = target_vectorization(decoded_sentence) if i= 0: tokenized_target_sentence=tensor_pad(tokenized_target_sentence)[:20].unsqueeze(0) predictions = prediction(tokenized_input_sentence, tokenized_target_sentence) # The second index is the word position, the third index are the words # The .item() extracts the tensor index from the tensor probs=torch.multinomial(predictions[i,:], num_samples=1) sampled_token_index = predictions[i, :].argmax().item() sampled_token = english_tokens.decode()[sampled_token_index] decoded_sentence += \" \" + sampled_token # If the predicted token is end stop if sampled_token == \"[end]\": break return decoded_sentence The code below loops through the test data and generates predictions:\nclass style: BOLD = '\\033[1m' UNDERLINE = '\\033[4m' END = '\\033[0m' # Looping through the test dataset to generate some translations: for i, elem in enumerate(test_dataloader): if i % 1000 == 0: print(style.BOLD+'Orginal'+style.END) german=trans(elem[0].tolist()[0], 'ger') print(german) print(style.BOLD+'Translation'+style.END) print(trans(elem[1].tolist()[0], 'eng')) print(style.BOLD+'Machine Translation'+style.END) print(decode_sequence_no_print(german)) print('\\n') References:\nThe original Transformers paper ‘Attention is All You Need’, Vaswani et al. (2017)\nThe repo for Andrej Karpathy’s nanoGPT\nJay Alammar’s ‘The illustrated transformer’\nFrançois Chollet’s book ‘Deep Learning with Python (2nd edition)’\nOpenAI’s, ‘Language Models are Few-Shot Learners’\nPrevious Transformer notes:\n Transformers 1: The Attention Mechanism Transformers 2: The Transformer’s Structure  ",
  "wordCount" : "4213",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/printworks.jpg","datePublished": "2024-05-18T00:00:00Z",
  "dateModified": "2024-05-18T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/transformer3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Transformers 3: Building and training the Transformer
    </h1>
    <div class="post-meta">May 18, 2024&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/printworks.jpg" alt="">
        
</figure>
  <div class="post-content"><p>Printworks</p>
<h3 id="transformers-3-building-and-training-the-transformer">Transformers 3: Building and training the Transformer<a hidden class="anchor" aria-hidden="true" href="#transformers-3-building-and-training-the-transformer">#</a></h3>
<p>Having discussed <a href="https://johnardavies.github.io/technical/transformer2/">how attention works</a> and <a href="https://johnardavies.github.io/technical/transformer2/">the structure of
Transformers</a> we’ll now implement a simple Transformer that takes German text and generates English translations. To do this
we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement
a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s
book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.
This note will cover:</p>
<p><strong>1. Processing the text data</strong><br>
<strong>2. The implementation of attention</strong><br>
<strong>3. The implementation of other Transformer components</strong><br>
<strong>3. Putting all the components together to create a Transformer</strong><br>
<strong>4. Training</strong><br>
<strong>5. Generation of translations</strong></p>
<p>To start we will initialise the model with a PyTorch dataclass (<code>config.py</code>) that sets out the model&rsquo;s key parameters:</p>
<pre tabindex="0"><code>from dataclasses import dataclass

@dataclass
class TransformerConfig:

    &quot;&quot;&quot;With a dataclass we don't need to write an init function, just specify class attributes and their types&quot;&quot;&quot;

    block_size: int = 20  # Was 20
    vocab_size: int = 15000  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    dim_embedding: int = 600
    dim_inner_layer: int =  2000
    n_head: int = 6

    dropout: float = 0.2
    bias: bool = False
    epoch: int = 30
    batch_size: int = 30


config = TransformerConfig()
</code></pre><p>The model will have an embedding dimension of 600 and a vocabulary size of 15000 words for each language. Attention will be calculated over 6 heads in parallel, which due to how it is implemented involves 100 dimensional query,
key and value vectors. During training German text and a corresponding English translation
will be input in batches of 30 text-pairs. Both the German text and English translation text are capped at 20 tokens in length. In training we&rsquo;ll loop through the training data 30 times (30 epochs).
20% dropout is implemented at a number of stages of the network.</p>
<p><strong>1. Processing the text data</strong></p>
<p>To get some text data to train and test the Transformer we download the english-german translation pairs from
<a href="https://www.manythings.org/anki/">anki</a> and unzip the file:</p>
<pre tabindex="0"><code>$ wget https://www.manythings.org/anki/deu-eng.zip &amp;&amp; unzip deu-eng.zip
</code></pre><p>This produces the text file (deu.txt) which consists of pairs of English and German text, for example:<br>
<code>Bless you.	Gesundheit</code><br>
<code>Buy a gun.	Leg dir eine Schusswaffe</code></p>
<p>We want to encode these into numbers (for both English and German). We will process this with the python script <code>text_processing.py</code>.
To do this we&rsquo;ll start by processing the pairs by removing punctuation, setting the text to lowercase and adding [start] and [end]
tags at the beginning and end of all of the English texts.</p>
<pre tabindex="0"><code># Strip punctuation
def strip_punctuation(s):
    &quot;&quot;&quot;Function that removes punctuation&quot;&quot;&quot;
    return str(''.join(c for c in s if c not in punctuation))

# Split the text file into german and english
# We start each line of the english text with start and end it with end

text_file = &quot;lang/deu.txt&quot;
with open(text_file) as language_file:
    lines = language_file.read().split(&quot;\n&quot;)
    print(&quot;There are &quot; + str(len(lines)) + &quot; lines&quot;)

text_pairs = []
for i, line in enumerate(lines):
    try:
        english, german = line.split(&quot;\t&quot;)[0:2] 
        english = &quot;[start] &quot; + strip_punctuation(english.lower()) + &quot; [end]&quot;
        text_pairs.append([strip_punctuation(german.lower()), english])
    except: 
        print(&quot;failed to load %s&quot; % (i))
</code></pre><p>We then calculate the distinct words used in all the German and all the English terms.
We then create a class EncodeDecode which is initiated for each language separately
and has two methods that are dictionaries allowing the most common 15,000 words in each language to be encoded into numbers and decoded back.
More sophisticated encoding based on parts of speech or combinations of words are also possible.</p>
<pre tabindex="0"><code># To get the tokens used in English and German text we create
# two separate lists and from these create a single german 
# text and a single english text
german_list = [item[0] for item in text_pairs]
english_list = [item[1] for item in text_pairs]


german_text=' '.join(german_list)
english_text=' '.join(english_list)


class EncodeDecode():
    &quot;&quot;&quot;Class that produces two dictionaries from input text, one of which maps words
    to numbers and the other one maps numbers to words.&quot;&quot;&quot;

    def __init__(self, text, vocab):
        self.text = text
        # Get the tokens
        self.tokens_list = list(set(self.text.split()))
        self.vocab_size=vocab
        # Get the most common tokens
        self.tokens_counter=Counter(self.text.split()).most_common(self.vocab_size)
        self.tokens_vocab=[item for item, count in self.tokens_counter]
      

    def decode(self):
        # Create a decoder from the numbers to the tokens
        decoder = {i+1: token for i, token in enumerate(self.tokens_vocab)}
        return decoder

    def encode(self):
        # Create an encoder of the tokens to numbers
        encoder = {token: i+1 for i, token in enumerate(self.tokens_vocab)}
        return encoder

# Initialise the english and german classes
english_tokens=EncodeDecode(english_text, 14999)
german_tokens=EncodeDecode(german_text, 14999)
</code></pre><p>We then apply the dictionaries to encode the English and German texts into numbers.
The english translation text is then processed into two copies. One which has the [end] token removed
which will be input for training and one which has the start token removed which is used as the target text the model
is trying to predict. Each German and English text is padded with 0s to get to 20 tokens. The encoder and decoder
calculations above start at 1 in order to keep 0 for the padding.</p>
<pre tabindex="0"><code>
%%time
# Encode the tokens if they are in the 15000 most common tokens

text_pairs_encoded = [
    [
        [
            german_encoded[element]
            for element in pair[0].split()
            if element in german_tokens.tokens_vocab
        ],
        [
            english_encoded[element]
            for element in pair[1].split()
            if element in english_tokens.tokens_vocab
        ]
    ]
    for pair in text_pairs
]


# Take the encoded text pairs and move the english translation one element to the right
text_pairs_encoded_split = [
    (elem[0], elem[1][:-1], elem[1][1:]) for elem in text_pairs_encoded
]

# We pad the data to feed the data into a network in a consistent size
def tensor_pad(x):
    &quot;&quot;&quot;converts list to tensor and pads to length 20&quot;&quot;&quot;
    return torch.nn.functional.pad(torch.tensor(x, dtype=torch.int64), (0, 20))

# pads all of the tensors so that they are of size 20
text_pairs_encoded_padded = [
    [
        tensor_pad(item1)[:config.block_size],
        tensor_pad(item2)[:config.block_size],
        tensor_pad(item3)[:config.block_size],
    ]
    for item1, item2, item3 in text_pairs_encoded_split
]

</code></pre><p>Having done that we split the data into  a 20% test and 80% training sample.</p>
<pre tabindex="0"><code># Calculate how many observations are needed for a 20% test sample
test_len = round(len(text_pairs_encoded_padded) * 0.2)

# Calculate the number of training observations as residual
train_len = round(len(text_pairs_encoded_padded)) - test_len

# Split the data into training and test datasets
train_dataset, test_dataset = random_split(
    text_pairs_encoded_padded, [train_len, test_len]
)

# Lastly we create a dataloader to iterate through the data config.batch_size
train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)

</code></pre><p><strong>2. The implementation of attention in PyTorch</strong>
The following code forms part of the transformers script <code>Transformer.py</code> which is the core model script.
Below is the implementation of attention that Andrej Karpathy used in nanoGPT. As with all PyTorch layers it sets up the parts that
are used to construct the network when it is initiated. The forward method then details what happens when the data is input into an
initialised attention layer. All layer classes inherit from the nn.Module class which is the class for all neural network modules.</p>
<pre tabindex="0"><code>class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.dim_embedding % config.n_head == 0
        # key, query, value projections for all heads, but in a batc
        # nn.Linear applies a linear y=Ax +b transformation to the input
        # the input dimension is the first argument
        # the output dimension is the second argument
        # the last argument is the b (bias) term 
        self.c_attn = nn.Linear(config.dim_embedding, 3 * config.dim_embedding, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.dim_embedding, config.dim_embedding, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print(&quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;)
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(&quot;bias&quot;, torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        # Split the input tensor dimension down into the batch size B , the sequence length T and the emnedding dimensionality C
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (dim_embedding)
       # print(B, T, C)
        # calculate query, key, values 
        # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension
        q, k, v  = self.c_attn(x).split(self.dim_embedding, dim=2)
        # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training 
else 0, is_causal=False)
        else:
            # manual implementation of attention
            # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            # Calculate the masked attention
         #   att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
            # Apply the softmax layer so that everything sums to 1
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            # Multiply the attention results by the value vectors
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
            # Change the shape of the tensor back to B, T, C removing the heads
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

</code></pre><p>This shows creation of key, query and value vectors from the embeddings, which is implemented by passing the embedding vector into
c_attn(x) which generates these three vectors. These vectors are then split again by the number of heads to perform the attention calculation.
The vectors for each of the query key and value vectors have 600 dimesions and we have 6 heads, so we end up with 6 sets of 100 dimensional vectors for each of the original query, key
and value vectors.</p>
<pre tabindex="0"><code>q, k, v  = self.c_attn(x).split(self.dim_embedding, dim=2)
# split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs
</code></pre><p>The method contains an optimised attention mechanism that uses flash attention to process these collections of vectors. Here we have <code>is_causal=True</code> as this is the encoder side and in
translation we assume that the Transformer has access to the full German text before starting translation. By contrast on the encoder side <code>is_causal=False</code> when processing the
English translations as in generating the prediction this is done recursively with the prediction for the next word of the translation based on the predicted words before. In the
training data thereefore, to predict the n&rsquo;th
word of the translation access is given to the n-1 English words before. For the very first translated word the <code>[start]</code> token is used to initalise the translation.</p>
<pre tabindex="0"><code>            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
</code></pre><p>If flash attention is not available there is also a version that implements attention
directly in Python. This illustrates more explicitly the components that make up attention.
The matrix multiplication of query and the transposed key matrix and then normalising the results by the square root of the key
vector&rsquo;s dimensionality:</p>
<pre tabindex="0"><code>att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
</code></pre><p>The application of the softmax layer:</p>
<pre tabindex="0"><code>att = F.softmax(att, dim=-1)
</code></pre><p>The multiplication of the attention weights by the values vectors:</p>
<pre tabindex="0"><code> y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
</code></pre><p>The concatenation of the head outputs and then the projection of resulting vector down to the model&rsquo;s embedding dimension. In the Karpathy implementation he then adds a dropout layer.</p>
<pre tabindex="0"><code> y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

# output projection
 y = self.resid_dropout(self.c_proj(y))
</code></pre><p><strong>3. The implementation of other Transformer components</strong></p>
<p>The Embedding class below takes in the encoded text and then passes each word to two embedding layers. There is the word embedding layer (wte) and the position embedding layer (wtp). These map the word and its position into two vectors of the chosen embedding
dimensionality. These two vectors are then added together encoding both the word and its position in the text in a single vector. This is done for all of the 20 words/tokens in a piece of text fed into the network, and their relative position in the text.</p>
<pre tabindex="0"><code>class Embedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        #  Creates the text embedding and the position embedding
        # The embedding layer automatically does the one hot encoding so this
        # does not need to be created directly
        self.config = config
        self.wte = nn.Embedding(15000, config.dim_embedding)
        self.wtp = nn.Embedding(20, config.dim_embedding)
  

    def forward(self, x):
        # Applies the word embedding and then adds it to the position embedding
        x = self.wte(x)
        # The position embedding is applied over a tensor that ranges 0 to 20 representing each word's position
        x = x + self.wtp(torch.arange(0, 20))
        return x
</code></pre><p>The LayerNorm class applies layer normalisation, calculating the mean and standard deviation across the batch for each position in the text and elements of the vector
per word and then subtracting these means and dividing by the standard deviation from the corresponding elements across the batch.</p>
<pre tabindex="0"><code>class LayerNorm(nn.Module):
    &quot;&quot;&quot; LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False &quot;&quot;&quot;

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return Fun.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
</code></pre><p>A series of layers that implements a layer norm, a dense layer with a skip connection and then a dense layer again:</p>
<pre tabindex="0"><code>class Processing_layer(nn.Module):
      def __init__(self, config):
            super().__init__()
            self.denselayer1 = nn.Linear(config.dim_embedding, config.dim_embedding)
            self.denselayer2 = nn.Linear(config.dim_embedding, config.dim_embedding)
            self.layernorm1 = LayerNorm(config.dim_embedding, bias=config.bias)
            self.layernorm2 = LayerNorm(config.dim_embedding, bias=config.bias)

      def forward(self, x):
             # A layer norm and then two dense layers\n&quot;,
             x_in = self.layernorm1(x)
             x = self.denselayer1(x_in)
             x = self.denselayer2(x)+x_in
             x = self.layernorm2(x)
             return x

</code></pre><p>There is also a class that implements masked causal self attention:</p>
<pre tabindex="0"><code>class CausalSelfAttentionMasked(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.dim_embedding % config.n_head == 0
        # key, query, value projections for all heads, but in a batc
        # nn.Linear applies a linear y=Ax +b transformation to the input
        # the input dimension is the first argument
        # the output dimension is the second argument
        # the last argument is the b (bias) term 
        self.c_attn = nn.Linear(config.dim_embedding, 3 * config.dim_embedding, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.dim_embedding, config.dim_embedding, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print(&quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;)
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(&quot;bias&quot;, torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        # Split the input tensor dimension down into the batch size B , the sequence length T and the emnedding dimensionality G
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (dim_embedding)
        # calculate query, key, values 
        # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension
        q, k, v  = self.c_attn(x).split(self.dim_embedding, dim=2)
        # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
        else:
            # manual implementation of attention
            # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            # Calculate the masked attention
            att = att.masked_fill(attn_mask == 0, float('-inf'))
            # Apply the softmax layer so that everything sums to 1
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            # Multiply the attention results by the value vectors
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
            # Change the shape of the tensor back to B, T, C removing the heads
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y
</code></pre><p>The Decoder has two inputs: the output from the encoder and the output from the previous layer of the decoder&rsquo;s masked self-attention class. The encoder output is split into
key and value vectors. These vectors are then combined with the query vectors created from the vectors output from the decoder&rsquo;s previous layer in an attention calculation.</p>
<pre tabindex="0"><code>class EncoderDecoderAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.dim_embedding % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn_en = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )

        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding

        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(
                &quot;bias&quot;,
                torch.tril(torch.ones(config.block_size, config.block_size)).view(
                    1, 1, config.block_size, config.block_size
                ),
            )

    def forward(self, x, e):

        # Generating the query and key vectors from the vectors from the encoder
        (
            B,
            T,
            C,
        ) = (
            e.size()
        )  # batch size, sequence length, embedding dimensionality (dim_embedding)
        # calculate query, key, values
        # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension
        _, k, v = self.c_attn_en(e).split(self.dim_embedding, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        #   print(q.size(), k.size())

        # Generating the value vector from  the decoder input
        (
            B,
            T,
            C,
        ) = (
            x.size()
        )  # batch size, sequence length, embedding dimensionality (dim_embedding)

        # calculate values for all heads in batch and move head forward to be the batch dim
        q, _, _ = self.c_attn(x).split(self.dim_embedding, dim=2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=False,
            )
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(attn_mask == 0, float(&quot;-inf&quot;))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y
            
</code></pre><p><strong>3. Putting all the components together to create a Transformer</strong></p>
<p>We now combine all of these components together into a single Transformer class which we will initialise and then train.</p>
<pre tabindex="0"><code>class Encoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create embeddings for both the encoder and the decoder
        self.encoder_embed = Embedding(config)

        # Create an attention mechanism for the encoder
        self.attention_encoder = CausalSelfAttention(config)
        

        # Set up a processing layer
        self.encoder_processing_layer = Processing_layer(config)

    def forward(self, x):
        # Encode the language we want to translate from
        x = self.encoder_embed(x)

        # Apply the attention mechanism and add the input
        x = self.attention_encoder(x) + x

        # apply layer norm, two dense layers and a layer norm again
        x = self.encoder_processing_layer(x)

        return x
    
    
    
class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create embeddings for both the encoder and the decoder
        self.decoder_embed = Embedding(config)

        # Create an attention mechanism for the decoder
        self.attention_decoder = CausalSelfAttentionMasked(config)
        
        # Create a layernorm layer
        self.layernorm = LayerNorm(config.dim_embedding, bias=config.bias)

        # Create the encoder decoder attention
        self.decoder_attn = EncoderDecoderAttention(config)

        # Set up a processing sections one for the encoder and the other for the decoder
        self.decoder_processing_layer = Processing_layer(config)

     
        # The final layer which maps the models embedding dimension batch to the vocab size
        self.finallayer = nn.Linear(config.dim_embedding, config.vocab_size+1)

    def forward(self, x, y):

        # Encode the language we want to translate into
        y = self.decoder_embed(y)
        
        # Apply the attention mechanism and add the input
        y = self.attention_decoder(y) + y
        
        y = self.layernorm(y)

        # Take the ouput from the encoder and last layer of decoder and calculate attention again then add the input
        y = self.decoder_attn(y, x) + y

        # apply layer norm, two dense layers and a layer norm again
        y = self.decoder_processing_layer(y)

        y = self.finallayer(y)
        return y
    
class Transformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create embeddings for both the encoder and the decoder
        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
            
    def forward(self, x, y):
        # Encode the language we want to translate from
        encoder_out = self.encoder(x)

        # Apply the attention mechanism and add the input
        y = self.decoder(encoder_out,y)
        return y
# instantiate the Transformer with the config file
model = Transformer(config)
</code></pre><p><strong>4. Training</strong>
The code below is from the python script <code>train.py</code> trains the model for 30 epochs. It minimises the cross entropy between the predicted translations and the actual translations.</p>
<pre tabindex="0"><code>%%time

losses = []

torch.set_grad_enabled(True)

#optimizer = optim.Adam(model2.parameters(), lr=1e-3)

optimizer = optim.SGD(model4.parameters(), lr=0.01, momentum=0.9)


for epoch in range(0, 30):
    for i, (german, english, output) in enumerate(train_dataloader):
        model.train()
        yhat = model(german, english)
        loss = Fun.cross_entropy(
            yhat.view(-1, yhat.size(-1)), output.view(-1), ignore_index=-1
        )
        loss.backward()
        optimizer.step()
        # clear the gradients
        optimizer.zero_grad()
        losses.append(loss)
        if i % 400 == 0:
            print(f&quot;Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}&quot;)
</code></pre><p><strong>5. Generation of translations</strong></p>
<p>The function below generates a translation from the input text. This is included in the script <code>translate.py</code>. It takes as an input the sentence we want to translate, it then:</p>
<ol>
<li>encodes that text into integers using the coding developed in the text processing phase.</li>
<li>adds an extra dimension with the unsqueeze(0) so that the model which is trained on batches of 30, can read a single sentence rather than needing a batched input of multiple texts.</li>
<li>it then generates the translation one token at a time.</li>
<li>it starts with a blank decoding sentence and the encoded text we want the trained model to translate. The model takes these inputs and generates a probability distribution for its  prediction of the first word of the translation.</li>
<li>the model then samples from the probability distribution of the next word i.e. the word with highest predicted probability is most likely to be chosen.</li>
<li>the predicted word is then added to the decoded sentence as its first word.</li>
<li>the original sentence and the updated decoded sentence is then input into the model to get the next word.</li>
<li>this is done recursively until we predict the token [end] and the translation ends.</li>
</ol>
<pre tabindex="0"><code>def prediction(x,y):
    logits=model(x,y)
    logits=logits.squeeze(0) # Will remove the first dimension if it is set to 0
    # returns a tensor with all specified dimensions of input of size 1 removed - in this case the first dimension
    # The dim = -1 applies softmax over the last dimension of the tensor
    return Fun.softmax(logits, dim=-1)
</code></pre><p>Functions that encode strings of text as integers according to the encoding of the languages developed earlier:</p>
<pre tabindex="0"><code>def target_vectorization(x):
    return [english_encoded[element] for element in x.split() if element in english_tokens.tokens_vocab]

def source_vectorization(x):
    return [german_encoded[element] for element in x.split() if element in german_tokens.tokens_vocab]
</code></pre><pre tabindex="0"><code>def decode_sequence_no_print(input_sentence):
    # Unsqueezing adds an extra dimension so that the tensor is of size torch.Size([1, 20])
    tokenized_input_sentence = tensor_pad(source_vectorization(input_sentence))[
        :20
    ].unsqueeze(0)
    decoded_sentence = &quot;[start]&quot;
    # Loop through the sentence word by word
    for i in range(config.block_size):
        tokenized_target_sentence = target_vectorization(decoded_sentence)

        if i&lt;0:
          tokenized_target_sentence = torch.zeros(20,dtype=torch.int).unsqueeze(0) 

        elif i &gt;= 0:
           tokenized_target_sentence=tensor_pad(tokenized_target_sentence)[:20].unsqueeze(0)

        predictions = prediction(tokenized_input_sentence, tokenized_target_sentence)
        # The second index is the word position, the third index are the words
        # The .item() extracts the tensor index from the tensor
        probs=torch.multinomial(predictions[i,:], num_samples=1)
           
        sampled_token_index = predictions[i, :].argmax().item()

        sampled_token = english_tokens.decode()[sampled_token_index]

        decoded_sentence += &quot; &quot; + sampled_token
        # If the predicted token is end stop
        if sampled_token == &quot;[end]&quot;:
            break
    return decoded_sentence
</code></pre><p>The code below loops through the test data and generates predictions:</p>
<pre tabindex="0"><code>class style:
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'

# Looping through the test dataset to generate some translations:

for i, elem in enumerate(test_dataloader):
    if i % 1000 == 0:
      
        print(style.BOLD+'Orginal'+style.END)
        german=trans(elem[0].tolist()[0], 'ger')
        print(german)
        print(style.BOLD+'Translation'+style.END)
        print(trans(elem[1].tolist()[0], 'eng'))
        print(style.BOLD+'Machine Translation'+style.END)
        print(decode_sequence_no_print(german))
        print('\n')
</code></pre><p><strong>References:</strong></p>
<p><a href="https://arxiv.org/abs/1706.03762/">The original Transformers paper &lsquo;Attention is All You Need&rsquo;,
Vaswani et al. (2017)</a></p>
<p><a href="https://github.com/karpathy/nanoGPT/">The repo for Andrej Karpathy&rsquo;s nanoGPT</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s ‘The illustrated transformer’</a></p>
<p>François Chollet’s book ‘Deep Learning with Python (2nd edition)’</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">OpenAI&rsquo;s, &lsquo;Language Models are Few-Shot Learners&rsquo;</a></p>
<p><strong>Previous Transformer notes</strong>:</p>
<ol>
<li><a href="https://t.co/hFufxkFXaQ"><strong>Transformers 1: The Attention Mechanism</strong></a></li>
<li><a href="https://johnardavies.github.io/technical/transformer2/"><strong>Transformers 2: The Transformer&rsquo;s Structure</strong></a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
      <li><a href="https://johnardavies.github.io/tags/digital/">digital</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
