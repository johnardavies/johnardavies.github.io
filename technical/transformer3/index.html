<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers 3: Building and training a Transformer | John&#39;s Site</title>
<meta name="keywords" content="technical, digital" />
<meta name="description" content="Printworks, London
Transformers 3: Building and training a Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that translates German text into English. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/transformer3/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Transformers 3: Building and training a Transformer" />
<meta property="og:description" content="Printworks, London
Transformers 3: Building and training a Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that translates German text into English. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/transformer3/" />
<meta property="og:image" content="https://johnardavies.github.io/printworks.jpg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2024-05-18T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-05-18T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/printworks.jpg" />
<meta name="twitter:title" content="Transformers 3: Building and training a Transformer"/>
<meta name="twitter:description" content="Printworks, London
Transformers 3: Building and training a Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that translates German text into English. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers 3: Building and training a Transformer",
      "item": "https://johnardavies.github.io/technical/transformer3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers 3: Building and training a Transformer",
  "name": "Transformers 3: Building and training a Transformer",
  "description": "Printworks, London\nTransformers 3: Building and training a Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that translates German text into English. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet\u0026rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.",
  "keywords": [
    "technical", "digital"
  ],
  "articleBody": "Printworks, London\nTransformers 3: Building and training a Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that translates German text into English. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet’s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow. The relatively small scale of the model built here and the data it is trained on means it won’t be able to perfectly translate text, but will be able to generate translations to some extent.\nThis note covers:\n1. Specifying the parameters of the Transformer\n2. Processing the text data\n3. The structure of the Transformer\n4. The implementation of attention\n5. The implementation of other Transformer components\n6. Putting all the components together to create a Transformer\n7. Training\n8. Generation of translations\nThe code covered in the post is available from the repo here.\n1. Specifying the parameters of the Transformer To start we will initialise the model with a Python dataclass (config.py) that specifies the model’s key parameters:\nfrom dataclasses import dataclass @dataclass class TransformerConfig: \"\"\"With a dataclass we don't need to write an init function, just specify class attributes and their types\"\"\" vocab_size: int = 15000 dim_embedding: int = 600 dim_inner_layer: int = 2000 n_head: int = 6 batch_size: int = 30 block_size: int = 20 epochs: int = 30 dropout: float = 0.2 bias: bool = False config = TransformerConfig() The model has an embedding dimension of 600 and a vocabulary size of 15000 for each language. At various stages in the Transformer there are inner layers that increase the dimensionality of the vectors describing the words to 2000, giving the model more weights to capture the language structure. Attention is calculated over 6 heads in parallel, which due to how it is implemented involves 100 dimensional query, key and value vectors. During training German text and a corresponding English translation will be input in batches of 30 text-pairs. Both the German text and English translation texts are fed in blocks that are capped at 20 tokens in length. In training we’ll loop through the training data 30 times (30 epochs). 20% dropout is implemented at a number of stages of the network.\n2. Processing the text data To get some text data to train and test the Transformer we download the english-german translation pairs from anki and unzip the file:\n$ wget https://www.manythings.org/anki/deu-eng.zip \u0026\u0026 unzip deu-eng.zip This produces the text file (deu.txt) which consists of pairs of English and German text, for example:\nBless you.\tGesundheit\nBuy a gun.\tLeg dir eine Schusswaffe\nWe want to convert the words in both the English and German texts into numbers and generate test and training data sets. To do this we process the texts with the Python script text_processing.py. This starts by simplifying the text, processing the pairs to remove punctuation and setting the text to lowercase. [start] and [end] tags are added at the beginning and end of all of the English texts.\nimport torch import torch.nn.functional as Fun from torch.utils.data.dataset import random_split from collections import Counter from string import punctuation import pickle from config import TransformerConfig # Initialize configuration config = TransformerConfig() # Strip punctuation def strip_punctuation(s): \"\"\"Function that removes punctuation\"\"\" return str(\"\".join(c for c in s if c not in punctuation)) def tensor_pad(x): \"\"\"converts list to tensor and pads to length 20\"\"\" return Fun.pad(torch.tensor(x, dtype=torch.int64), (0, config.block_size), value=0) def main(): \"\"\"This is the function which processes the text to output it in a form used by the model\"\"\" text_file = \"deu.txt\" with open(text_file) as language_file: lines = language_file.read().split(\"\\n\") print(\"There are \" + str(len(lines)) + \" lines\") # Split the text file into German and English, setting it to lower case and removing punctuation # We append a [start] at the beginning of each line of the English text and end it with [end] text_pairs = [] for i, line in enumerate(lines): try: english, german = line.split(\"\\t\")[0:2] english = \"[start] \" + strip_punctuation(english.lower()) + \" [end]\" text_pairs.append([strip_punctuation(german.lower()), english]) except: print(\"failed to load line %s\" % (i)) We combine all the English texts, and separately combine the German texts, to calculate the distinct words used in each language. We then create a class WordsNumbers which is initiated for each language separately and identifies the most common words in each. It has two methods that are dictionaries allowing the most common 15,000 words in each language to be converted into numbers and the numbers converted back to words. Technically it is 14,999 words as we reserve the 0 token to represent the blank spaces used to pad both English and German texts to get to 20 words/tokens if they have fewer than that. Mapping the texts' words into numbers is the simplest possible approach, more sophisticated encodings based on parts of speech or combinations of words are also possible.\n # To get the tokens used in English and German texts we create # two separate lists and from these create a single German text and a single English text german_list = [item[0] for item in text_pairs] english_list = [item[1] for item in text_pairs] german_text = \" \".join(german_list) english_text = \" \".join(english_list) class WordsNumbers: \"\"\"Class that produces two dictionaries from input text, one of which maps words to numbers and the other one reverses this mapping numbers to words.\"\"\" def __init__(self, text, config): self.text = text self.config = config # Get the tokens self.tokens_list = list(set(self.text.split())) # Get the most common tokens # Subtract -1 as we want to leave 1 of the 15,0000 numbers to code the padding token rather than words. We use 0 for this. self.tokens_counter = Counter(self.text.split()).most_common( self.config.vocab_size - 1 ) self.tokens_vocab = [item for item, count in self.tokens_counter] def to_words(self): # Converts from the numbers to words word_dict = {i + 1: token for i, token in enumerate(self.tokens_vocab)} return word_dict def to_numbers(self): # Comverts from words to numbers number_dict = {token: i + 1 for i, token in enumerate(self.tokens_vocab)} return number_dict We then apply the dictionaries to encode the English and German texts into numbers. The English translation text is processed into two copies. One that has the [end] token removed which will be input for training and one which has the [start] token removed which is used as the target text the model is trying to predict. The [start] token is used in translation to tell the model to start generating a translation so we need it as an input, but do not need it as an output. Conversely, we want the model to be able to predict when the translation sentence ends so include the [end] token in the target for the model to predict, so we do not use it as an input. Each German and English text is padded with 0s to get to block sizes of 20 tokens. The methods to convert between words and numbers start their numeric count at 1 in order to keep 0 for the padding. We then split the dataset 80%:20% into training and test datasets respectively.\n # Get the German tokens and the English tokens english_tokens = WordsNumbers(english_text, config) german_tokens = WordsNumbers(german_text, config) # Creates dictionaries converting words into numbers english_numeric = english_tokens.to_numbers() german_numeric = german_tokens.to_numbers() # Save the English and German dictionaries with open(\"english_dictionary.pkl\", \"wb\") as eng: pickle.dump(english_numeric, eng) with open(\"german_dictionary.pkl\", \"wb\") as ger: pickle.dump(german_numeric, ger) # Convert the texts into numbers for the words that are in the 15000 most common words in either language text_pairs_encoded = [ [ [ german_numeric[element] for element in pair[0].split() if element in german_tokens.tokens_vocab ], [ english_numeric[element] for element in pair[1].split() if element in english_tokens.tokens_vocab ], ] for pair in text_pairs ] # Split the data between: # the encoder input in german # the decoder input in english, where the end token is removed elem[1][:-1] # english output we are trying to predict which is shifted one token to the right elem[1][1:] text_pairs_encoded_split = [ (elem[0], elem[1][:-1], elem[1][1:]) for elem in text_pairs_encoded ] # Pads each bit of text to 20 tokens by adding 0s with tensor_pad and truncating at block size text_pairs_encoded_padded = [ [ tensor_pad(item1)[: config.block_size], tensor_pad(item2)[: config.block_size], tensor_pad(item3)[: config.block_size], ] for item1, item2, item3 in text_pairs_encoded_split ] # Calculate how many observations are needed for a 20% test sample test_len = round(len(text_pairs_encoded_padded) * 0.2) # Calculate the number of training observations as the residual train_len = round(len(text_pairs_encoded_padded)) - test_len # Get the train dataset and the test dataset train_dataset, test_dataset = random_split( text_pairs_encoded_padded, [train_len, test_len] ) # Save the test and train datasets as pickle files torch.save(train_dataset, \"train_dataset.pt\") torch.save(test_dataset, \"test_dataset.pt\") # Only run main if this script is run directly if __name__ == \"__main__\": main() 3. The structure of the Transformer The structure of the Transformer is descibed in more detail in the previous note. The annotated schematic below shows how the components described below correspond to the architecture in the original Transformer’s paper. We cover the components individually in sections 4 and 5. We then use them to implement an encoder, a decoder and combine the two to create a Transformer in section 6.\nBroadly speaking we take the processed German and English text, embed them into vectors with embedding layers. The embedding vectors from the language we want to translate from (here German) pass into the Encoder layer, where an attention calculation is done on them (Multi-Head attention) and further processing applied (Encoder Processing layer). The embedding vectors from the language we want to translate into (here English) pass into the Decoder layer where a different attention calculation is applied taking into account that the translation is built one word at a time (Masked Mult Head Attention). The results from this are then combined with the outputs of the encoder in another kind of attention calculation (Encoder Decoder Attention). More processing layers (Decoder Processing layer) are applied to the outputs of this calculation and the model then outputs the results of the final layer which has the dimensionality of the vocabulary size we are using (here 15,000).\n￼ Original figure is from ‘Attention is All You Need’, Vaswani et al.(2017)\nIn our example we have one Transformer layer so N=1 (as shown in the diagram). If we had more Transformer layers, the outputs of the first encoder layer would feed into the next encoder layer, and so on, until the last encoder layer. The output from the last encoder layer would then feed into each of the decoder layers for an attention calculation, where each decoder layer after the first taking its input from the encoder and the preceding decoder layer.\nOne element in the diagram missing in our implementation is that there is no softmax layer in our Transformer. This is because the PyTorch implementation of the cross-entropy metric that we are optimising in training automatically applies a softmax so it does not need to be added to train the model. However, for the translation after training we want the model to output a probability distribution and so apply a softmax layer to the outputs of the trained Transformer there.\n4. The implementation of attention The following code forms part of the Transformer script Transformer.py which is the core model script. Below is the implementation of attention that Andrej Karpathy used in nanoGPT (there are some minor changes to comments and removing some parts not used at this stage as the original nanoGPT was designed for language generation). As with all PyTorch network classes it inherits from the nn.Module class and sets up the components that are used to construct the network when it is initiated. The forward method then details what happens when the data is input into the network layers that are specified in the class.\nimport torch import torch.nn.functional as Fun from torch import nn from config import TransformerConfig # Initialize configuration config = TransformerConfig() class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() # Checks that the dimension of the embedding vector can be divided by the number of heads assert config.dim_embedding % config.n_head == 0 # set embedding and head sizes self.n_head = config.n_head self.dim_embedding = config.dim_embedding # nn.Linear applies a linear y=Ax+b transformation to the input # the input dimension is the first argument # the output dimension is the second argument # the last argument is the b (bias) term # Sets up a layer that increases the dimensionality of the embedding 3x to calculate the query, key and value vectors self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.dropout = config.dropout # Uses a faster implementation of attention if scaled_dot_product_attention available in module torch.nn.functional # (which it is from PyTorch = 2.0) self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) def forward(self, x): # Get the values of the batch size, sequence length and embedding dimensionality ( B, T, C, ) = ( x.size() ) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimension dim_embedding on the 2nd dimension q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) if self.flash: # efficient attention using Flash Attention CUDA kernels # self.training is set to true when model.train() is initiated y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False, ) else: # manual implementation of attention # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Apply the softmax layer so that everything sums to 1 att = F.softmax(att, dim=-1) # Apply dropout att = self.attn_dropout(att) # Multiply the attention results by the value vectors y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) # Change the shape of the tensor back to B, T, C removing the heads y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # output projection and droput y = self.resid_dropout(self.c_proj(y)) return y The code below shows the creation of key, query and value vectors from the embeddings, which is implemented by passing the embedding vectors into c_attn(x) which generates these three types of vectors. These vectors are then split again by the number of heads to perform the attention calculation. The original vectors used to produce the query, key and value vectors have 600 dimensions and there are 6 heads, so this produces 6 sets of query, key and value vectors of 100 dimensions each, for 6 different sets of parallel attention calculations. We start with a tensor of size (B, T, C) where B is batch size, T is block size and C the embedding dimensionality of the data entering the Transformer. This then gets converted to a tensor of size (B, nh, T, hs) as C is split into the number of separate heads nh and the size of the vectors hs they process. At the end of the attention calculation the results from the different heads are combined and we end back at (B, T, C).\nq, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs The method contains an optimised attention mechanism to process these collections of vectors scaled_dot_product_attention. Here we have is_causal=True as this is the encoder side and in translation we assume that the Transformer has access to the full German text before starting translation. This means that attention is calculated between all the vectors.\nBy contrast on the decoder side in the Masked Multi-Head Attention layer is_causal=False (and attention calculated using previous words only) when processing the English translations as in generating the prediction this is done autoregressively with the prediction for the next word of the translation based on the predicted words before. In the training data therefore, to predict the n’th word of the translation the Transformer is given access to the n-1 English words before. For the very first translated word the [start] token is used as the ‘word 0’ to initialise the translation.\ny = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True) If scaled_dot_product_attention is not available then the class also implements attention directly in Python. This illustrates more explicitly the stages in calculating attention. The multiplication of the query and the transposed key matrices of associated vectors and then normalising the results by the square root of the key vectors' dimensionality:\natt = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) The application of the softmax layer:\natt = F.softmax(att, dim=-1) The multiplication of the attention weights by the values vectors:\n y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) The concatenation of the head outputs, which creates vectors of the standard embedding dimension, which are then transformed by another set of weights into a vector of the same dimensionality. In the Karpathy implementation he then adds a dropout layer.\n y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) 5. The implementation of other Transformer components The other Transformer components that are implemented are:\n1. The Embedding layers\n2. Processing layers\n3. Masked Multi-Head Attention layers\n4. Encoder Decoder Attention layers\n5.1 The Embedding layers\nThe Embedding class below takes in the encoded text and then passes each word to two embedding layers. There is the word embedding layer (wte) and the position embedding layer (wtp). These map the word and its position in the text into two vectors of the chosen embedding dimensionality. These two vectors are then added together encoding both the word and its position in the text in a single vector. This is done for all of the 20 words/tokens in a piece of text fed into the network, and their relative position in the text.\nclass Embedding(nn.Module): def __init__(self, config): super().__init__() # Creates the text embedding and the position embedding # nn.Embedding automatically does the one hot encoding so this # does not need to be created directly self.config = config self.wte = nn.Embedding(config.vocab_size, config.dim_embedding) self.wtp = nn.Embedding(config.block_size, config.dim_embedding) def forward(self, x): # Generates the word embedding from the text x = self.wte(x) # Generates the position embedding is applied over a tensor representing word position position_ids = ( torch.arange(self.config.block_size).unsqueeze(0).repeat(x.size(0), 1) ) position_embeddings = self.wtp(position_ids) # Add the two embeddings x = x + position_embeddings return x 5.2 Processing layers\nThe LayerNorm class applies layer normalisation, calculating the mean and standard deviation across the batch for each word position in the text and elements of the vector per word and then subtracting these means and dividing by the standard deviation from the corresponding elements across the batch.\nclass LayerNorm(nn.Module): \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\" def __init__(self, ndim, bias): super().__init__() self.weight = nn.Parameter(torch.ones(ndim)) self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None def forward(self, input): return Fun.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5) A series of layers that implement layer normalisation, a dense layer again and then a dense layer with a skip connection and then layer normalisation. We label this pattern of layers which appears in both the decoder and encoder as processing layers.\nclass ProcessingLayer(nn.Module): def __init__(self, config): super().__init__() self.denselayer1 = nn.Linear(config.dim_embedding, config.dim_inner_layer) self.denselayer2 = nn.Linear(config.dim_inner_layer, config.dim_embedding) self.layernorm1 = LayerNorm(config.dim_embedding, bias=config.bias) self.layernorm2 = LayerNorm(config.dim_embedding, bias=config.bias) def forward(self, x): # A layer norm and then two dense layers\\n\", x_in = self.layernorm1(x) x = self.denselayer1(x_in) x = self.denselayer2(x) + x_in x = self.layernorm2(x) return x 5.3 Masked Multi-Head Attention layers\nAs the english translation is generated one word at a time, in training we assume that attention is only calculated between the words prior to given a word i.e. words that would be already known in producing the translation. To do this we apply a mask to the attention calculation to restrict the calculation of attention in this way.\nThere is a class that implements masked multi-head attention for the text in the language we want to translate into. This forms the first attention layer of the decoder processing the english text translations during training and the generated english text translations during translation. In the scaled_dot_product_attention implementation of attention this is implemented by applying is_causal=True.\nIn the alternative direct implementation of attention the mask is applied directly to attention using the mask in the line below:\natt = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\nwhere the bias is defined in the term:\n self.register_buffer( \"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view( 1, 1, config.block_size, config.block_size ), ) The register_buffer creates a tensor called “bias” that will not be updated during training, unlike the model weights. The bias term has the form of a lower triangular matrix. If the block size had three words, then the register buffer has size [1,1,3,3] and looks like this:\ntensor([[[[1., 0., 0.], [1., 1., 0.], [1., 1., 1.]]]]) In the masked_fill T is the block_size and the upper triangle elements that are 0 are set to -inf in the mask. When the mask is applied to the attention the lower triangular elements corresponds to the attention of the first word being based on the word itself, the attention of the second word being based on the first and second word etc etc maintaining the causality that attention is only calculated between a given word and its predecessors (but not future words). As the next layer is a softmax layer which has exponential functions, exp^0 = 1, so a 0 as an input would still give positive values for the upper diagonal elements, but as the input tends to -inf the exponentials tend to 0 so setting 0 elements to -inf removes them from the output post softmax layer.\nclass MaskedMultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() # Checks that the dimension of the embedding vector can be divided by the number of heads assert config.dim_embedding % config.n_head == 0 # set embedding and head size self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularisation self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.dropout = config.dropout # Uses a faster implementation of attention if scaled_dot_product_attention available in module torch.nn.functional # (which it is from PyTorch = 2.0) self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer( \"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view( 1, 1, config.block_size, config.block_size ), ) def forward(self, x): # Get the values of the batch size, block size and embedding dimensionality ( B, T, C, ) = ( x.size() ) # calculate query, key and value vectors # by splitting the output of the attention layer into tensors of dimension dim_embedding on the 2nd dimension q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True, ) else: # manual implementation of attention # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Calculate the masked attention att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # Apply the softmax layer so that everything sums to 1 att = F.softmax(att, dim=-1) # Apply dropout att = self.attn_dropout(att) # Multiply the attention results by the value vectors y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) # Change the shape of the tensor back to B, T, C removing the heads y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # output projection and dropout y = self.resid_dropout(self.c_proj(y)) return y 5.4 Encoder Decoder Attention layer\nThe second attention layer in the decoder (the Encoder Decoder Attention layer) has two inputs: the output from the encoder and the normalised output from the decoder’s masked attention layer. In the encoder decoder attention layer the encoder output is transformed into key and value vectors. These vectors are then combined in an attention calculation (without masking) with the query vectors created from the vectors output from the decoder’s previous layer.\nclass EncoderDecoderAttention(nn.Module): def __init__(self, config): super().__init__() # Checks that the dimension of the embedding vector can be divided by the number of heads assert config.dim_embedding % config.n_head == 0 # set embedding and head sizes self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.c_attn_en = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.dropout = config.dropout # Uses a faster implementation of attention if scaled_dot_product_attention available in module torch.nn.functional # (which it is from PyTorch = 2.0) self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) def forward(self, x, e): # Get the values of the batch size, block size and embedding dimensionality ( B, T, C, ) = ( e.size() ) # calculate the key and value vectors from the output of the encoder _, k, v = self.c_attn_en(e).split(self.dim_embedding, dim=2) k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # calculate the query vectors from the output of the previous decoder layers q, _, _ = self.c_attn(x).split(self.dim_embedding, dim=2) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) - (B, nh, T, T) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False, ) else: # manual implementation of attention att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = att.masked_fill(attn_mask == 0, float(\"-inf\")) att = F.softmax(att, dim=-1) att = self.attn_dropout(att) y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # output projection and dropout y = self.resid_dropout(self.c_proj(y)) return y 6. Putting all the components together to create a Transformer We now use all of these components to create an encoder layer and a decoder layer. The encoder and decoder are then combined together into a single Transformer class which we will initialise and then train.\n# The encoder class class Encoder(nn.Module): def __init__(self, config): super().__init__() self.config = config # Create embeddings for the encoder self.encoder_embed = Embedding(config) # Create an attention mechanism for the encoder self.attention_encoder = MultiHeadAttention(config) # Set up a processing layer self.encoder_processing_layer = ProcessingLayer(config) def forward(self, x): # Encode the language we want to translate from x = self.encoder_embed(x) # Apply the attention mechanism and add the input x = self.attention_encoder(x) + x # apply layer norm, two dense layers and a layer norm again x = self.encoder_processing_layer(x) return x # The decoder class class Decoder(nn.Module): def __init__(self, config): super().__init__() self.config = config # Create an embedding layer for the decoder self.decoder_embed = Embedding(config) # Create an attention mechanism for the decoder self.attention_decoder = MaskedMultiHeadAttention(config) # Create a layernorm layer self.layernorm = LayerNorm(config.dim_embedding, bias=config.bias) # Create the encoder decoder attention self.decoder_attn = EncoderDecoderAttention(config) # Set up a processing layer for the decoder self.decoder_processing_layer = ProcessingLayer(config) # The final layer which maps the models embedding dimension back to the vocab size self.final_layer = nn.Linear(config.dim_embedding, config.vocab_size) def forward(self, x, y): # Encode the language we want to translate into y = self.decoder_embed(y) # Apply the attention mechanism and add the input y = self.attention_decoder(y) + y # Apply layer norm y = self.layernorm(y) # Take the output from the encoder and last layer of decoder and calculate attention again then add the input y = self.decoder_attn(y, x) + y # apply layer norm, two dense layers and a layer norm again y = self.decoder_processing_layer(y) # Map the embedding dimension back to the vocabularly size y = self.final_layer(y) return y # The Transformers class class Transformer(nn.Module): def __init__(self, config): super().__init__() assert config.vocab_size is not None assert config.block_size is not None self.config = config # Create both the encoder and the decoder layers self.encoder = Encoder(config) self.decoder = Decoder(config) def forward(self, x, y): # Encode the language we want to translate from encoder_out = self.encoder(x) # Take the output from the encoder and translated text and pass to the decoder y = self.decoder(encoder_out, y) return y 7. Training The code below is from the python script train.py which trains the model for 30 epochs. It updates the model weights to minimise the cross-entropy between the predicted translations and the actual translations. In evaluating the metric the 0 padding tokens are ignored. The loss on the training data is output to the screen from batches at regular intervals during each epoch of training. The average loss on the test dataset is computed at the end of each epoch. The model uses an Adam optimiser.\nStarting training a new model from scratch is run with:\n$ python train.py \"new_model\" At the end of each epoch the model and optimizer state are saved to a file indexed by the epoch number. If the training is stopped and one wants to resume from a saved model, then the following format is used (here the model passed to commence training is that saved after the 11th epoch).\n$ python train.py model_post_11 Training the model for 30 epochs on a Mac Air M2 with 24 GB RAM doing other things took about 7.5 days.\nfrom torch.utils.data import DataLoader from torch.utils.data.dataset import random_split from torch.utils.tensorboard import SummaryWriter import torch.nn.functional as Fun from torch import nn import torch.optim as optim import sys # Import the model config and the Transformer class import config from Transformer import * writer = SummaryWriter() model = Transformer(config) # Import the training and test datasets and convert them into data loaders train_dataset = torch.load(\"train_dataset.pt\") test_dataset = torch.load(\"test_dataset.pt\") train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True) test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True) torch.set_grad_enabled(True) optimizer = optim.Adam(model.parameters()) def save_checkpoint(model, optimizer, save_path, epoch): \"\"\"function to save checkpoints on the model weights, the optimiser state and epoch\"\"\" torch.save( { \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), \"epoch\": epoch, }, save_path, ) # If a \"new_model is being created don't load anything, but if saved model is input load the model and the optimizer state and the last epoch value if sys.argv[1] == \"new_model\": epoch_start = 0 elif sys.argv[1] is not None: state = torch.load(sys.argv[1]) model.load_state_dict(state[\"model_state_dict\"]) optimizer.load_state_dict(state[\"optimizer_state_dict\"]) epoch_start = state[\"epoch\"] # The training loop (The + 1 is to get the numbers we want from range) for epoch in range(epoch_start + 1, config.epochs + 1): # Set the model to training mode model.train() for i, (german, english, output) in enumerate(train_dataloader): # clear any existing gradients before running a new batch optimizer.zero_grad() # Generates a predicted translation yhat = model(german, english) # yhat has dimensions (batch_size, block_size, vocab_size) # yhat.view(-1, yhat.size(-1)) has the dimensions ((batch_size X block_size), vocab_size) # output has the dimensions (batch_size, block_size) # output.view(-1) has the dimensions of (batch_size x block_size) # Calculates the cross entropy, ignoring the 0s loss = Fun.cross_entropy( yhat.view(-1, yhat.size(-1)), output.view(-1), ignore_index=0 ) # write the loss writer.add_scalar(\"Loss/train\", loss, epoch) # computes the grad of how the loss changes as the weight changes loss.backward() # update the weights optimizer.step() # print how the training is doing at regular stages during the epoch if i % 400 == 0: print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\") writer.flush() # Save the model and optimizer path_to_save = \"model_post_\" + str(epoch) save_checkpoint(model, optimizer, path_to_save, epoch) # Compute average loss on the test set model.eval() # Set the model to evaluation mode turning off dropout val_loss = 0.0 with torch.no_grad(): # No gradient computation for validation for german_test, english_test, output_test in test_dataloader: yhat = model(german_test, english_test) loss = Fun.cross_entropy( yhat.view(-1, yhat.size(-1)), output_test.view(-1), ignore_index=0 ) val_loss += loss.item() avg_val_loss = val_loss / len(test_dataloader) writer.add_scalar(\"Loss/test\", avg_val_loss, epoch) print(f\"Epoch: {epoch}, Avg_val_loss: {avg_val_loss}\") 8. Generation of translations The script below translate.py generates translations. It imports the test dataset converting it to a dataloader and the two dictionaries that map the english and german words to numbers. It imports the saved trained model that is identified in the first argument of the script (here it is assumed to be saved in the same directory) and a sentence to translate as the second argument.\n$ python translate.py model_post_30 \"das wetter ist gut\" If no text to translate is provided as a second argument, the model loops through the test data set generating translations.\nimport torch.nn.functional as Fun from torch import nn import torch.optim as optim from torch.utils.data import DataLoader import pickle import text_processing as text_pro from Transformer import * import sys # Load the english and german dictionaries with open(\"german_dictionary.pkl\", \"rb\") as file_ger: # Load the pickled object german_tokens = pickle.load(file_ger) with open(\"english_dictionary.pkl\", \"rb\") as file_en: # Load the pickled object english_tokens = pickle.load(file_en) # Reverse the dictionaries to go from the numbers back to the words # This is used previously in text processing consider refactoring decode_to_english = {v: k for k, v in english_tokens.items()} decode_to_german = {v: k for k, v in german_tokens.items()} # Functions to convert the sentences back into words from numbers def source_vectorization(x): \"\"\"Converts the German words into numbers\"\"\" return [ german_tokens[element] for element in x.split() if element in german_tokens.keys() ] def target_vectorization(x): \"\"\"Converts the English words into numbers\"\"\" return [ english_tokens[element] for element in x.split() if element in english_tokens.keys() ] # Load the test dataset and convert into a data loader of batch size 1 test_dataset = torch.load(\"test_dataset.pt\") test_dataloader = DataLoader(test_dataset, batch_size=1 , shuffle=True) # Set up the Transformer and load its configuration model_predict = Transformer(config) # Take the model that the script is going to use to translate from as the first argument in the command line model = sys.argv[1] state = torch.load(model) # Loads the model model_predict.load_state_dict(state[\"model_state_dict\"]) The section below generates the translations. It takes the sentence we want to translate and then:\n  Converts that text into integers using the coding developed in the text processing phase, adding any 0 padding needed to get to 20 tokens.\n  Adds an extra dimension with the unsqueeze(0) so that the model which is trained on batches of 30, can read a single sentence rather than needing a batched input of multiple texts i.e. it is effectively creating a dummy batch dimension for the sentence so the model can read the sentence as if it were in a batch.\n  The model then generates the translation one token at a time. It starts with the text we want the trained model to translate (converted into numbers) and a blank translation sentence consisting of the number representing the [start] token and then the 19 0s which forms the start of the translation.\n  The model takes these two inputs and generates a probability distribution for its prediction of the first word of the translation. It then samples from the probability distribution i.e. the word with highest predicted probability is most likely to be chosen.\n  The number representing the predicted word is then added to the decoded sentence as the next word after [start], so it is now the numbers representing 2 words and 18 0s.\n  The original sentence and the updated translation sentence is then input into the trained model to generate the second word of the translation etc etc.\n  All of the above is in terms of the numbers that represent the words in both languages. As the sentence is built it gets translated back from the numbers to the english words. This continues until the model predicts the token [end] and the translation process stops.\n  # Sets the model state to evaluate model_predict.eval() def prediction(x, y): \"\"\"This gives the probability distribution over English words for a given German translation\"\"\" logits = model_predict(x, y) logits = logits.squeeze(0) # Will remove the first dimension if it is set to 0 # The dim = -1 applies softmax over the last dimension of the tensor return Fun.softmax(logits, dim=-1) def decode_sequence(input_sentence): \"\"\"This function generates the translation\"\"\" # Unsqueezing adds an extra dimension so that the model which was trained on batches can read single sentences tokenized_input_sentence = text_pro.tensor_pad(source_vectorization(input_sentence))[ : config.block_size ].unsqueeze(0) # initalises the decoded sentence with [start] decoded_sentence = \"[start]\" # Loop through the sentence word by word for i in range(0, config.block_size): tokenized_target_sentence = text_pro.tensor_pad(target_vectorization(decoded_sentence))[ : config.block_size ].unsqueeze(0) # Generate predictions predictions = prediction(tokenized_input_sentence, tokenized_target_sentence) # The first index in the predictions tensor is the word position in the sentence # the second index is the predicted word # The .item() extracts the tensor index from the tensor sampled_token_index = torch.multinomial(predictions[i, :], num_samples=1).item() # Gets the word corresponding to the index sampled_token = decode_to_english[sampled_token_index] # Appends the word to the predicted translation to date decoded_sentence += \" \" + sampled_token # If the predicted token is [end] stop if sampled_token == \"[end]\": break return decoded_sentence def trans(x, lan): \"\"\"This is a function to translate the English and German text from the number-word dictionaries that we have of them\"\"\" results = \"\" for elem in x: if elem != 0: if lan == \"ger\": results = results + \" \" + decode_to_german[elem] if lan == \"eng\": results = results + \" \" + decode_to_english[elem] return results # Style class to format the print statements class style: BOLD = \"\\033[1m\" UNDERLINE = \"\\033[4m\" END = \"\\033[0m\" # If there are no arguments other than the script itself and the model evaluate the test sample at regular intervals if len(sys.argv)==2: # Looking at selected sentences from the test data for i, elem in enumerate(test_dataloader): if i % 3400 == 0: print(style.BOLD + \"Orginal\" + style.END) german = trans(elem[0].tolist()[0], \"ger\") print(german) print(style.BOLD + \"Translation\" + style.END) print(trans(elem[1].tolist()[0], \"eng\")) print(style.BOLD + \"Machine Translation\" + style.END) print(decode_sequence(german)) print(\"\\n\") # If there is also an additional argument i.e. a sentence to translate, translate the sentence and then exit elif len(sys.argv) == 3: print(decode_sequence(sys.argv[2])) sys.exit() Looking at a sample of random test German sentences fed in we obtain the following machine translations from the Transformer compared to the English translation in the test data :\n**Orginal** wann hast du das geräusch gehört **Translation** [start] when did you hear the sound **Machine Translation** [start] when did you hear the noise [end] **Orginal** schenke dem keine beachtung **Translation** [start] dont pay any attention to that **Machine Translation** [start] pay attention to disobeyed in [end] **Orginal** ich habe die bücher die ich mir aus der bibliothek ausgeliehen hatte zurückgebracht und mir ein paar neue ausgeliehen **Translation** [start] i returned the books i borrowed from the library and i borrowed some new ones **Machine Translation** [start] ive wears a few books that for me [end] **Orginal** wie oft essen sie schokolade **Translation** [start] how often do you eat chocolate **Machine Translation** [start] how often eat chocolate [end] **Orginal** ich kann ein geheimnis bewahren **Translation** [start] i can keep a secret **Machine Translation** [start] i can drive a secret [end] **Orginal** möchtest du wissen wer das gemacht hat **Translation** [start] do you want to know who did this **Machine Translation** [start] do you want to know who was doing that [end] **Orginal** ich möchte mit deiner mutter sprechen **Translation** [start] i want to talk to your mother **Machine Translation** [start] i want to know with your mother [end] Two of the translations are essentially correct, but for slight differences in emphasis which will not matter in many cases. One is understandable, but has a grammatical error. Three have several of the words right, but due to some mistakes do not have a clear meaning. One of the translations is completely wrong.\nTrying it on an example of one of the standard challenges for English speakers learning German, the fact that meanings can change with variations of the definite article, it translates “ich renne in den park” correctly as “i am running into the park”, but “ich renne in der park” is translated to the same thing, when it should be “i am running in the park” (The switch from die to dem changes the meaning). The model has clearly developed some ability to translate from the data, but it is far from perfect.\nReferences:\nThe original Transformers paper ‘Attention is All You Need’, Vaswani et al. (2017)\nThe repo for Andrej Karpathy’s nanoGPT\nJay Alammar’s ‘The illustrated transformer’\nFrançois Chollet’s book ‘Deep Learning with Python (2nd edition)’\nOpenAI’s, ‘Language Models are Few-Shot Learners’\nPrevious Transformer notes:\n Transformers 1: The Attention Mechanism Transformers 2: The Transformer’s Structure  ",
  "wordCount" : "6970",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/printworks.jpg","datePublished": "2024-05-18T00:00:00Z",
  "dateModified": "2024-05-18T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/transformer3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Transformers 3: Building and training a Transformer
    </h1>
    <div class="post-meta">May 18, 2024&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/printworks.jpg" alt="">
        
</figure>
  <div class="post-content"><p>Printworks, London</p>
<h3 id="transformers-3-building-and-training-a-transformer">Transformers 3: Building and training a Transformer<a hidden class="anchor" aria-hidden="true" href="#transformers-3-building-and-training-a-transformer">#</a></h3>
<p>Having discussed <a href="https://johnardavies.github.io/technical/transformer1/">how attention works</a> and <a href="https://johnardavies.github.io/technical/transformer2/">the structure of
Transformers</a> we’ll now implement a simple Transformer that translates German text into English. To do this
we’ll take the attention mechanism and other network components that Andrej Karpathy developed in <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a> for language generation and reuse them to implement
a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s
book <a href="https://github.com/fchollet/deep-learning-with-python-notebooks">‘Deep Learning with Python’</a> which is written in Keras/TensorFlow. The relatively small scale of the model built here and the data it is trained on means it won&rsquo;t be able to perfectly translate text, but will be able to generate translations to some extent.</p>
<p><strong>This note covers</strong>:</p>
<p><strong>1. Specifying the parameters of the Transformer</strong><br>
<strong>2. Processing the text data</strong><br>
<strong>3. The structure of the Transformer</strong><br>
<strong>4. The implementation of attention</strong><br>
<strong>5. The implementation of other Transformer components</strong><br>
<strong>6. Putting all the components together to create a Transformer</strong><br>
<strong>7. Training</strong><br>
<strong>8. Generation of translations</strong></p>
<p>The code covered in the post is available from the repo <a href="https://github.com/johnardavies/transformer_example">here</a>.</p>
<h3 id="1-specifying-the-parameters-of-the-transformer">1. Specifying the parameters of the Transformer<a hidden class="anchor" aria-hidden="true" href="#1-specifying-the-parameters-of-the-transformer">#</a></h3>
<p>To start we will initialise the model with a Python dataclass (<code>config.py</code>) that specifies the model&rsquo;s key parameters:</p>
<pre tabindex="0"><code>from dataclasses import dataclass


@dataclass
class TransformerConfig:

    &quot;&quot;&quot;With a dataclass we don't need to write an init function, just specify class attributes and their types&quot;&quot;&quot;

    vocab_size: int = 15000  
    dim_embedding: int = 600
    dim_inner_layer: int =  2000
    n_head: int = 6
    batch_size: int = 30 
    block_size: int = 20 
    epochs: int = 30
    dropout: float = 0.2
    bias: bool = False
   
config = TransformerConfig()
</code></pre><p>The model has an embedding dimension of 600 and a vocabulary size of 15000 for each language. At various stages in the Transformer there are inner layers that increase the dimensionality of the vectors describing the words to 2000, giving the model more weights to capture the language structure. Attention is calculated over 6 heads in parallel, which due to how it is implemented involves 100 dimensional query, key and value vectors.  During training German text and a corresponding English translation will be input in batches of 30 text-pairs. Both the German text and English translation texts are fed in blocks that are capped at 20 tokens in length. In training we&rsquo;ll loop through the training data 30 times (30 epochs). 20% dropout is implemented at a number of stages of the network.</p>
<h3 id="2-processing-the-text-data">2. Processing the text data<a hidden class="anchor" aria-hidden="true" href="#2-processing-the-text-data">#</a></h3>
<p>To get some text data to train and test the Transformer we download the english-german translation pairs from
<a href="https://www.manythings.org/anki/">anki</a> and unzip the file:</p>
<pre tabindex="0"><code>$ wget https://www.manythings.org/anki/deu-eng.zip &amp;&amp; unzip deu-eng.zip
</code></pre><p>This produces the text file (deu.txt) which consists of pairs of English and German text, for example:<br>
<code>Bless you.	Gesundheit</code><br>
<code>Buy a gun.	Leg dir eine Schusswaffe</code></p>
<p>We want to convert the words in both the English and German texts into numbers and generate test and training data sets. To do this we process the texts with the Python script <code>text_processing.py</code>. This starts by simplifying the text, processing the pairs to remove punctuation and setting the text to lowercase. [start] and [end] tags are added at the beginning and end of all of the English texts.</p>
<pre tabindex="0"><code>import torch
import torch.nn.functional as Fun
from torch.utils.data.dataset import random_split

from collections import Counter
from string import punctuation
import pickle

from config import TransformerConfig

# Initialize configuration
config = TransformerConfig()

# Strip punctuation
def strip_punctuation(s):
    &quot;&quot;&quot;Function that removes punctuation&quot;&quot;&quot;
    return str(&quot;&quot;.join(c for c in s if c not in punctuation))


def tensor_pad(x):
    &quot;&quot;&quot;converts list to tensor and pads to length 20&quot;&quot;&quot;
    return Fun.pad(torch.tensor(x, dtype=torch.int64), (0, config.block_size), value=0)


def main():
    &quot;&quot;&quot;This is the function which processes the text to output it in a form used by the model&quot;&quot;&quot;
    text_file = &quot;deu.txt&quot;
    with open(text_file) as language_file:
        lines = language_file.read().split(&quot;\n&quot;)
        print(&quot;There are &quot; + str(len(lines)) + &quot; lines&quot;)

    # Split the text file into German and English, setting it to lower case and removing punctuation
    # We append a [start] at the beginning of each line of the English text and end it with [end]
    text_pairs = []
    for i, line in enumerate(lines):
        try:
            english, german = line.split(&quot;\t&quot;)[0:2]
            english = &quot;[start] &quot; + strip_punctuation(english.lower()) + &quot; [end]&quot;
            text_pairs.append([strip_punctuation(german.lower()), english])
        except:
            print(&quot;failed to load line %s&quot; % (i))
</code></pre><p>We combine all the English texts, and separately combine the German texts, to calculate the distinct words used in each language. We then create a class WordsNumbers which is initiated for each language separately and identifies the most common words in each. It has two methods that are dictionaries allowing the most common 15,000 words in each language to be converted into numbers and the numbers converted back to words. Technically it is 14,999 words as we reserve the 0 token to represent the blank spaces used to pad both English and German texts to get to 20 words/tokens if they have fewer than that. Mapping the texts' words into numbers is the simplest possible approach, more sophisticated encodings based on parts of speech or combinations of words are also possible.</p>
<pre tabindex="0"><code>    # To get the tokens used in English and German texts we create
    # two separate lists and from these create a single German text and a single English text
    german_list = [item[0] for item in text_pairs]
    english_list = [item[1] for item in text_pairs]

    german_text = &quot; &quot;.join(german_list)
    english_text = &quot; &quot;.join(english_list)

    class WordsNumbers:
        &quot;&quot;&quot;Class that produces two dictionaries from input text, one of which maps words
        to numbers and the other one reverses this mapping numbers to words.&quot;&quot;&quot;

        def __init__(self, text, config):
            self.text = text
            self.config = config
            # Get the tokens
            self.tokens_list = list(set(self.text.split()))

            # Get the most common tokens
            # Subtract -1 as we want to leave 1 of the 15,0000 numbers to code the padding token rather than words. We use 0 for this.
            self.tokens_counter = Counter(self.text.split()).most_common(
                self.config.vocab_size - 1
            )
            self.tokens_vocab = [item for item, count in self.tokens_counter]

        def to_words(self):
            # Converts from the numbers to words
            word_dict = {i + 1: token for i, token in enumerate(self.tokens_vocab)}
            return word_dict

        def to_numbers(self):
            # Comverts from words to numbers
            number_dict = {token: i + 1 for i, token in enumerate(self.tokens_vocab)}
            return number_dict
</code></pre><p>We then apply the dictionaries to encode the English and German texts into numbers.
The English translation text is processed into two copies. One that has the [end] token removed
which will be input for training and one which has the [start] token removed which is used as the target text the model is trying to predict. The [start] token is used in translation to tell the model to start generating a translation so we need it as an input, but do not need it as an output. Conversely, we want the model to be able to predict when the translation sentence ends so include the [end] token in the target for the model to predict, so we do not use it as an input. Each German and English text is padded with 0s to get to block sizes of 20 tokens. The methods to convert between words and numbers start their numeric count at 1 in order to keep 0 for the padding. We then split the dataset 80%:20% into training and test datasets respectively.</p>
<pre tabindex="0"><code>    # Get the German tokens and the English tokens
    english_tokens = WordsNumbers(english_text, config)
    german_tokens = WordsNumbers(german_text, config)

    # Creates dictionaries converting words into numbers
    english_numeric = english_tokens.to_numbers()
    german_numeric = german_tokens.to_numbers()

    # Save the English and German dictionaries
    with open(&quot;english_dictionary.pkl&quot;, &quot;wb&quot;) as eng:
        pickle.dump(english_numeric, eng)

    with open(&quot;german_dictionary.pkl&quot;, &quot;wb&quot;) as ger:
        pickle.dump(german_numeric, ger)

    #  Convert the texts into numbers for the words that are in the 15000 most common words in either language
    text_pairs_encoded = [
        [
            [
                german_numeric[element]
                for element in pair[0].split()
                if element in german_tokens.tokens_vocab
            ],
            [
                english_numeric[element]
                for element in pair[1].split()
                if element in english_tokens.tokens_vocab
            ],
        ]
        for pair in text_pairs
    ]

    # Split the data between:
    # the encoder input in german
    # the decoder input in english, where the end token is removed elem[1][:-1]
    # english output we are trying to predict which is shifted one token to the right elem[1][1:]
    text_pairs_encoded_split = [
        (elem[0], elem[1][:-1], elem[1][1:]) for elem in text_pairs_encoded
    ]

    # Pads each bit of text to 20 tokens by adding 0s with tensor_pad and truncating at block size
    text_pairs_encoded_padded = [
        [
            tensor_pad(item1)[: config.block_size],
            tensor_pad(item2)[: config.block_size],
            tensor_pad(item3)[: config.block_size],
        ]
        for item1, item2, item3 in text_pairs_encoded_split
    ]
    # Calculate how many observations are needed for a 20% test sample
    test_len = round(len(text_pairs_encoded_padded) * 0.2)

    # Calculate the number of training observations as the residual
    train_len = round(len(text_pairs_encoded_padded)) - test_len

    # Get the train dataset and the test dataset
    train_dataset, test_dataset = random_split(
        text_pairs_encoded_padded, [train_len, test_len]
    )

    # Save the test and train datasets as pickle files
    torch.save(train_dataset, &quot;train_dataset.pt&quot;)
    torch.save(test_dataset, &quot;test_dataset.pt&quot;)


# Only run main if this script is run directly
if __name__ == &quot;__main__&quot;:
    main()   
</code></pre><h3 id="3-the-structure-of-the-transformer">3. The structure of the Transformer<a hidden class="anchor" aria-hidden="true" href="#3-the-structure-of-the-transformer">#</a></h3>
<p>The structure of the Transformer is descibed in more detail in <a href="https://johnardavies.github.io/technical/transformer2/">the previous note</a>. The annotated schematic below shows how the components described below correspond to the architecture in the original Transformer&rsquo;s paper. We cover the components individually in sections 4 and 5. We then use them to implement an encoder, a decoder and combine the two to create a Transformer in section 6.</p>
<p>Broadly speaking we take the processed German and English text, embed them into vectors with embedding layers. The embedding vectors from the language we want to translate from (here German) pass into the Encoder layer, where an attention calculation is done on them (<code>Multi-Head attention</code>) and further processing applied (<code>Encoder Processing layer</code>). The embedding vectors from the language we want to translate into (here English) pass into the Decoder layer where a different attention calculation is applied taking into account that the translation is built one word at a time (<code>Masked Mult Head Attention</code>). The results from this are then combined with the outputs of the encoder in another kind of attention calculation (<code>Encoder Decoder Attention</code>). More processing layers (<code>Decoder Processing layer</code>) are applied to the outputs of this calculation and the model then outputs the results of the final layer which has the dimensionality of the vocabulary size we are using (here 15,000).</p>
<p><img loading="lazy" src="https://johnardavies.github.io/transformers_annotate.png" alt="transformers_annotate"  />
￼
Original figure is from ‘Attention is All You Need’, Vaswani et al.(2017)</p>
<p>In our example we have one Transformer layer so N=1 (as shown in the diagram). If we had more Transformer layers, the outputs of the first encoder layer would feed into the next encoder layer, and so on, until the last encoder layer. The output from the last encoder layer would then feed into each of the decoder layers for an attention calculation, where each decoder layer after the first taking its input from the encoder and the preceding decoder layer.</p>
<p>One element in the diagram missing in our implementation is that there is no softmax layer in our Transformer. This is because the PyTorch implementation of the cross-entropy metric that we are optimising in training automatically applies a softmax so it does not need to be added to train the model. However, for the translation after training we want the model to output a probability distribution and so apply a softmax layer to the outputs of the trained Transformer there.</p>
<h3 id="4-the-implementation-of-attention">4. The implementation of attention<a hidden class="anchor" aria-hidden="true" href="#4-the-implementation-of-attention">#</a></h3>
<p>The following code forms part of the Transformer script <code>Transformer.py</code> which is the core model script.
Below is the implementation of attention that Andrej Karpathy used in nanoGPT (there are some minor changes to comments and removing some parts not used at this stage as the original nanoGPT was designed for language generation). As with all PyTorch network classes it inherits from the nn.Module class and sets up the components that are used to construct the network when it is initiated. The forward method then details what happens when the data is input into the network layers that are specified in the class.</p>
<pre tabindex="0"><code>import torch
import torch.nn.functional as Fun
from torch import nn
from config import TransformerConfig

# Initialize configuration
config = TransformerConfig()

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Checks that the dimension of the embedding vector can be divided by the number of heads
        assert config.dim_embedding % config.n_head == 0

        # set embedding and head sizes
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding

        # nn.Linear applies a linear y=Ax+b transformation to the input
        # the input dimension is the first argument
        # the output dimension is the second argument
        # the last argument is the b (bias) term

        # Sets up a layer that increases the dimensionality of the embedding 3x to calculate the query, key and value vectors
        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )

        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.dropout = config.dropout

        # Uses a faster implementation of attention if scaled_dot_product_attention available in module torch.nn.functional
        # (which it is from PyTorch &gt;= 2.0)
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )

    def forward(self, x):
        # Get the values of the batch size, sequence length and embedding dimensionality 
        (
            B,
            T,
            C,
        ) = (
            x.size()
        )  
        # calculate query, key, values
        # by splitting the output of the attention layer into tensors of dimension dim_embedding on the 2nd dimension
        q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2)
        # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            # self.training is set to true when model.train() is initiated
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=False,
            )

        else:
            # manual implementation of attention
            # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
       
            # Apply the softmax layer so that everything sums to 1
            att = F.softmax(att, dim=-1)

            # Apply dropout
            att = self.attn_dropout(att)

            # Multiply the attention results by the value vectors
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)

        # Change the shape of the tensor back to B, T, C removing the heads
        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side
      
        # output projection and droput
        y = self.resid_dropout(self.c_proj(y))
       
        return y
</code></pre><p>The code below shows the creation of key, query and value vectors from the embeddings, which is implemented by passing the embedding vectors into
c_attn(x) which generates these three types of vectors. These vectors are then split again by the number of heads to perform the attention calculation.
The original vectors used to produce the query, key and value vectors have 600 dimensions and there are 6 heads, so this produces 6 sets of query, key and value vectors of 100 dimensions each, for 6 different sets of parallel attention calculations. We start with a tensor of size <code>(B, T, C)</code> where B is batch size, T is block size and C the embedding dimensionality of the data entering the Transformer. This then gets converted to a tensor of size <code>(B, nh, T, hs)</code> as C is split into the number of separate heads nh and the size of the vectors hs they process. At the end of the attention calculation the results from the different heads are combined and we end back at <code>(B, T, C)</code>.</p>
<pre tabindex="0"><code>q, k, v  = self.c_attn(x).split(self.dim_embedding, dim=2)
# split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs
</code></pre><p>The method contains an optimised attention mechanism to process these collections of vectors <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">scaled_dot_product_attention</a>. Here we have <code>is_causal=True</code> as this is the encoder side and in
translation we assume that the Transformer has access to the full German text before starting translation. This means that attention is calculated between all the vectors.</p>
<p>By contrast on the decoder side in the Masked Multi-Head Attention layer <code>is_causal=False</code> (and attention calculated using previous words only) when processing the English translations as in generating the prediction this is done autoregressively with the prediction for the next word of the translation based on the predicted words before. In the training data therefore, to predict the n&rsquo;th word of the translation the Transformer is given access to the n-1 English words before. For the very first translated word the <code>[start]</code> token is used as the &lsquo;word 0&rsquo; to initialise the translation.</p>
<pre tabindex="0"><code>y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
</code></pre><p>If <code>scaled_dot_product_attention</code> is not available then the class also implements attention
directly in Python. This illustrates more explicitly the stages in calculating attention.
The multiplication of the query and the transposed key matrices of associated vectors and then normalising the results by the square root of the key
vectors' dimensionality:</p>
<pre tabindex="0"><code>att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
</code></pre><p>The application of the softmax layer:</p>
<pre tabindex="0"><code>att = F.softmax(att, dim=-1)
</code></pre><p>The multiplication of the attention weights by the values vectors:</p>
<pre tabindex="0"><code> y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
</code></pre><p>The concatenation of the head outputs, which creates vectors of the standard embedding dimension, which are then transformed by another set of weights into a vector of the same dimensionality. In the Karpathy implementation he then adds a dropout layer.</p>
<pre tabindex="0"><code> y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

# output projection
 y = self.resid_dropout(self.c_proj(y))
</code></pre><h3 id="5-the-implementation-of-other-transformer-components">5. The implementation of other Transformer components<a hidden class="anchor" aria-hidden="true" href="#5-the-implementation-of-other-transformer-components">#</a></h3>
<p>The other Transformer components that are implemented are:</p>
<p><strong>1. The Embedding layers</strong><br>
<strong>2. Processing layers</strong><br>
<strong>3. Masked Multi-Head Attention layers</strong><br>
<strong>4. Encoder Decoder Attention layers</strong></p>
<p><strong>5.1 The Embedding layers</strong></p>
<p>The Embedding class below takes in the encoded text and then passes each word to two embedding layers. There is the word embedding layer (wte) and the position embedding layer (wtp). These map the word and its position in the text into two vectors of the chosen embedding dimensionality. These two vectors are then added together encoding both the word and its position in the text in a single vector. This is done for all of the 20 words/tokens in a piece of text fed into the network, and their relative position in the text.</p>
<pre tabindex="0"><code>class Embedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        # Creates the text embedding and the position embedding
        # nn.Embedding automatically does the one hot encoding so this
        # does not need to be created directly
        self.config = config
        self.wte = nn.Embedding(config.vocab_size, config.dim_embedding)
        self.wtp = nn.Embedding(config.block_size, config.dim_embedding)

    def forward(self, x):
        # Generates the word embedding from the text
        x = self.wte(x)
        #  Generates the position embedding is applied over a tensor representing word position 
        position_ids = (
            torch.arange(self.config.block_size).unsqueeze(0).repeat(x.size(0), 1)
        )
        position_embeddings = self.wtp(position_ids)
        # Add the two embeddings
        x = x + position_embeddings

        return x

</code></pre><p><strong>5.2 Processing layers</strong></p>
<p>The LayerNorm class applies layer normalisation, calculating the mean and standard deviation across the batch for each word position in the text and elements of the vector
per word and then subtracting these means and dividing by the standard deviation from the corresponding elements across the batch.</p>
<pre tabindex="0"><code>class LayerNorm(nn.Module):
    &quot;&quot;&quot; LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False &quot;&quot;&quot;

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return Fun.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
</code></pre><p>A series of layers that implement layer normalisation, a dense layer again and then a dense layer with a skip connection and then layer normalisation.
We label this pattern of layers which appears in both the decoder and encoder as processing layers.</p>
<pre tabindex="0"><code>class ProcessingLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.denselayer1 = nn.Linear(config.dim_embedding, config.dim_inner_layer)
        self.denselayer2 = nn.Linear(config.dim_inner_layer, config.dim_embedding)
        self.layernorm1 = LayerNorm(config.dim_embedding, bias=config.bias)
        self.layernorm2 = LayerNorm(config.dim_embedding, bias=config.bias)

    def forward(self, x):
        # A layer norm and then two dense layers\n&quot;,
        x_in = self.layernorm1(x)
        x = self.denselayer1(x_in)
        x = self.denselayer2(x) + x_in
        x = self.layernorm2(x)
        return x


</code></pre><p><strong>5.3 Masked Multi-Head Attention layers</strong></p>
<p>As the english translation is generated one word at a time, in training we assume that attention is only calculated between the words prior to given a word i.e. words that would be already known in producing the translation. To do this we apply a mask to the attention calculation to restrict the calculation of attention in this way.</p>
<p>There is a class that implements masked multi-head attention for the text in the language we want to translate into. This forms the first attention layer of the decoder processing the english text translations during training and the generated english text translations during translation. In the scaled_dot_product_attention implementation of attention this is implemented by applying <code>is_causal=True</code>.</p>
<p>In the alternative direct implementation of attention the mask is applied directly to attention using the mask in the line below:<br>
<code>att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))</code><br>
where the bias is defined in the term:</p>
<pre tabindex="0"><code>  self.register_buffer(
                &quot;bias&quot;,
                torch.tril(torch.ones(config.block_size, config.block_size)).view(
                    1, 1, config.block_size, config.block_size
                ),
            )

</code></pre><p>The register_buffer creates a tensor called &ldquo;bias&rdquo; that will not be updated during training, unlike the model weights. The bias term has the form of a lower triangular matrix. If the block size had three words, then the register buffer has size [1,1,3,3] and looks like this:</p>
<pre tabindex="0"><code>tensor([[[[1., 0., 0.],
          [1., 1., 0.],
          [1., 1., 1.]]]])
</code></pre><p>In the masked_fill T is the block_size and the upper triangle elements that are 0 are set to -inf in the mask. When the mask is applied to the attention the lower triangular elements corresponds to the attention of the first word being based on the word itself, the attention of the second word being based on the first and second word etc etc maintaining the causality that attention is only calculated between a given word and its predecessors (but not future words). As the next layer is a softmax layer which has exponential functions, exp^0 = 1, so a 0 as an input would still give positive values for the upper diagonal elements, but as the input tends to -inf the exponentials tend to 0 so setting 0 elements to -inf removes them from the output post softmax layer.</p>
<pre tabindex="0"><code>class MaskedMultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        # Checks that the dimension of the embedding vector can be divided by the number of heads
        assert config.dim_embedding % config.n_head == 0
        
        # set embedding and head size
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding

        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )

        # regularisation
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.dropout = config.dropout

        # Uses a faster implementation of attention if scaled_dot_product_attention available in module torch.nn.functional
        # (which it is from PyTorch &gt;= 2.0)
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(
                &quot;bias&quot;,
                torch.tril(torch.ones(config.block_size, config.block_size)).view(
                    1, 1, config.block_size, config.block_size
                ),
            )

    def forward(self, x):
        # Get the values of the batch size, block size and embedding dimensionality 
        (
            B,
            T,
            C,
        ) = (
            x.size()
        )  

        # calculate query, key and value vectors
        # by splitting the output of the attention layer into tensors of dimension dim_embedding on the 2nd dimension
        q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2)

        # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)

        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=True,
            )
       
        else:
            # manual implementation of attention
            # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))

            # Calculate the masked attention
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))

            # Apply the softmax layer so that everything sums to 1
            att = F.softmax(att, dim=-1)
            
            # Apply dropout
            att = self.attn_dropout(att)

            # Multiply the attention results by the value vectors
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
            
        # Change the shape of the tensor back to B, T, C removing the heads
        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side

        # output projection and dropout
        y = self.resid_dropout(self.c_proj(y))

        return y
</code></pre><p><strong>5.4 Encoder Decoder Attention layer</strong></p>
<p>The second attention layer in the decoder (<code>the Encoder Decoder Attention layer</code>) has two inputs: the output from the encoder and the normalised output from the decoder&rsquo;s masked attention layer. In the encoder decoder attention layer the encoder output is transformed into key and value vectors. These vectors are then combined in an attention calculation (without masking) with the query vectors created from the vectors output from the decoder&rsquo;s previous layer.</p>
<pre tabindex="0"><code>class EncoderDecoderAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Checks that the dimension of the embedding vector can be divided by the number of heads
        assert config.dim_embedding % config.n_head == 0

        # set embedding and head sizes
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding
 
        self.c_attn_en = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )

        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )

        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.dropout = config.dropout

        # Uses a faster implementation of attention if scaled_dot_product_attention available in module torch.nn.functional
        # (which it is from PyTorch &gt;= 2.0)
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )

    def forward(self, x, e):

       # Get the values of the batch size, block size and embedding dimensionality 
        (
            B,
            T,
            C,
        ) = (
            e.size()
        )  

        # calculate the key and value vectors from the output of the encoder
        _, k, v = self.c_attn_en(e).split(self.dim_embedding, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
       
        # calculate the query vectors from the output of the previous decoder layers
        q, _, _ = self.c_attn(x).split(self.dim_embedding, dim=2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=False,
            )
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(attn_mask == 0, float(&quot;-inf&quot;))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side

        # output projection and dropout
        y = self.resid_dropout(self.c_proj(y))

        return y
</code></pre><h3 id="6-putting-all-the-components-together-to-create-a-transformer">6. Putting all the components together to create a Transformer<a hidden class="anchor" aria-hidden="true" href="#6-putting-all-the-components-together-to-create-a-transformer">#</a></h3>
<p>We now use all of these components to create an encoder layer and a decoder layer. The encoder and decoder are then combined together into a single Transformer class which we will initialise and then train.</p>
<pre tabindex="0"><code># The encoder class

class Encoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Create embeddings for the encoder 
        self.encoder_embed = Embedding(config)

        # Create an attention mechanism for the encoder
        self.attention_encoder = MultiHeadAttention(config)

        # Set up a processing layer
        self.encoder_processing_layer = ProcessingLayer(config)

    def forward(self, x):
        # Encode the language we want to translate from
        x = self.encoder_embed(x)

        # Apply the attention mechanism and add the input
        x = self.attention_encoder(x) + x

        # apply layer norm, two dense layers and a layer norm again
        x = self.encoder_processing_layer(x)

        return x

# The decoder class

class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Create an embedding layer for the decoder 
        self.decoder_embed = Embedding(config)

        # Create an attention mechanism for the decoder
        self.attention_decoder = MaskedMultiHeadAttention(config)

        # Create a layernorm layer
        self.layernorm = LayerNorm(config.dim_embedding, bias=config.bias)

        # Create the encoder decoder attention
        self.decoder_attn = EncoderDecoderAttention(config)

        # Set up a processing layer for the decoder
        self.decoder_processing_layer = ProcessingLayer(config)

        # The final layer which maps the models embedding dimension back to the vocab size
        self.final_layer = nn.Linear(config.dim_embedding, config.vocab_size)

    def forward(self, x, y):

        # Encode the language we want to translate into
        y = self.decoder_embed(y)

        # Apply the attention mechanism and add the input
        y = self.attention_decoder(y) + y
        
        # Apply layer norm
        y = self.layernorm(y)

        # Take the output from the encoder and last layer of decoder and calculate attention again then add the input
        y = self.decoder_attn(y, x) + y

        # apply layer norm, two dense layers and a layer norm again
        y = self.decoder_processing_layer(y)
        
        # Map the embedding dimension back to the vocabularly size
        y = self.final_layer(y)

        return y

# The Transformers class

class Transformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create both the encoder and the decoder layers
        self.encoder = Encoder(config)
        self.decoder = Decoder(config)

    def forward(self, x, y):
        # Encode the language we want to translate from
        encoder_out = self.encoder(x)

        # Take the output from the encoder and translated text and pass to the decoder
        y = self.decoder(encoder_out, y)

        return y
</code></pre><h3 id="7-training">7. Training<a hidden class="anchor" aria-hidden="true" href="#7-training">#</a></h3>
<p>The code below is from the python script <code>train.py</code> which trains the model for 30 epochs. It updates the model weights to minimise the cross-entropy between the predicted translations and the actual translations. In evaluating the metric the 0 padding tokens are ignored. The loss on the training data is output to the screen from batches at regular intervals during each epoch of training. The average loss  on the test dataset is computed at the end of each epoch. The model uses an Adam optimiser.</p>
<p>Starting training a new model from scratch is run with:</p>
<pre tabindex="0"><code>$ python train.py &quot;new_model&quot;
</code></pre><p>At the end of each epoch the model and optimizer state are saved to a file indexed by the epoch number. If the training is stopped and one wants to resume from a saved model, then the following format is used (here the model passed to commence training is that saved after the 11th epoch).</p>
<pre tabindex="0"><code>$ python train.py model_post_11
</code></pre><p>Training the model for 30 epochs on a Mac Air M2 with 24 GB RAM doing other things took about 7.5 days.</p>
<pre tabindex="0"><code>from torch.utils.data import DataLoader
from torch.utils.data.dataset import random_split
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as Fun
from torch import nn
import torch.optim as optim

import sys


# Import the model config and the Transformer class
import config
from Transformer import *

writer = SummaryWriter()

model = Transformer(config)


# Import the training and test datasets and convert them into data loaders
train_dataset = torch.load(&quot;train_dataset.pt&quot;)
test_dataset = torch.load(&quot;test_dataset.pt&quot;)

train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True)

torch.set_grad_enabled(True)

optimizer = optim.Adam(model.parameters())


def save_checkpoint(model, optimizer, save_path, epoch):
    &quot;&quot;&quot;function to save checkpoints on the model weights, the optimiser state and epoch&quot;&quot;&quot;
    torch.save(
        {
            &quot;model_state_dict&quot;: model.state_dict(),
            &quot;optimizer_state_dict&quot;: optimizer.state_dict(),
            &quot;epoch&quot;: epoch,
        },
        save_path,
    )

# If a &quot;new_model is being created don't load anything, but if saved model is input load the model and the optimizer state and the last epoch value
if sys.argv[1] == &quot;new_model&quot;:
    epoch_start = 0
elif sys.argv[1] is not None:
    state = torch.load(sys.argv[1])
    model.load_state_dict(state[&quot;model_state_dict&quot;])
    optimizer.load_state_dict(state[&quot;optimizer_state_dict&quot;])
    epoch_start = state[&quot;epoch&quot;]


# The training loop (The + 1 is to get the numbers we want from range)
for epoch in range(epoch_start + 1, config.epochs + 1):
    # Set the model to training mode
    model.train()
    for i, (german, english, output) in enumerate(train_dataloader):

        # clear any existing gradients before running a new batch
        optimizer.zero_grad()

        # Generates a predicted translation
        yhat = model(german, english)

        # yhat has dimensions (batch_size, block_size, vocab_size)
        # yhat.view(-1, yhat.size(-1)) has the dimensions ((batch_size X block_size), vocab_size)
        # output has the dimensions (batch_size, block_size)
        # output.view(-1) has the dimensions of (batch_size x block_size)

        # Calculates the cross entropy, ignoring the 0s
        loss = Fun.cross_entropy(
            yhat.view(-1, yhat.size(-1)), output.view(-1), ignore_index=0
        )
        # write the loss
        writer.add_scalar(&quot;Loss/train&quot;, loss, epoch)

        # computes the grad of how the loss changes as the weight changes 
        loss.backward()

        # update the weights
        optimizer.step()

        # print how the training is doing at regular stages during the epoch
        if i % 400 == 0:
            print(f&quot;Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}&quot;)
        writer.flush()

    # Save the model and optimizer
    path_to_save = &quot;model_post_&quot; + str(epoch)
    save_checkpoint(model, optimizer, path_to_save, epoch)

    # Compute average loss on the test set
    model.eval()  # Set the model to evaluation mode turning off dropout
    val_loss = 0.0
    with torch.no_grad():  # No gradient computation for validation
        for german_test, english_test, output_test in test_dataloader:
            yhat = model(german_test, english_test)
            loss = Fun.cross_entropy(
                yhat.view(-1, yhat.size(-1)), output_test.view(-1), ignore_index=0
            )
            val_loss += loss.item()
        avg_val_loss = val_loss / len(test_dataloader)
        writer.add_scalar(&quot;Loss/test&quot;, avg_val_loss, epoch)
        print(f&quot;Epoch: {epoch},  Avg_val_loss: {avg_val_loss}&quot;)
</code></pre><h3 id="8-generation-of-translations">8. Generation of translations<a hidden class="anchor" aria-hidden="true" href="#8-generation-of-translations">#</a></h3>
<p>The script below <code>translate.py</code> generates translations. It imports the test dataset converting it to a dataloader and the two dictionaries that map the english and german words to numbers. It imports the saved trained model that is identified in the first argument of the script (here it is assumed to be saved in the same directory) and a sentence to translate as the second argument.</p>
<pre tabindex="0"><code>$ python translate.py model_post_30 &quot;das wetter ist gut&quot;
</code></pre><p>If no text to translate is provided as a second argument, the model loops through the test data set generating translations.</p>
<pre tabindex="0"><code>import torch.nn.functional as Fun
from torch import nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pickle
import text_processing as text_pro

from Transformer import *
import sys


# Load the english and german dictionaries
with open(&quot;german_dictionary.pkl&quot;, &quot;rb&quot;) as file_ger:
    # Load the pickled object
    german_tokens = pickle.load(file_ger)


with open(&quot;english_dictionary.pkl&quot;, &quot;rb&quot;) as file_en:
    # Load the pickled object
    english_tokens = pickle.load(file_en)

# Reverse the dictionaries to go from the numbers back to the words
# This is used previously in text processing consider refactoring
decode_to_english = {v: k for k, v in english_tokens.items()}

decode_to_german = {v: k for k, v in german_tokens.items()}


# Functions to convert the sentences back into words from numbers

def source_vectorization(x):
    &quot;&quot;&quot;Converts the German words into numbers&quot;&quot;&quot;
    return [
        german_tokens[element]
        for element in x.split()
        if element in german_tokens.keys()
    ]


def target_vectorization(x):
    &quot;&quot;&quot;Converts the English words into numbers&quot;&quot;&quot;
    return [
        english_tokens[element]
        for element in x.split()
        if element in english_tokens.keys()
    ]

# Load the test dataset and convert into a data loader of batch size 1
test_dataset = torch.load(&quot;test_dataset.pt&quot;)
test_dataloader = DataLoader(test_dataset, batch_size=1 , shuffle=True)

# Set up the Transformer and load its configuration
model_predict = Transformer(config)

# Take the model that the script is going to use to translate from as the first argument in the command line
model = sys.argv[1]

state = torch.load(model)

# Loads the model
model_predict.load_state_dict(state[&quot;model_state_dict&quot;])

</code></pre><p>The section below generates the translations. It takes the sentence we want to translate and then:</p>
<ol>
<li>
<p>Converts that text into integers using the coding developed in the text processing phase, adding any 0 padding needed to get to 20 tokens.</p>
</li>
<li>
<p>Adds an extra dimension with the unsqueeze(0) so that the model which is trained on batches of 30, can read a single sentence rather than needing a batched input of multiple texts i.e. it is effectively creating a dummy batch dimension for the sentence so the model can read the sentence as if it were in a batch.</p>
</li>
<li>
<p>The model then generates the translation one token at a time. It starts with the text we want the trained model to translate (converted into numbers) and a blank translation sentence consisting of the number representing the [start] token and then the 19 0s which forms the start of the translation.</p>
</li>
<li>
<p>The model takes these two inputs and generates a probability
distribution for its prediction of the first word of the translation. It then samples from the probability distribution i.e. the word with highest predicted probability is most likely to be chosen.</p>
</li>
<li>
<p>The number representing the predicted word is then added to the decoded sentence as the next word after [start], so it is now the numbers representing 2 words and 18 0s.</p>
</li>
<li>
<p>The original sentence and the updated translation sentence is then input into the trained model to generate the second word of the translation etc etc.</p>
</li>
<li>
<p>All of the above is in terms of the numbers that represent the words in both languages. As the sentence is built it gets translated back from the numbers to the english words. This continues until the model predicts the token [end] and the translation process stops.</p>
</li>
</ol>
<pre tabindex="0"><code># Sets the model state to evaluate
model_predict.eval()

def prediction(x, y):
    &quot;&quot;&quot;This gives the probability distribution over English words for a given German translation&quot;&quot;&quot;
    logits = model_predict(x, y)
    logits = logits.squeeze(0)  # Will remove the first dimension if it is set to 0
    # The dim = -1 applies softmax over the last dimension of the tensor
    return Fun.softmax(logits, dim=-1)

def decode_sequence(input_sentence):
    &quot;&quot;&quot;This function generates the translation&quot;&quot;&quot;

    # Unsqueezing adds an extra dimension so that the model which was trained on batches can read single sentences
    tokenized_input_sentence = text_pro.tensor_pad(source_vectorization(input_sentence))[
        : config.block_size
    ].unsqueeze(0)

    #  initalises the decoded sentence with [start]
    decoded_sentence = &quot;[start]&quot;

    # Loop through the sentence word by word
    for i in range(0, config.block_size):
        tokenized_target_sentence = text_pro.tensor_pad(target_vectorization(decoded_sentence))[
            : config.block_size
        ].unsqueeze(0)

        # Generate predictions
        predictions = prediction(tokenized_input_sentence, tokenized_target_sentence)

        # The first index in the predictions tensor is the word position in the sentence
        # the second index is the predicted word
        # The .item() extracts the tensor index from the tensor
        sampled_token_index = torch.multinomial(predictions[i, :], num_samples=1).item()

        # Gets the word corresponding to the index
        sampled_token = decode_to_english[sampled_token_index]

        # Appends the word to the predicted translation to date
        decoded_sentence += &quot; &quot; + sampled_token

        # If the predicted token is [end] stop
        if sampled_token == &quot;[end]&quot;:
            break
    return decoded_sentence


def trans(x, lan):
    &quot;&quot;&quot;This is a function to translate the English and German text from the number-word dictionaries that we have of them&quot;&quot;&quot;
    results = &quot;&quot;
    for elem in x:
        if elem != 0:
            if lan == &quot;ger&quot;:
                results = results + &quot; &quot; + decode_to_german[elem]
            if lan == &quot;eng&quot;:
                results = results + &quot; &quot; + decode_to_english[elem]
    return results


# Style class to format the print statements
class style:
    BOLD = &quot;\033[1m&quot;
    UNDERLINE = &quot;\033[4m&quot;
    END = &quot;\033[0m&quot;


# If there are no arguments other than the script itself and the model evaluate the test sample at regular intervals
if len(sys.argv)==2:
        
# Looking at selected sentences from the test data
 for i, elem in enumerate(test_dataloader):
    if i % 3400 == 0:
    
        print(style.BOLD + &quot;Orginal&quot; + style.END)
        german = trans(elem[0].tolist()[0], &quot;ger&quot;)
        print(german)
        print(style.BOLD + &quot;Translation&quot; + style.END)
        print(trans(elem[1].tolist()[0], &quot;eng&quot;))
        print(style.BOLD + &quot;Machine Translation&quot; + style.END)
        print(decode_sequence(german))
        print(&quot;\n&quot;)
                
# If there is also an additional argument i.e. a sentence to translate, translate the sentence and then exit
elif len(sys.argv) == 3:
       print(decode_sequence(sys.argv[2]))
       sys.exit()
        
</code></pre><p>Looking at a sample of random test German sentences fed in we obtain the following machine translations from the Transformer compared to the English translation in the test data :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">**Orginal**
 wann hast du das geräusch gehört
**Translation**
 [start] when did you hear the sound
**Machine Translation**
[start] when did you hear the noise [end]


**Orginal**
 schenke dem keine beachtung
**Translation**
 [start] dont pay any attention to that
**Machine Translation**
[start] pay attention to disobeyed in [end]


**Orginal**
 ich habe die bücher die ich mir aus der bibliothek ausgeliehen hatte zurückgebracht und mir ein paar neue ausgeliehen
**Translation**
 [start] i returned the books i borrowed from the library and i borrowed some new ones
**Machine Translation**
[start] ive wears a few books that for me [end]


**Orginal**
 wie oft essen sie schokolade
**Translation**
 [start] how often do you eat chocolate
**Machine Translation**
[start] how often eat chocolate [end]


**Orginal**
 ich kann ein geheimnis bewahren
**Translation**
 [start] i can keep a secret
**Machine Translation**
[start] i can drive a secret [end]


**Orginal**
 möchtest du wissen wer das gemacht hat
**Translation**
 [start] do you want to know who did this
**Machine Translation**
[start] do you want to know who was doing that [end]


**Orginal**
 ich möchte mit deiner mutter sprechen
**Translation**
 [start] i want to talk to your mother
**Machine Translation**
[start] i want to know with your mother [end]
</code></pre></div><p>Two of the translations are essentially correct, but for slight differences in emphasis which will not matter in many cases. One is understandable, but has a grammatical error. Three have several of the words right, but due to some mistakes do not have a clear meaning. One of the translations is completely wrong.</p>
<p>Trying it on an example of one of the standard challenges for English speakers learning German, the fact that meanings can change with variations of the definite article, it translates &ldquo;ich renne in den park&rdquo; correctly as &ldquo;i am running into the park&rdquo;, but &ldquo;ich renne in der park&rdquo; is translated to the same thing, when it should be &ldquo;i am running in the park&rdquo; (The switch from die to dem changes the meaning).
The model has clearly developed some ability to translate from the data, but it is far from perfect.</p>
<p><strong>References:</strong></p>
<p><a href="https://arxiv.org/abs/1706.03762/">The original Transformers paper &lsquo;Attention is All You Need&rsquo;,
Vaswani et al. (2017)</a></p>
<p><a href="https://github.com/karpathy/nanoGPT/">The repo for Andrej Karpathy&rsquo;s nanoGPT</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s ‘The illustrated transformer’</a></p>
<p>François Chollet’s book ‘Deep Learning with Python (2nd edition)’</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">OpenAI&rsquo;s, &lsquo;Language Models are Few-Shot Learners&rsquo;</a></p>
<p><strong>Previous Transformer notes</strong>:</p>
<ol>
<li><a href="https://t.co/hFufxkFXaQ"><strong>Transformers 1: The Attention Mechanism</strong></a></li>
<li><a href="https://johnardavies.github.io/technical/transformer2/"><strong>Transformers 2: The Transformer&rsquo;s Structure</strong></a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
      <li><a href="https://johnardavies.github.io/tags/digital/">digital</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
