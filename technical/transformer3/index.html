<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers 3: Building and training the Transformer | John&#39;s Site</title>
<meta name="keywords" content="technical, digital" />
<meta name="description" content="Printworks, April 2023
Transformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/transformer3/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Transformers 3: Building and training the Transformer" />
<meta property="og:description" content="Printworks, April 2023
Transformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/transformer3/" />
<meta property="og:image" content="https://johnardavies.github.io/printworks.jpg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2024-05-18T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-05-18T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/printworks.jpg" />
<meta name="twitter:title" content="Transformers 3: Building and training the Transformer"/>
<meta name="twitter:description" content="Printworks, April 2023
Transformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers 3: Building and training the Transformer",
      "item": "https://johnardavies.github.io/technical/transformer3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers 3: Building and training the Transformer",
  "name": "Transformers 3: Building and training the Transformer",
  "description": "Printworks, April 2023\nTransformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet\u0026rsquo;s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow.",
  "keywords": [
    "technical", "digital"
  ],
  "articleBody": "Printworks, April 2023\nTransformers 3: Building and training the Transformer Having discussed how attention works and the structure of Transformers we’ll now implement a simple Transformer that takes German text and generates English translations. To do this we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet’s book ‘Deep Learning with Python’ which is written in Keras/TensorFlow. The relatively small scale of the model and data involved, means that it won’t be able to completely translate text, but will be able to generate translations to some extent.\nThis note will cover:\n1. Processing the text data\n2. The implementation of attention\n3. The implementation of other Transformer components\n3. Putting all the components together to create a Transformer\n4. Training\n5. Generation of translations\nTo start we will initialise the model with a PyTorch dataclass (config.py) that sets out the model’s key parameters:\nfrom dataclasses import dataclass @dataclass class TransformerConfig: \"\"\"With a dataclass we don't need to write an init function, just specify class attributes and their types\"\"\" block_size: int = 20 # Was 20 vocab_size: int = 15000 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency dim_embedding: int = 600 dim_inner_layer: int = 2000 n_head: int = 6 dropout: float = 0.2 bias: bool = False epoch: int = 30 batch_size: int = 30 config = TransformerConfig() The model will have an embedding dimension of 600 and a vocabulary size of 15000 words for each language. Attention will be calculated over 6 heads in parallel, which due to how it is implemented involves 100 dimensional query, key and value vectors. During training German text and a corresponding English translation will be input in batches of 30 text-pairs. Both the German text and English translation text are capped at 20 tokens in length. In training we’ll loop through the training data 30 times (30 epochs). 20% dropout is implemented at a number of stages of the network.\n1. Processing the text data\nTo get some text data to train and test the Transformer we download the english-german translation pairs from anki and unzip the file:\n$ wget https://www.manythings.org/anki/deu-eng.zip \u0026\u0026 unzip deu-eng.zip This produces the text file (deu.txt) which consists of pairs of English and German text, for example:\nBless you.\tGesundheit\nBuy a gun.\tLeg dir eine Schusswaffe\nWe want to encode these into numbers (for both English and German). We will process this with the python script text_processing.py. To do this we’ll start by simplifying the text by processing the pairs by removing punctuation and setting the text to lowercase. [start] and [end] tags are added at the beginning and end of all of the English texts.\nimport torch import torch.nn.functional as Fun from torch.utils.data.dataset import random_split from collections import Counter from string import punctuation import pickle from config import * # Strip punctuation def strip_punctuation(s): \"\"\"Function that removes punctuation\"\"\" return str(\"\".join(c for c in s if c not in punctuation)) def tensor_pad(x): \"\"\"converts list to tensor and pads to length 20\"\"\" return Fun.pad( torch.tensor(x, dtype=torch.int64), (0, config.block_size), value=0 ) # Split the text file into german and english # We start each line of the English text with start text_file = \"deu-eng/deu.txt\" with open(text_file) as language_file: lines = language_file.read().split(\"\\n\") print(\"There are \" + str(len(lines)) + \" lines\") text_pairs = [] for i, line in enumerate(lines): try: english, german = line.split(\"\\t\")[0:2] english = \"[start] \" + strip_punctuation(english.lower())+\" [end]\" text_pairs.append([strip_punctuation(german.lower()), english]) except: print(\"failed to load %s\" % (i)) We then calculate the distinct words used in all the German and all the English terms. We then create a class EncodeDecode which is initiated for each language separately and has two methods that are dictionaries allowing the most common 15,000 words in each language to be encoded into numbers and decoded back. More sophisticated encoding based on parts of speech or combinations of words are also possible.\n# To get the tokens used in English and German text we create # two separate lists and from these create a single german # text and a single english text german_list = [item[0] for item in text_pairs] english_list = [item[1] for item in text_pairs] german_text=' '.join(german_list) english_text=' '.join(english_list) class EncodeDecode(): \"\"\"Class that produces two dictionaries from input text, one of which maps words to numbers and the other one reverses this mapping numbers to words.\"\"\" def __init__(self, text, vocab): self.text = text # Get the tokens self.tokens_list = list(set(self.text.split())) self.vocab_size=vocab # Get the most common tokens self.tokens_counter=Counter(self.text.split()).most_common(self.vocab_size) self.tokens_vocab=[item for item, count in self.tokens_counter] def decode(self): # Create a decoder from the numbers to the tokens decoder = {i+1: token for i, token in enumerate(self.tokens_vocab)} return decoder def encode(self): # Create an encoder of the tokens to numbers encoder = {token: i+1 for i, token in enumerate(self.tokens_vocab)} return encoder We then apply the dictionaries to encode the English and German texts into numbers. The English translation text is then processed into two copies. One which has the [end] token removed which will be input for training and one which has the start token removed which is used as the target text the model is trying to predict. Each German and English text is padded with 0s to get to 20 tokens. The encoder and decoder calculations above start at 1 in order to keep 0 for the padding.\n# Get the German tokens and the English tokens english_tokens = EncodeDecode(english_text, config.vocab_size - 1) german_tokens = EncodeDecode(german_text, config.vocab_size - 1) # Creates encoding dictionaries english_encoded = english_tokens.encode() german_encoded = german_tokens.encode() # Save the english and german dictionaries with open(\"english_dictionary_v4.pkl\", \"wb\") as eng: pickle.dump(english_tokens.encode(), eng) with open(\"german_dictionary_v4.pkl\", \"wb\") as ger: pickle.dump(german_tokens.encode(), ger) # Encode the tokens if they are in the 15000 most common tokens text_pairs_encoded = [ [ [ german_encoded[element] for element in pair[0].split() if element in german_tokens.tokens_vocab ], [ english_encoded[element] for element in pair[1].split() if element in english_tokens.tokens_vocab ], ] for pair in text_pairs ] # Split the data between the encode input in german the encoder input in english and the # english output we are looking for # The encoder input is shifted one token to the left of the target # The -1 removes the last token in the list # The elem[1][:-1] is the language we are trying to translate into that is input during training # The elem[1][1:] is what we're trying to predict text_pairs_encoded_split = [ (elem[0], elem[1][:-1], elem[1][1:]) for elem in text_pairs_encoded ] def tensor_pad(x): \"\"\"converts list to tensor and pads to length 20\"\"\" return Fun.pad( torch.tensor(x, dtype=torch.int64), (0, config.block_size) #, value=0 ) # applies tensorpad text_pairs_encoded_padded = [ [ tensor_pad(item1)[: config.block_size], tensor_pad(item2)[: config.block_size], tensor_pad(item3)[: config.block_size], ] for item1, item2, item3 in text_pairs_encoded_split ] # Calculate how many observations are needed for a 20% test sample test_len = round(len(text_pairs_encoded_padded) * 0.2) # Calculate the number of training observations as residual train_len = round(len(text_pairs_encoded_padded)) - test_len # Get the train dataset and the test dataset train_dataset, test_dataset = random_split( text_pairs_encoded_padded, [train_len, test_len] ) # Save the test and train datasets as pickle files torch.save(train_dataset, \"train_dataset.pt\") torch.save(test_dataset, \"test_dataset.pt\") 2. The implementation of attention in PyTorch\nThe following code forms part of the transformers script Transformer.py which is the core model script. Below is the implementation of attention that Andrej Karpathy used in nanoGPT. As with all PyTorch network classes it sets up the parts that are used to construct the network when it is initiated. The forward method then details what happens when the data is input into an initialised attention layer. All layer classes inherit from the nn.Module class which is the class for all neural network modules.\n# The imports for the Transformer script import torch import torch.nn.functional as Fun from torch import nn from config import * class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() assert config.dim_embedding % config.n_head == 0 # key, query, value projections for all heads, but in a batc # nn.Linear applies a linear y=Ax +b transformation to the input # the input dimension is the first argument # the output dimension is the second argument # the last argument is the b (bias) term self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.dropout = config.dropout # flash attention make GPU go brrrrr but support is only in PyTorch = 2.0 self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) def forward(self, x): # Split the input tensor dimension down into the batch size B , the sequence length T and the emnedding dimensionality G ( B, T, C, ) = ( x.size() ) # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) if self.flash: # efficient attention using Flash Attention CUDA kernels # self.training is set to true when model.train() is initiated y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False, ) else: # manual implementation of attention # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Apply the softmax layer so that everything sums to 1 att = F.softmax(att, dim=-1) att = self.attn_dropout(att) # Multiply the attention results by the value vectors y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) # Change the shape of the tensor back to B, T, C removing the heads print(y.size(), \"post matrix mult\") y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # print(y.size(), 'proj') # output projection y = self.resid_dropout(self.c_proj(y)) # print(y.size()) return y This shows creation of key, query and value vectors from the embeddings, which is implemented by passing the embedding vector into c_attn(x) which generates these three vectors. These vectors are then split again by the number of heads to perform the attention calculation. The vectors for each of the query key and value vectors have 600 dimensions and there are 6 heads, so we end up with 6 sets of 100 dimensional vectors for each of the original query, key and value vectors.\nq, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs The method contains an optimised attention mechanism that uses flash attention to process these collections of vectors. Here we have is_causal=True as this is the encoder side and in translation we assume that the Transformer has access to the full German text before starting translation. By contrast on the encoder side is_causal=False when processing the English translations as in generating the prediction this is done recursively with the prediction for the next word of the translation based on the predicted words before. In the training data therefore, to predict the n’th word of the translation access is given to the n-1 English words before. For the very first translated word the [start] token is used to initialise the translation.\n y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True) If flash attention is not available there is also a version that implements attention directly in Python. This illustrates more explicitly the components that make up attention. The matrix multiplication of query and the transposed key matrix and then normalising the results by the square root of the key vector’s dimensionality:\natt = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) The application of the softmax layer:\natt = F.softmax(att, dim=-1) The multiplication of the attention weights by the values vectors:\n y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) The concatenation of the head outputs and then the projection of resulting vector down to the model’s embedding dimension. In the Karpathy implementation he then adds a dropout layer.\n y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) 3. The implementation of other Transformer components\nThe Embedding class below takes in the encoded text and then passes each word to two embedding layers. There is the word embedding layer (wte) and the position embedding layer (wtp). These map the word and its position into two vectors of the chosen embedding dimensionality. These two vectors are then added together encoding both the word and its position in the text in a single vector. This is done for all of the 20 words/tokens in a piece of text fed into the network, and their relative position in the text.\nclass Embedding(nn.Module): def __init__(self, config): super().__init__() # Creates the text embedding and the position embedding # The embedding automatically does the one hot encoding so this # does not need to be created directly self.config = config self.wte = nn.Embedding(config.vocab_size, config.dim_embedding) self.wtp = nn.Embedding(config.block_size, config.dim_embedding) def forward(self, x): # Applies the word embedding and then adds it to the position embedding x = self.wte(x) # The position embedding is applied over a tensor that ranges 0 to 20 # The embedding is torch.Size([20, 200]) and is broadcast onto the larger tensor position_ids = ( torch.arange(self.config.block_size).unsqueeze(0).repeat(x.size(0), 1) ) positions = torch.arange(0, config.block_size) position_embeddings = self.wtp(position_ids) x = x + position_embeddings return x The LayerNorm class applies layer normalisation, calculating the mean and standard deviation across the batch for each position in the text and elements of the vector per word and then subtracting these means and dividing by the standard deviation from the corresponding elements across the batch.\nclass LayerNorm(nn.Module): \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\" def __init__(self, ndim, bias): super().__init__() self.weight = nn.Parameter(torch.ones(ndim)) self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None def forward(self, input): return Fun.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5) A series of layers that implements a layer norm, a dense layer with a skip connection and then a dense layer again: We label this pattern of layers, which appears in both the decoder and encoder as processing layers.\nclass Processing_layer(nn.Module): def __init__(self, config): super().__init__() self.denselayer1 = nn.Linear(config.dim_embedding, config.dim_inner_layer) self.denselayer2 = nn.Linear(config.dim_inner_layer, config.dim_embedding) self.layernorm1 = LayerNorm(config.dim_embedding, bias=config.bias) self.layernorm2 = LayerNorm(config.dim_embedding, bias=config.bias) def forward(self, x): # A layer norm and then two dense layers\\n\", x_in = self.layernorm1(x) x = self.denselayer1(x_in) x = self.denselayer2(x) + x_in x = self.layernorm2(x) return x There is also a class that implements masked causal self attention:\nclass MaskedMultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() assert config.dim_embedding % config.n_head == 0 # key, query, value projections for all heads, but in a batc # nn.Linear applies a linear y=Ax +b transformation to the input # the input dimension is the first argument # the output dimension is the second argument # the last argument is the b (bias) term self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.dropout = config.dropout # flash attention make GPU go brrrrr but support is only in PyTorch = 2.0 self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer( \"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view( 1, 1, config.block_size, config.block_size ), ) def forward(self, x): # Split the input tensor dimension down into the batch size B , the sequence length T and the embedding dimensionality G ( B, T, C, ) = ( x.size() ) # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2) # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True, ) else: # manual implementation of attention # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # Calculate the masked attention att = att.masked_fill(attn_mask == 0, float(\"-inf\")) # Apply the softmax layer so that everything sums to 1 att = F.softmax(att, dim=-1) att = self.attn_dropout(att) # Multiply the attention results by the value vectors y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) # Change the shape of the tensor back to B, T, C removing the heads y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) return y The second attention layer in the decoder has two inputs: the output from the encoder and the output from the previous layer of the decoder’s masked self-attention class. The encoder output is transformed into key and value vectors. These vectors are then combined with the query vectors created from the vectors output from the decoder’s previous layer in an attention calculation.\nclass EncoderDecoderAttention(nn.Module): def __init__(self, config): super().__init__() assert config.dim_embedding % config.n_head == 0 # key, query, value projections for all heads, but in a batch self.c_attn_en = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) self.c_attn = nn.Linear( config.dim_embedding, 3 * config.dim_embedding, bias=config.bias ) # output projection self.c_proj = nn.Linear( config.dim_embedding, config.dim_embedding, bias=config.bias ) # regularization self.attn_dropout = nn.Dropout(config.dropout) self.resid_dropout = nn.Dropout(config.dropout) self.n_head = config.n_head self.dim_embedding = config.dim_embedding self.dropout = config.dropout # flash attention make GPU go brrrrr but support is only in PyTorch = 2.0 self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") if not self.flash: print( \"WARNING: using slow attention. Flash Attention requires PyTorch = 2.0\" ) # causal mask to ensure that attention is only applied to the left in the input sequence self.register_buffer( \"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view( 1, 1, config.block_size, config.block_size ), ) def forward(self, x, e): # Generating the query and key vectors from the vectors from the encoder ( B, T, C, ) = ( e.size() ) # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate query, key, values # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension _, k, v = self.c_attn_en(e).split(self.dim_embedding, dim=2) k = k.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # print(q.size(), k.size()) # Generating the value vector from the decoder input ( B, T, C, ) = ( x.size() ) # batch size, sequence length, embedding dimensionality (dim_embedding) # calculate values for all heads in batch and move head forward to be the batch dim q, _, _ = self.c_attn(x).split(self.dim_embedding, dim=2) q = q.view(B, T, self.n_head, C // self.n_head).transpose( 1, 2 ) # (B, nh, T, hs) # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) - (B, nh, T, T) if self.flash: # efficient attention using Flash Attention CUDA kernels y = torch.nn.functional.scaled_dot_product_attention( q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False, ) else: # manual implementation of attention att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = att.masked_fill(attn_mask == 0, float(\"-inf\")) att = F.softmax(att, dim=-1) att = self.attn_dropout(att) y = att @ v # (B, nh, T, T) x (B, nh, T, hs) - (B, nh, T, hs) y = ( y.transpose(1, 2).contiguous().view(B, T, C) ) # re-assemble all head outputs side by side # output projection y = self.resid_dropout(self.c_proj(y)) return y 3. Putting all the components together to create a Transformer\nWe now combine all of these components to create an encoder and decoder class which we then combine together into a single Transformer class which we will initialise and then train.\nclass Encoder(nn.Module): def __init__(self, config): super().__init__() assert config.vocab_size is not None assert config.block_size is not None self.config = config # Create embeddings for both the encoder and the decoder self.encoder_embed = Embedding(config) # Create an attention mechanism for the encoder self.attention_encoder = MultiHeadAttention(config) # Set up a processing layer self.encoder_processing_layer = Processing_layer(config) def forward(self, x): # Encode the language we want to translate from x = self.encoder_embed(x) # Apply the attention mechanism and add the input x = self.attention_encoder(x) + x # apply layer norm, two dense layers and a layer norm again x = self.encoder_processing_layer(x) return x class Decoder(nn.Module): def __init__(self, config): super().__init__() self.config = config # Create embeddings for both the encoder and the decoder self.decoder_embed = Embedding(config) # Create an attention mechanism for the decoder self.attention_decoder = CausalSelfAttentionMasked(config) # Create a layernorm layer self.layernorm = LayerNorm(config.dim_embedding, bias=config.bias) # Create the encoder decoder attention self.decoder_attn = EncoderDecoderAttention(config) # Set up a processing sections one for the encoder and the other for the decoder self.decoder_processing_layer = Processing_layer(config) # The final layer which maps the models embedding dimension batch to the vocab size self.finallayer = nn.Linear(config.dim_embedding, config.vocab_size) def forward(self, x, y): # Encode the language we want to translate into y = self.decoder_embed(y) # Apply the attention mechanism and add the input y = self.attention_decoder(y) + y y = self.layernorm(y) # Take the ouput from the encoder and last layer of decoder and calculate attention again then add the input y = self.decoder_attn(y, x) + y # apply layer norm, two dense layers and a layer norm again y = self.decoder_processing_layer(y) y = self.finallayer(y) The annotated schematic shows how the components above correspond to the architecture in the original paper. One element missing is that there is no softmax layer in the Transformer class. The reason for is that the PyTorch crossentropy metric that we are optimising in training automatically applies a softmax so it is not needed to train the model. However, for the translation post training we do want the model to output a probability distribution and so apply a softmax layer to the outputs of the trained Transformer there. ￼ Original figure is from ‘Attention is All You Need’, Vaswani et al.(2017)\n4. Training The code below is from the python script train.py which trains the model for 30 epochs. It minimises the cross entropy between the predicted translations and the actual translations. Training the model for 30 epochs on a Mac M2 Air with 24 GB RAM doing other things took about 7.5 days.\nfrom torch.utils.data import DataLoader from torch.utils.data.dataset import random_split from torch.utils.tensorboard import SummaryWriter import torch.nn.functional as Fun from torch import nn import torch.optim as optim import torch.optim.lr_scheduler as lr_scheduler import sys # Import the model config and the Transformer class import config from Transformer import * writer = SummaryWriter() model = Transformer(config) # Import the training and test datasets and convert them into data loaders train_dataset = torch.load(\"train_dataset.pt\") test_dataset = torch.load(\"test_dataset.pt\") train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True) test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True) losses = [] torch.set_grad_enabled(True) optimizer = optim.Adam(model.parameters()) def save_checkpoint(model, optimizer, save_path, epoch): torch.save( { \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), \"epoch\": epoch, }, save_path, ) # If a saved model is input load the model and the optimizer state, else start from 0 if sys.argv[1] == \"new_model\": epoch_start = 0 elif sys.argv[1] is not None: state = torch.load(sys.argv[1]) model.load_state_dict(state[\"model_state_dict\"]) optimizer.load_state_dict(state[\"optimizer_state_dict\"]) epoch_start = state[\"epoch\"] # Save the model and optimizer epoch = 0 path_to_save = \"model_post\" + str(epoch) save_checkpoint(model, optimizer, path_to_save, epoch) num_epochs = 30 for epoch in range(epoch_start + 1, epoch_start + 1 + num_epochs): model.train() for i, (german, english, output) in enumerate(train_dataloader): optimizer.zero_grad() yhat = model(german, english) # yhat has dimensions (batch_size, block_size, vocab_size) # yhat.view(-1, yhat.size(-1)) has the dimensions ((batch_size X block_size), vocab_size) # output has the dimensions (batch_size, block_size) # output.view(-1) has the dimensions of (batch_size x block_size) loss = Fun.cross_entropy( yhat.view(-1, yhat.size(-1)), output.view(-1), ignore_index=0 ) writer.add_scalar(\"Loss/train\", loss, epoch) loss.backward() optimizer.step() # clear the gradients losses.append(loss) if i % 400 == 0: print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\") writer.flush() # Save the model and optimizer save_checkpoint(model, optimizer, path_to_save, epoch) # Compute average loss on the test set model.eval() # Set the model to evaluation mode val_loss = 0.0 with torch.no_grad(): # No gradient computation for validation for german_test, english_test, output_test in test_dataloader: yhat = model(german_test, english_test) loss = Fun.cross_entropy( yhat.view(-1, yhat.size(-1)), output_test.view(-1), ignore_index=0 ) val_loss += loss.item() avg_val_loss = val_loss / len(test_dataloader) writer.add_scalar(\"Loss/test\", avg_val_loss, epoch) print(f\"Epoch: {epoch}, Avg_val_loss: {avg_val_loss}\") 5. Generation of translations\nThe function below generates a translation from the input text. This is included in the script translate.py. It takes as an input the sentence we want to translate, it then:\n encodes that text into integers using the coding developed in the text processing phase. adds an extra dimension with the unsqueeze(0) so that the model which is trained on batches of 30, can read a single sentence rather than needing a batched input of multiple texts. it then generates the translation one token at a time. it starts with the encoded text we want the trained model to translate and a blank decoding sentence consisting of the encoded [start] token and then the padding tokens 0. The model takes these inputs and generates a probability distribution for its prediction of the first word of the translation. the model then samples from the probability distribution of the next word i.e. the word with highest predicted probability is most likely to be chosen. the predicted word is then added to the decoded sentence as its first word. the original sentence and the updated decoded sentence is then input into the model to get the next word. this is done recursively until we predict the token [end] and the translation ends.  import torch.nn.functional as Fun from torch import nn import torch.optim as optim import pickle from torch.utils.data import DataLoader from Transformer import * import numpy as np from string import punctuation import sys # Take the model that the script is going to use to translate from as the first argument in the command line model = sys.argv[1] def tensor_pad(x): \"\"\"converts list to tensor and pads to length 20\"\"\" return torch.nn.functional.pad( torch.tensor(x, dtype=torch.int64), (0, config.block_size), value=0 ) # Load the english and german dictionaries with open(\"german_dictionary_v3.pkl\", \"rb\") as file_ger: # Load the pickled object german_tokens = pickle.load(file_ger) with open(\"english_dictionary_v3.pkl\", \"rb\") as file_en: # Load the pickled object english_tokens = pickle.load(file_en) # Reverse the dictionaries to go from the numbers back to the original # This is used previously in text processing consider removing decode_to_english = {v: k for k, v in english_tokens.items()} decode_to_german = {v: k for k, v in german_tokens.items()} # Functions to encode and decode the data def source_vectorization(x): \"\"\"Converts the German words into numbers\"\"\" return [ german_tokens[element] for element in x.split() if element in german_tokens.keys() ] def target_vectorization(x): \"\"\"Converts the English words into numbers\"\"\" return [ english_tokens[element] for element in x.split() if element in english_tokens.keys() ] test_dataset = torch.load(\"test_dataset.pt\") test_dataloader = DataLoader(test_dataset, batch_size=1) # , shuffle=True) # Set up the transformer and load the past state model_predict = Transformer(config) state = torch.load(model) model_predict.load_state_dict(state[\"model_state_dict\"]) # Sets the model state to evaluate model_predict.eval() def prediction(x, y): \"\"\"This gives the probability distribution over English words for a given German translation\"\"\" logits = model_predict(x, y) logits = logits.squeeze(0) # Will remove the first dimension if it is set to 0 # returns a tensor with all specified dimensions of input of size 1 removed - in this case the first dimension # The dim = -1 applies softmax over the last dimension of the tensor return Fun.softmax(logits, dim=-1) def decode_sequence(input_sentence): \"\"\"This function generates a translation recursively\"\"\" # Unsqueezing adds an extra dimension so that the tensor is of size torch.Size([1, 20]) tokenized_input_sentence = tensor_pad(source_vectorization(input_sentence))[ : config.block_size ].unsqueeze(0) # print(tokenized_input_sentence) decoded_sentence = \"[start]\" # Loop through the sentence word by word for i in range(0, config.block_size): tokenized_target_sentence = tensor_pad(target_vectorization(decoded_sentence))[ : config.block_size ].unsqueeze(0) # Generate predictions predictions = prediction(tokenized_input_sentence, tokenized_target_sentence) # The second index in the positions tensor is the word position, the third index are the words # The .item() extracts the tensor index from the tensor sampled_token_index = torch.multinomial(predictions[i, :], num_samples=1).item() # Gets the word corresponding to the index sampled_token = decode_to_english[sampled_token_index] # Appends the word to the predicted translation to date decoded_sentence += \" \" + sampled_token # If the predicted token is end stop if sampled_token == \"[end]\": break return decoded_sentence The code below loops through the test data and generates predictions:\ndef trans(x, lan): \"\"\"This is a function to translate the English and German text from the numeric representations that we have of them in the pickle file\"\"\" results = \"\" for elem in x: if elem != 0: if lan == \"ger\": results = results + \" \" + decode_to_german[elem] if lan == \"eng\": results = results + \" \" + decode_to_english[elem] return results # Style class to format the print statements class style: BOLD = \"\\033[1m\" UNDERLINE = \"\\033[4m\" END = \"\\033[0m\" if sys.argv[2] is not None: print(decode_sequence(sys.argv[2])) sys.exit() for i, elem in enumerate(test_dataloader): if i % 3400 == 0: print(style.BOLD + \"Orginal\" + style.END) german = trans(elem[0].tolist()[0], \"ger\") print(german) print(style.BOLD + \"Translation\" + style.END) print(trans(elem[1].tolist()[0], \"eng\")) print(style.BOLD + \"Machine Translation\" + style.END) print(decode_sequence(german)) print(\"\\n\") Looking at a sample of random test sentences fed in we obtain the following:\nOrginal wann hast du das geräusch gehört Translation [start] when did you hear the sound Machine Translation [start] when did you hear the noise [end] Orginal schenke dem keine beachtung Translation [start] dont pay any attention to that Machine Translation [start] pay attention to disobeyed in [end] Orginal ich habe die bücher die ich mir aus der bibliothek ausgeliehen hatte zurückgebracht und mir ein paar neue ausgeliehen Translation [start] i returned the books i borrowed from the library and i borrowed some new ones Machine Translation [start] ive wears a few books that for me [end] Orginal wie oft essen sie schokolade Translation [start] how often do you eat chocolate Machine Translation [start] how often eat chocolate [end] Orginal ich kann ein geheimnis bewahren Translation [start] i can keep a secret Machine Translation [start] i can drive a secret [end] Orginal möchtest du wissen wer das gemacht hat Translation [start] do you want to know who did this Machine Translation [start] do you want to know who was doing that [end] Orginal ich möchte mit deiner mutter sprechen Translation [start] i want to talk to your mother Machine Translation [start] i want to know with your mother [end] Two of the translations are good. One is basically correct, but has a grammar error. Three have most of the words, but due to some mistakes are not correct. One of the translations is completely wrong. Trying it on one of the standard problems for English speakers learning German, it translates “ich renne in die park” correctly as “I am running into the park”, but “ich renne in dem park” is translated to the same thing, when it should be “i am running in the park” (The switch in the definite article from die to dem changes the meaning). The model has clearly developed some ability to translate from the data, but it is far from perfect.\nReferences:\nThe original Transformers paper ‘Attention is All You Need’, Vaswani et al. (2017)\nThe repo for Andrej Karpathy’s nanoGPT\nJay Alammar’s ‘The illustrated transformer’\nFrançois Chollet’s book ‘Deep Learning with Python (2nd edition)’\nOpenAI’s, ‘Language Models are Few-Shot Learners’\nPrevious Transformer notes:\n Transformers 1: The Attention Mechanism Transformers 2: The Transformer’s Structure  ",
  "wordCount" : "5407",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/printworks.jpg","datePublished": "2024-05-18T00:00:00Z",
  "dateModified": "2024-05-18T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/transformer3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Transformers 3: Building and training the Transformer
    </h1>
    <div class="post-meta">May 18, 2024&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/printworks.jpg" alt="">
        
</figure>
  <div class="post-content"><p>Printworks, April 2023</p>
<h3 id="transformers-3-building-and-training-the-transformer">Transformers 3: Building and training the Transformer<a hidden class="anchor" aria-hidden="true" href="#transformers-3-building-and-training-the-transformer">#</a></h3>
<p>Having discussed <a href="https://johnardavies.github.io/technical/transformer2/">how attention works</a> and <a href="https://johnardavies.github.io/technical/transformer2/">the structure of
Transformers</a> we’ll now implement a simple Transformer that takes German text and generates English translations. To do this
we’ll take the attention mechanism and other network components that Andrej Karpathy developed in nanoGPT for language generation and reuse them to implement
a Transformer for language translation. This uses the PyTorch framework. The language translation Transformer’s structure follows the example in François Chollet&rsquo;s
book ‘Deep Learning with Python’ which is written in Keras/TensorFlow. The relatively small scale of the model and data involved, means that it won&rsquo;t be able to completely
translate text, but will be able to generate translations to some extent.</p>
<p>This note will cover:</p>
<p><strong>1. Processing the text data</strong><br>
<strong>2. The implementation of attention</strong><br>
<strong>3. The implementation of other Transformer components</strong><br>
<strong>3. Putting all the components together to create a Transformer</strong><br>
<strong>4. Training</strong><br>
<strong>5. Generation of translations</strong></p>
<p>To start we will initialise the model with a PyTorch dataclass (<code>config.py</code>) that sets out the model&rsquo;s key parameters:</p>
<pre tabindex="0"><code>from dataclasses import dataclass


@dataclass
class TransformerConfig:

    &quot;&quot;&quot;With a dataclass we don't need to write an init function, just specify class attributes and their types&quot;&quot;&quot;

    block_size: int = 20  # Was 20
    vocab_size: int = 15000  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    dim_embedding: int = 600
    dim_inner_layer: int =  2000
    n_head: int = 6

    dropout: float = 0.2
    bias: bool = False
    epoch: int = 30
    batch_size: int = 30


config = TransformerConfig()
</code></pre><p>The model will have an embedding dimension of 600 and a vocabulary size of 15000 words for each language. Attention will be calculated over 6 heads in parallel, which due to how it is implemented involves 100 dimensional query,
key and value vectors. During training German text and a corresponding English translation
will be input in batches of 30 text-pairs. Both the German text and English translation text are capped at 20 tokens in length. In training we&rsquo;ll loop through the training data 30 times (30 epochs).
20% dropout is implemented at a number of stages of the network.</p>
<p><strong>1. Processing the text data</strong></p>
<p>To get some text data to train and test the Transformer we download the english-german translation pairs from
<a href="https://www.manythings.org/anki/">anki</a> and unzip the file:</p>
<pre tabindex="0"><code>$ wget https://www.manythings.org/anki/deu-eng.zip &amp;&amp; unzip deu-eng.zip
</code></pre><p>This produces the text file (deu.txt) which consists of pairs of English and German text, for example:<br>
<code>Bless you.	Gesundheit</code><br>
<code>Buy a gun.	Leg dir eine Schusswaffe</code></p>
<p>We want to encode these into numbers (for both English and German). We will process this with the python script <code>text_processing.py</code>.
To do this we&rsquo;ll start by simplifying the text by processing the pairs by removing punctuation and setting the text to lowercase. [start] and [end]
tags are added at the beginning and end of all of the English texts.</p>
<pre tabindex="0"><code>import torch
import torch.nn.functional as Fun
from torch.utils.data.dataset import random_split

from collections import Counter
from string import punctuation
import pickle

from config import *


# Strip punctuation
def strip_punctuation(s):
    &quot;&quot;&quot;Function that removes punctuation&quot;&quot;&quot;
    return str(&quot;&quot;.join(c for c in s if c not in punctuation))


def tensor_pad(x):
    &quot;&quot;&quot;converts list to tensor and pads to length 20&quot;&quot;&quot;
    return Fun.pad(
        torch.tensor(x, dtype=torch.int64), (0, config.block_size), value=0
    )
# Split the text file into german and english
# We start each line of the English text with start

text_file = &quot;deu-eng/deu.txt&quot;
with open(text_file) as language_file:
    lines = language_file.read().split(&quot;\n&quot;)
    print(&quot;There are &quot; + str(len(lines)) + &quot; lines&quot;)

text_pairs = []
for i, line in enumerate(lines):
    try:
        english, german = line.split(&quot;\t&quot;)[0:2]
        english = &quot;[start] &quot; + strip_punctuation(english.lower())+&quot; [end]&quot;
        text_pairs.append([strip_punctuation(german.lower()), english])
    except:
        print(&quot;failed to load %s&quot; % (i))
</code></pre><p>We then calculate the distinct words used in all the German and all the English terms.
We then create a class EncodeDecode which is initiated for each language separately
and has two methods that are dictionaries allowing the most common 15,000 words in each language to be encoded into numbers and decoded back.
More sophisticated encoding based on parts of speech or combinations of words are also possible.</p>
<pre tabindex="0"><code># To get the tokens used in English and German text we create
# two separate lists and from these create a single german 
# text and a single english text
german_list = [item[0] for item in text_pairs]
english_list = [item[1] for item in text_pairs]


german_text=' '.join(german_list)
english_text=' '.join(english_list)


class EncodeDecode():
    &quot;&quot;&quot;Class that produces two dictionaries from input text, one of which maps words
    to numbers and the other one reverses this mapping numbers to words.&quot;&quot;&quot;

    def __init__(self, text, vocab):
        self.text = text
        # Get the tokens
        self.tokens_list = list(set(self.text.split()))
        self.vocab_size=vocab
        # Get the most common tokens
        self.tokens_counter=Counter(self.text.split()).most_common(self.vocab_size)
        self.tokens_vocab=[item for item, count in self.tokens_counter]
      

    def decode(self):
        # Create a decoder from the numbers to the tokens
        decoder = {i+1: token for i, token in enumerate(self.tokens_vocab)}
        return decoder

    def encode(self):
        # Create an encoder of the tokens to numbers
        encoder = {token: i+1 for i, token in enumerate(self.tokens_vocab)}
        return encoder

</code></pre><p>We then apply the dictionaries to encode the English and German texts into numbers.
The English translation text is then processed into two copies. One which has the [end] token removed
which will be input for training and one which has the start token removed which is used as the target text the model
is trying to predict. Each German and English text is padded with 0s to get to 20 tokens. The encoder and decoder
calculations above start at 1 in order to keep 0 for the padding.</p>
<pre tabindex="0"><code># Get the German tokens and the English tokens

english_tokens = EncodeDecode(english_text, config.vocab_size - 1)
german_tokens = EncodeDecode(german_text, config.vocab_size - 1)

# Creates encoding dictionaries

english_encoded = english_tokens.encode()
german_encoded = german_tokens.encode()

# Save the english and german dictionaries
with open(&quot;english_dictionary_v4.pkl&quot;, &quot;wb&quot;) as eng:
    pickle.dump(english_tokens.encode(), eng)

with open(&quot;german_dictionary_v4.pkl&quot;, &quot;wb&quot;) as ger:
    pickle.dump(german_tokens.encode(), ger)


# Encode the tokens if they are in the 15000 most common tokens

text_pairs_encoded = [
    [
        [
            german_encoded[element]
            for element in pair[0].split()
            if element in german_tokens.tokens_vocab
        ],
        [
            english_encoded[element]
            for element in pair[1].split()
            if element in english_tokens.tokens_vocab
        ],
    ]
    for pair in text_pairs
]


# Split the data between the encode input in german the encoder input in english and the
# english output we are looking for
# The encoder input is shifted one token to the left of the target

# The -1 removes the last token in the list
# The elem[1][:-1] is the language we are trying to translate into that is input during training
# The elem[1][1:] is what we're trying to predict

text_pairs_encoded_split = [
    (elem[0], elem[1][:-1], elem[1][1:]) for elem in text_pairs_encoded
]

def tensor_pad(x):
    &quot;&quot;&quot;converts list to tensor and pads to length 20&quot;&quot;&quot;
    return Fun.pad(
        torch.tensor(x, dtype=torch.int64), (0, config.block_size) #, value=0
    )

# applies tensorpad

text_pairs_encoded_padded = [
    [
        tensor_pad(item1)[: config.block_size],
        tensor_pad(item2)[: config.block_size],
        tensor_pad(item3)[: config.block_size],
    ]
    for item1, item2, item3 in text_pairs_encoded_split
]

# Calculate how many observations are needed for a 20% test sample
test_len = round(len(text_pairs_encoded_padded) * 0.2)

# Calculate the number of training observations as residual
train_len = round(len(text_pairs_encoded_padded)) - test_len

# Get the train dataset and the test dataset
train_dataset, test_dataset = random_split(
    text_pairs_encoded_padded, [train_len, test_len]
)


# Save the test and train datasets as pickle files
torch.save(train_dataset, &quot;train_dataset.pt&quot;)
torch.save(test_dataset, &quot;test_dataset.pt&quot;)

</code></pre><p><strong>2. The implementation of attention in PyTorch</strong></p>
<p>The following code forms part of the transformers script <code>Transformer.py</code> which is the core model script.
Below is the implementation of attention that Andrej Karpathy used in nanoGPT. As with all PyTorch network classes it sets up the parts that
are used to construct the network when it is initiated. The forward method then details what happens when the data is input into an
initialised attention layer. All layer classes inherit from the nn.Module class which is the class for all neural network modules.</p>
<pre tabindex="0"><code># The imports for the Transformer script
import torch
import torch.nn.functional as Fun
from torch import nn
from config import *

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.dim_embedding % config.n_head == 0
        # key, query, value projections for all heads, but in a batc
        # nn.Linear applies a linear y=Ax +b transformation to the input
        # the input dimension is the first argument
        # the output dimension is the second argument
        # the last argument is the b (bias) term
        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )

    def forward(self, x):
        # Split the input tensor dimension down into the batch size B , the sequence length T and the emnedding dimensionality G
        (
            B,
            T,
            C,
        ) = (
            x.size()
        )  # batch size, sequence length, embedding dimensionality (dim_embedding)
        # calculate query, key, values
        # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension
        q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2)
        # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            # self.training is set to true when model.train() is initiated
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=False,
            )

        else:
            # manual implementation of attention
            # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            # Apply the softmax layer so that everything sums to 1
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            # Multiply the attention results by the value vectors
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
            # Change the shape of the tensor back to B, T, C removing the heads
            print(y.size(), &quot;post matrix mult&quot;)
        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side
        #   print(y.size(), 'proj')
        # output projection
        y = self.resid_dropout(self.c_proj(y))
        # print(y.size())
        return y
</code></pre><p>This shows creation of key, query and value vectors from the embeddings, which is implemented by passing the embedding vector into
c_attn(x) which generates these three vectors. These vectors are then split again by the number of heads to perform the attention calculation.
The vectors for each of the query key and value vectors have 600 dimensions and there are 6 heads, so we end up with 6 sets of 100 dimensional vectors for each of the original query, key
and value vectors.</p>
<pre tabindex="0"><code>q, k, v  = self.c_attn(x).split(self.dim_embedding, dim=2)
# split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs
</code></pre><p>The method contains an optimised attention mechanism that uses flash attention to process these collections of vectors. Here we have <code>is_causal=True</code> as this is the encoder side and in
translation we assume that the Transformer has access to the full German text before starting translation. By contrast on the encoder side <code>is_causal=False</code> when processing the
English translations as in generating the prediction this is done recursively with the prediction for the next word of the translation based on the predicted words before. In the
training data therefore, to predict the n&rsquo;th
word of the translation access is given to the n-1 English words before. For the very first translated word the <code>[start]</code> token is used to initialise the translation.</p>
<pre tabindex="0"><code>            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
</code></pre><p>If flash attention is not available there is also a version that implements attention
directly in Python. This illustrates more explicitly the components that make up attention.
The matrix multiplication of query and the transposed key matrix and then normalising the results by the square root of the key
vector&rsquo;s dimensionality:</p>
<pre tabindex="0"><code>att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
</code></pre><p>The application of the softmax layer:</p>
<pre tabindex="0"><code>att = F.softmax(att, dim=-1)
</code></pre><p>The multiplication of the attention weights by the values vectors:</p>
<pre tabindex="0"><code> y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
</code></pre><p>The concatenation of the head outputs and then the projection of resulting vector down to the model&rsquo;s embedding dimension. In the Karpathy implementation he then adds a dropout layer.</p>
<pre tabindex="0"><code> y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

# output projection
 y = self.resid_dropout(self.c_proj(y))
</code></pre><p><strong>3. The implementation of other Transformer components</strong></p>
<p>The Embedding class below takes in the encoded text and then passes each word to two embedding layers. There is the word embedding layer (wte) and the position embedding layer (wtp). These map the word and its position into two vectors of the chosen embedding
dimensionality. These two vectors are then added together encoding both the word and its position in the text in a single vector. This is done for all of the 20 words/tokens in a piece of text fed into the network, and their relative position in the text.</p>
<pre tabindex="0"><code>class Embedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        #  Creates the text embedding and the position embedding
        # The embedding automatically does the one hot encoding so this
        # does not need to be created directly
        self.config = config
        self.wte = nn.Embedding(config.vocab_size, config.dim_embedding)
        self.wtp = nn.Embedding(config.block_size, config.dim_embedding)

    def forward(self, x):
        # Applies the word embedding and then adds it to the position embedding
        x = self.wte(x)
        # The position embedding is applied over a tensor that ranges 0 to 20
        # The embedding is torch.Size([20, 200]) and is broadcast onto the larger tensor
        position_ids = (
            torch.arange(self.config.block_size).unsqueeze(0).repeat(x.size(0), 1)
        )
        positions = torch.arange(0, config.block_size)
        position_embeddings = self.wtp(position_ids)
        x = x + position_embeddings

        return x
</code></pre><p>The LayerNorm class applies layer normalisation, calculating the mean and standard deviation across the batch for each position in the text and elements of the vector
per word and then subtracting these means and dividing by the standard deviation from the corresponding elements across the batch.</p>
<pre tabindex="0"><code>class LayerNorm(nn.Module):
    &quot;&quot;&quot; LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False &quot;&quot;&quot;

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return Fun.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
</code></pre><p>A series of layers that implements a layer norm, a dense layer with a skip connection and then a dense layer again:
We label this pattern of layers, which appears in both the decoder and encoder as processing layers.</p>
<pre tabindex="0"><code>class Processing_layer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.denselayer1 = nn.Linear(config.dim_embedding, config.dim_inner_layer)
        self.denselayer2 = nn.Linear(config.dim_inner_layer, config.dim_embedding)
        self.layernorm1 = LayerNorm(config.dim_embedding, bias=config.bias)
        self.layernorm2 = LayerNorm(config.dim_embedding, bias=config.bias)

    def forward(self, x):
        # A layer norm and then two dense layers\n&quot;,
        x_in = self.layernorm1(x)
        x = self.denselayer1(x_in)
        x = self.denselayer2(x) + x_in
        x = self.layernorm2(x)
        return x


</code></pre><p>There is also a class that implements masked causal self attention:</p>
<pre tabindex="0"><code>class MaskedMultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.dim_embedding % config.n_head == 0
        # key, query, value projections for all heads, but in a batc
        # nn.Linear applies a linear y=Ax +b transformation to the input
        # the input dimension is the first argument
        # the output dimension is the second argument
        # the last argument is the b (bias) term
        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(
                &quot;bias&quot;,
                torch.tril(torch.ones(config.block_size, config.block_size)).view(
                    1, 1, config.block_size, config.block_size
                ),
            )

    def forward(self, x):
        # Split the input tensor dimension down into the batch size B , the sequence length T and the embedding dimensionality G
        (
            B,
            T,
            C,
        ) = (
            x.size()
        )  # batch size, sequence length, embedding dimensionality (dim_embedding)
        # calculate query, key, values
        # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension
        q, k, v = self.c_attn(x).split(self.dim_embedding, dim=2)
        # split k down by batch_size, sequence_length, number_heads, dimension_embedding/number_heads
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=True,
            )
        else:
            # manual implementation of attention
            # Calculate the inner product of the q and k vectors and normalise by square root of length of key vectors
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            # Calculate the masked attention
            att = att.masked_fill(attn_mask == 0, float(&quot;-inf&quot;))
            # Apply the softmax layer so that everything sums to 1
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            # Multiply the attention results by the value vectors
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
            # Change the shape of the tensor back to B, T, C removing the heads

        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))

        return y
</code></pre><p>The second attention layer in the decoder has two inputs: the output from the encoder and the output from the previous layer of the decoder&rsquo;s masked self-attention class. The encoder output is transformed into
key and value vectors. These vectors are then combined with the query vectors created from the vectors output from the decoder&rsquo;s previous layer in an attention calculation.</p>
<pre tabindex="0"><code>class EncoderDecoderAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.dim_embedding % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn_en = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )

        self.c_attn = nn.Linear(
            config.dim_embedding, 3 * config.dim_embedding, bias=config.bias
        )
        # output projection
        self.c_proj = nn.Linear(
            config.dim_embedding, config.dim_embedding, bias=config.bias
        )
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.dim_embedding = config.dim_embedding

        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
        self.flash = hasattr(torch.nn.functional, &quot;scaled_dot_product_attention&quot;)
        if not self.flash:
            print(
                &quot;WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0&quot;
            )
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer(
                &quot;bias&quot;,
                torch.tril(torch.ones(config.block_size, config.block_size)).view(
                    1, 1, config.block_size, config.block_size
                ),
            )

    def forward(self, x, e):

        # Generating the query and key vectors from the vectors from the encoder
        (
            B,
            T,
            C,
        ) = (
            e.size()
        )  # batch size, sequence length, embedding dimensionality (dim_embedding)
        # calculate query, key, values
        # by splitting the output of the attention layer into tensors of dimensio dim_embedding on the 2nd dimension
        _, k, v = self.c_attn_en(e).split(self.dim_embedding, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        #   print(q.size(), k.size())

        # Generating the value vector from  the decoder input
        (
            B,
            T,
            C,
        ) = (
            x.size()
        )  # batch size, sequence length, embedding dimensionality (dim_embedding)

        # calculate values for all heads in batch and move head forward to be the batch dim
        q, _, _ = self.c_attn(x).split(self.dim_embedding, dim=2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(
            1, 2
        )  # (B, nh, T, hs)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=None,
                dropout_p=self.dropout if self.training else 0,
                is_causal=False,
            )
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(attn_mask == 0, float(&quot;-inf&quot;))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
        y = (
            y.transpose(1, 2).contiguous().view(B, T, C)
        )  # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y
            
</code></pre><p><strong>3. Putting all the components together to create a Transformer</strong></p>
<p>We now combine all of these components to create an encoder and decoder class which we then combine together into a single Transformer class which we will initialise and then train.</p>
<pre tabindex="0"><code>class Encoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create embeddings for both the encoder and the decoder
        self.encoder_embed = Embedding(config)

        # Create an attention mechanism for the encoder
        self.attention_encoder = MultiHeadAttention(config)

        # Set up a processing layer
        self.encoder_processing_layer = Processing_layer(config)

    def forward(self, x):
        # Encode the language we want to translate from
        x = self.encoder_embed(x)

        # Apply the attention mechanism and add the input
        x = self.attention_encoder(x) + x

        # apply layer norm, two dense layers and a layer norm again
        x = self.encoder_processing_layer(x)

        return x


class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Create embeddings for both the encoder and the decoder
        self.decoder_embed = Embedding(config)

        # Create an attention mechanism for the decoder
        self.attention_decoder = CausalSelfAttentionMasked(config)

        # Create a layernorm layer
        self.layernorm = LayerNorm(config.dim_embedding, bias=config.bias)

        # Create the encoder decoder attention
        self.decoder_attn = EncoderDecoderAttention(config)

        # Set up a processing sections one for the encoder and the other for the decoder
        self.decoder_processing_layer = Processing_layer(config)

        # The final layer which maps the models embedding dimension batch to the vocab size
        self.finallayer = nn.Linear(config.dim_embedding, config.vocab_size)

    def forward(self, x, y):

        # Encode the language we want to translate into
        y = self.decoder_embed(y)

        # Apply the attention mechanism and add the input
        y = self.attention_decoder(y) + y

        y = self.layernorm(y)

        # Take the ouput from the encoder and last layer of decoder and calculate attention again then add the input
        y = self.decoder_attn(y, x) + y

        # apply layer norm, two dense layers and a layer norm again
        y = self.decoder_processing_layer(y)

        y = self.finallayer(y)
</code></pre><p>The annotated schematic shows how the components above correspond to the architecture in the original paper. One element missing is that
there is no softmax layer in the Transformer class. The reason for is that the PyTorch crossentropy metric that we are optimising in training automatically applies a softmax
so it is not needed to train the model. However, for the translation post training we do want the model to output a probability distribution
and so apply a softmax layer to the outputs of the trained Transformer there.
<img loading="lazy" src="https://johnardavies.github.io/transformers_annotate.png" alt="transformers_annotate"  />
￼
Original figure is from ‘Attention is All You Need’, Vaswani et al.(2017)</p>
<p><strong>4. Training</strong>
The code below is from the python script <code>train.py</code> which trains the model for 30 epochs. It minimises the cross entropy between the predicted translations and the actual translations.
Training the model for 30 epochs on a Mac M2 Air with 24 GB RAM doing other things took about 7.5 days.</p>
<pre tabindex="0"><code>from torch.utils.data import DataLoader
from torch.utils.data.dataset import random_split
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as Fun
from torch import nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler


import sys


# Import the model config and the Transformer class
import config
from Transformer import *



writer = SummaryWriter()


model = Transformer(config)


# Import the training and test datasets and convert them into data loaders
train_dataset = torch.load(&quot;train_dataset.pt&quot;)
test_dataset = torch.load(&quot;test_dataset.pt&quot;)

train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True)

losses = []


torch.set_grad_enabled(True)

optimizer = optim.Adam(model.parameters())


def save_checkpoint(model, optimizer, save_path, epoch):
    torch.save(
        {
            &quot;model_state_dict&quot;: model.state_dict(),
            &quot;optimizer_state_dict&quot;: optimizer.state_dict(),
            &quot;epoch&quot;: epoch,
        },
        save_path,
    )


# If a saved model is input load the model and the optimizer state, else start from 0
if sys.argv[1] == &quot;new_model&quot;:
    epoch_start = 0
elif sys.argv[1] is not None:
    state = torch.load(sys.argv[1])
    model.load_state_dict(state[&quot;model_state_dict&quot;])
    optimizer.load_state_dict(state[&quot;optimizer_state_dict&quot;])
    epoch_start = state[&quot;epoch&quot;]


# Save the model and optimizer

epoch = 0
path_to_save = &quot;model_post&quot; + str(epoch)
save_checkpoint(model, optimizer, path_to_save, epoch)


num_epochs = 30

for epoch in range(epoch_start + 1, epoch_start + 1 + num_epochs):
    model.train()
    for i, (german, english, output) in enumerate(train_dataloader):

        optimizer.zero_grad()

        yhat = model(german, english)

        # yhat has dimensions (batch_size, block_size, vocab_size)
        # yhat.view(-1, yhat.size(-1)) has the dimensions ((batch_size X block_size), vocab_size)
        # output has the dimensions (batch_size, block_size)
        # output.view(-1) has the dimensions of (batch_size x block_size)
        loss = Fun.cross_entropy(
            yhat.view(-1, yhat.size(-1)), output.view(-1), ignore_index=0
        )
        writer.add_scalar(&quot;Loss/train&quot;, loss, epoch)
        loss.backward()
        optimizer.step()
        # clear the gradients

        losses.append(loss)
        if i % 400 == 0:
            print(f&quot;Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}&quot;)
        writer.flush()

    # Save the model and optimizer
    save_checkpoint(model, optimizer, path_to_save, epoch)

    # Compute average loss on the test set
    model.eval()  # Set the model to evaluation mode
    val_loss = 0.0
    with torch.no_grad():  # No gradient computation for validation
        for german_test, english_test, output_test in test_dataloader:
            yhat = model(german_test, english_test)
            loss = Fun.cross_entropy(
                yhat.view(-1, yhat.size(-1)), output_test.view(-1), ignore_index=0
            )
            val_loss += loss.item()
        avg_val_loss = val_loss / len(test_dataloader)
        writer.add_scalar(&quot;Loss/test&quot;, avg_val_loss, epoch)
        print(f&quot;Epoch: {epoch},  Avg_val_loss: {avg_val_loss}&quot;)
</code></pre><p><strong>5. Generation of translations</strong></p>
<p>The function below generates a translation from the input text. This is included in the script <code>translate.py</code>. It takes as an input the sentence we want to translate, it then:</p>
<ol>
<li>encodes that text into integers using the coding developed in the text processing phase.</li>
<li>adds an extra dimension with the unsqueeze(0) so that the model which is trained on batches of 30, can read a single sentence rather than needing a batched input of multiple texts.</li>
<li>it then generates the translation one token at a time.</li>
<li>it starts with the encoded text we want the trained model to translate and a blank decoding sentence consisting of the encoded [start] token and then the padding tokens 0.
The model takes these inputs and generates a probability
distribution for its prediction of the first word of the translation.</li>
<li>the model then samples from the probability distribution of the next word i.e. the word with highest predicted probability is most likely to be chosen.</li>
<li>the predicted word is then added to the decoded sentence as its first word.</li>
<li>the original sentence and the updated decoded sentence is then input into the model to get the next word.</li>
<li>this is done recursively until we predict the token [end] and the translation ends.</li>
</ol>
<pre tabindex="0"><code>import torch.nn.functional as Fun
from torch import nn
import torch.optim as optim
import pickle

from torch.utils.data import DataLoader

from Transformer import *

import numpy as np
from string import punctuation
import sys

# Take the model that the script is going to use to translate from as the first argument in the command line
model = sys.argv[1]


def tensor_pad(x):
    &quot;&quot;&quot;converts list to tensor and pads to length 20&quot;&quot;&quot;
    return torch.nn.functional.pad(
        torch.tensor(x, dtype=torch.int64), (0, config.block_size), value=0
    )


# Load the english and german dictionaries
with open(&quot;german_dictionary_v3.pkl&quot;, &quot;rb&quot;) as file_ger:
    # Load the pickled object
    german_tokens = pickle.load(file_ger)


with open(&quot;english_dictionary_v3.pkl&quot;, &quot;rb&quot;) as file_en:
    # Load the pickled object
    english_tokens = pickle.load(file_en)

# Reverse the dictionaries to go from the numbers back to the original
# This is used previously in text processing consider removing
decode_to_english = {v: k for k, v in english_tokens.items()}

decode_to_german = {v: k for k, v in german_tokens.items()}


# Functions to encode and decode the data


def source_vectorization(x):
    &quot;&quot;&quot;Converts the German words into numbers&quot;&quot;&quot;
    return [
        german_tokens[element]
        for element in x.split()
        if element in german_tokens.keys()
    ]


def target_vectorization(x):
    &quot;&quot;&quot;Converts the English words into numbers&quot;&quot;&quot;
    return [
        english_tokens[element]
        for element in x.split()
        if element in english_tokens.keys()
    ]


test_dataset = torch.load(&quot;test_dataset.pt&quot;)
test_dataloader = DataLoader(test_dataset, batch_size=1)  # , shuffle=True)


# Set up the transformer and load the past state
model_predict = Transformer(config)

state = torch.load(model)

model_predict.load_state_dict(state[&quot;model_state_dict&quot;])

# Sets the model state to evaluate
model_predict.eval()

def prediction(x, y):
    &quot;&quot;&quot;This gives the probability distribution over English words for a given German translation&quot;&quot;&quot;
    logits = model_predict(x, y)
    logits = logits.squeeze(0)  # Will remove the first dimension if it is set to 0
    # returns a tensor with all specified dimensions of input of size 1 removed - in this case the first dimension
    # The dim = -1 applies softmax over the last dimension of the tensor
    return Fun.softmax(logits, dim=-1)

</code></pre><pre tabindex="0"><code>def decode_sequence(input_sentence):
    &quot;&quot;&quot;This function generates a translation recursively&quot;&quot;&quot;
    # Unsqueezing adds an extra dimension so that the tensor is of size torch.Size([1, 20])
    tokenized_input_sentence = tensor_pad(source_vectorization(input_sentence))[
        : config.block_size
    ].unsqueeze(0)
    #  print(tokenized_input_sentence)
    decoded_sentence = &quot;[start]&quot;
    # Loop through the sentence word by word
    for i in range(0, config.block_size):
        tokenized_target_sentence = tensor_pad(target_vectorization(decoded_sentence))[
            : config.block_size
        ].unsqueeze(0)

        # Generate predictions
        predictions = prediction(tokenized_input_sentence, tokenized_target_sentence)

        # The second index in the positions tensor is the word position, the third index are the words
        # The .item() extracts the tensor index from the tensor
        sampled_token_index = torch.multinomial(predictions[i, :], num_samples=1).item()

        # Gets the word corresponding to the index
        sampled_token = decode_to_english[sampled_token_index]

        # Appends the word to the predicted translation to date
        decoded_sentence += &quot; &quot; + sampled_token

        # If the predicted token is end stop
        if sampled_token == &quot;[end]&quot;:
            break
    return decoded_sentence

</code></pre><p>The code below loops through the test data and generates predictions:</p>
<pre tabindex="0"><code>def trans(x, lan):
    &quot;&quot;&quot;This is a function to translate the English and German text from the numeric representations that we have of them in the
    pickle file&quot;&quot;&quot;
    results = &quot;&quot;
    for elem in x:
        if elem != 0:
            if lan == &quot;ger&quot;:
                results = results + &quot; &quot; + decode_to_german[elem]
            if lan == &quot;eng&quot;:
                results = results + &quot; &quot; + decode_to_english[elem]
    return results


# Style class to format the print statements
class style:
    BOLD = &quot;\033[1m&quot;
    UNDERLINE = &quot;\033[4m&quot;
    END = &quot;\033[0m&quot;


if sys.argv[2] is not None:
    print(decode_sequence(sys.argv[2]))
    sys.exit()

for i, elem in enumerate(test_dataloader):
    if i % 3400 == 0:

        print(style.BOLD + &quot;Orginal&quot; + style.END)
        german = trans(elem[0].tolist()[0], &quot;ger&quot;)
        print(german)
        print(style.BOLD + &quot;Translation&quot; + style.END)
        print(trans(elem[1].tolist()[0], &quot;eng&quot;))
        print(style.BOLD + &quot;Machine Translation&quot; + style.END)
        print(decode_sequence(german))
        print(&quot;\n&quot;)
</code></pre><p>Looking at a sample of random test sentences fed in we obtain the following:</p>
<pre tabindex="0"><code>Orginal
 wann hast du das geräusch gehört
Translation
 [start] when did you hear the sound
Machine Translation
[start] when did you hear the noise [end]


Orginal
 schenke dem keine beachtung
Translation
 [start] dont pay any attention to that
Machine Translation
[start] pay attention to disobeyed in [end]


Orginal
 ich habe die bücher die ich mir aus der bibliothek ausgeliehen hatte zurückgebracht und mir ein paar neue ausgeliehen
Translation
 [start] i returned the books i borrowed from the library and i borrowed some new ones
Machine Translation
[start] ive wears a few books that for me [end]


Orginal
 wie oft essen sie schokolade
Translation
 [start] how often do you eat chocolate
Machine Translation
[start] how often eat chocolate [end]


Orginal
 ich kann ein geheimnis bewahren
Translation
 [start] i can keep a secret
Machine Translation
[start] i can drive a secret [end]


Orginal
 möchtest du wissen wer das gemacht hat
Translation
 [start] do you want to know who did this
Machine Translation
[start] do you want to know who was doing that [end]


Orginal
 ich möchte mit deiner mutter sprechen
Translation
 [start] i want to talk to your mother
Machine Translation
[start] i want to know with your mother [end]
</code></pre><p>Two of the translations are good. One is basically correct, but has a grammar error. Three have most of the words, but due to some mistakes are not correct.
One of the translations is completely wrong. Trying it on one of the standard problems for English speakers learning German, it translates &ldquo;ich renne in die park&rdquo; correctly as &ldquo;I am running into the park&rdquo;,
but &ldquo;ich renne in dem park&rdquo; is translated to the same thing, when it should be &ldquo;i am running in the park&rdquo; (The switch in the definite article from die to dem changes the meaning).
The model has clearly developed some ability to translate from the data, but it is far from perfect.</p>
<p><strong>References:</strong></p>
<p><a href="https://arxiv.org/abs/1706.03762/">The original Transformers paper &lsquo;Attention is All You Need&rsquo;,
Vaswani et al. (2017)</a></p>
<p><a href="https://github.com/karpathy/nanoGPT/">The repo for Andrej Karpathy&rsquo;s nanoGPT</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s ‘The illustrated transformer’</a></p>
<p>François Chollet’s book ‘Deep Learning with Python (2nd edition)’</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">OpenAI&rsquo;s, &lsquo;Language Models are Few-Shot Learners&rsquo;</a></p>
<p><strong>Previous Transformer notes</strong>:</p>
<ol>
<li><a href="https://t.co/hFufxkFXaQ"><strong>Transformers 1: The Attention Mechanism</strong></a></li>
<li><a href="https://johnardavies.github.io/technical/transformer2/"><strong>Transformers 2: The Transformer&rsquo;s Structure</strong></a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
      <li><a href="https://johnardavies.github.io/tags/digital/">digital</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
