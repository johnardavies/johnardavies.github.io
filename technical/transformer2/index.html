<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers 2: The Transformer&#39;s Structure | John&#39;s Site</title>
<meta name="keywords" content="technical, digital" />
<meta name="description" content="Helix, White City
Transformers 2: The structure of a Transformer The previous post discussed the attention mechanism that underlies how Transformers work. Continuing the series of notes on Transformers I’ll now look at the overall structure of Transformers and how attention is used within them in the context of language translation. I’ll cover:
1. The architecture of Transformers
2. How attention is included in the Transformer
1. The architecture of Transformers 1.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/transformer2/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Transformers 2: The Transformer&#39;s Structure" />
<meta property="og:description" content="Helix, White City
Transformers 2: The structure of a Transformer The previous post discussed the attention mechanism that underlies how Transformers work. Continuing the series of notes on Transformers I’ll now look at the overall structure of Transformers and how attention is used within them in the context of language translation. I’ll cover:
1. The architecture of Transformers
2. How attention is included in the Transformer
1. The architecture of Transformers 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/transformer2/" />
<meta property="og:image" content="https://johnardavies.github.io/helix.jpg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2024-01-12T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-01-12T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/helix.jpg" />
<meta name="twitter:title" content="Transformers 2: The Transformer&#39;s Structure"/>
<meta name="twitter:description" content="Helix, White City
Transformers 2: The structure of a Transformer The previous post discussed the attention mechanism that underlies how Transformers work. Continuing the series of notes on Transformers I’ll now look at the overall structure of Transformers and how attention is used within them in the context of language translation. I’ll cover:
1. The architecture of Transformers
2. How attention is included in the Transformer
1. The architecture of Transformers 1."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers 2: The Transformer's Structure",
      "item": "https://johnardavies.github.io/technical/transformer2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers 2: The Transformer's Structure",
  "name": "Transformers 2: The Transformer\u0027s Structure",
  "description": "Helix, White City\nTransformers 2: The structure of a Transformer The previous post discussed the attention mechanism that underlies how Transformers work. Continuing the series of notes on Transformers I’ll now look at the overall structure of Transformers and how attention is used within them in the context of language translation. I’ll cover:\n1. The architecture of Transformers\n2. How attention is included in the Transformer\n1. The architecture of Transformers 1.",
  "keywords": [
    "technical", "digital"
  ],
  "articleBody": "Helix, White City\nTransformers 2: The structure of a Transformer The previous post discussed the attention mechanism that underlies how Transformers work. Continuing the series of notes on Transformers I’ll now look at the overall structure of Transformers and how attention is used within them in the context of language translation. I’ll cover:\n1. The architecture of Transformers\n2. How attention is included in the Transformer\n1. The architecture of Transformers 1.1 The overall structure\nThe diagram below is taken from the original paper that introduced Transformers. The Transformer is split into two blocks: The encoder and the decoder. On the left-hand side is the encoder where the language we want to translate from is input. On the right-hand side is the decoder which generates the translation. As shown in the diagram both encoder and decoder contain attention layers.\nFigure is from ‘Attention is All You Need’, Vaswani et al.(2017)\nTo train the model to translate from one language to another, the original text and its corresponding translation are fed into the model at the bottom through the two embedding layers. The original text is processed by the encoder and then combined with results of the decoder to make a prediction of what the translation should be. A softmax layer is used by the decoder to output the probabilities of different words in the language the input text is translated into. The model weights are adjusted in training to minimise the errors that the model makes in translation.\nIf the Transformer was just focussed on generating text in one language, such as in GPT that answers questions, then the architecture would be simpler - effectively many repeated layers of the encoder followed by the linear and softmax layers.\n1.2 The numbers that parameterise the Transformer\nWithin this overall structure, a few key parameters specify the overall size of a Transformer, and are often used to describe them:\n1. The dimensionality of the vectors words are converted into in the embedding stage.\n2. The dimensionality of the query, key and value vectors in the attention layers.\nThe query and key vectors need to be the same dimensionality, but the value vector can have a different dimensionality.\n3. The number of heads in multi-head attention which are calculated in parallel.\n4. The number of encoder and decoder layers. In the diagram the Nx by both blocks which shows that they could be repeated N times\n2. How Attention is included in the Transformer Within the Transformer as shown in the diagram there aree a number of different features that are combined with the Multi-head attention layers to generate translations.\n1. Outputs (Shifted right)\n2. The positional encoding which is applied to both the inputs and the outputs\n3. The Masked Multi-head attention\n4. The Multi-head attention layer in the encoder that receives inputs from the decoder (encoder-decoder attention)\n5. The add (the residual connection) and normed layers\nWe’ll cover these in turn:\n2.1 Outputs (shifted right)\nIn training the text we want to translate from is fed into the encoder and the corresponding translations are fed into the decoder. In the diagram the translated text is labeled as output text as once trained the generated translations are the outputs. In training, the training text for the language we are translating into has the token [start] added at the beginning and finishes with the token [end]. The text in the language we are translating from is unchanged.\nFor the purposes of the example will translate from German to English. On the training stage the text that is fed into the Encoder are sentences written in German, while at the same time the corresponding sentence translated into English is fed into the Decoder. For example, if we feed in the German text ‘Geh’ then the English text we feed in at the same time is [start] ‘Go’ [end]. The Outputs (shifted right) offset means that when the Transformer receives ‘Geh’ and tries to predict the next word in the translation, then that word is ‘Get’ as the sentence has been shifted right by one token.\n2.2 The positional encoding which is applied to both the inputs\nAs discussed in the first post the words fed into the Transformer are converted into vectors using embeddings. In addition to the information about the words in the text there is information about their position in the text. This information can be incorporated by adding a vector of the same dimensionality as the embedding vector to represents a word’s position in the text its in. In the original Transformers paper this was done using an approach based on sine and cosine functions. An alternative approach is to train another embedding, that maps a vector representing the position of the word in the input text (which is typically a fixed length) into a vector the size of embedding. This vector is then added to the word embedding to encode position.\n2.3 The Masked Multi-head attention\\\nMasked Multi-Head attention, which is used in the decoder,is a refinement to how Multi-head attention is calculated. In the encoder attention is calculated, as in the first post, between all the words in the piece of text that’s fed in. However, in the decoder a different approach is used. To generate a prediction, the decoder operates recursively. It predicts the first word of the translation from the input text. It then takes the predicted word and the encoded input text to predict the second word. It then takes the first and second predicted words and the encoded text to produce the third word of the translation and so on. This means that the decoder, when it computes attention for a token/word should only have access to tokens/words that appear before the word. If it could access all the input translated text in training then it could just copy the inputs, but then it would just be memorising the answers rather than learning the language structure. To address this we apply a mask to the data, which is basically an upper diagonal matrix that sets the values that come up after a token to NA, depending on the token’s location i.e. first token can calculate attention using just itself, the second token can calculate attention using itself and the first token. The third token can calculate attention using itself, the second and first token etc etc.\n2.4 The Multi-head attention layer in the decoder that receives inputs from the encoder (encoder-decoder attention)\nIn a translation transformer we need to switch from one language to the other. This is done in the part of the Transformer shown below. To do this we take the key and query vectors that are output from the encoder for the language we are translating from and pass it to the decoder where we calculate the attention of these vectors with the corresponding value vectors that the decoder generates for the language we are translating into. As here we are doing a translation from English to German, writing it in matrix form where the indices G and E represent that the query and key vectors come from German and the value vectors come from English we calculate the attention as: 2.5 The add (the residual connection) and norm layer\\\nIn the diagram below we see the original embedding vector being split up into query, key and value vectors to calculate the Multi-Head attention. After the attention phase the original embedding vector is also added back into the results of the multi-headed attention and normalised. The addition of the original input is known as a residual connection. It has been found that doing this has made networks easier to train and converge on results. In the normalisation you subtract the mean of the values and divide by the standard deviation. Residual connections and normalisation are also implemented in feed forward network. The feed forward networks transform the outputs of the previous layer into a new set of values, typically by multiplying by a weights matrix. ￼\nNext steps\nHaving covered:\u2028 How attention works How Transformers use attention  We now move onto:\nHow Transformers are implemented in code How Transformers are trained and used to generate text  These are the subject of the next notes.\nReferences:\nThe original Transformers paper ‘Attention is All You Need’, Vaswani et al. (2017)\nThe repo for Andrej Karpathy’s nanoGPT\nJay Alammar’s ‘The illustrated transformer’\nFrançois Chollet’s book ‘Deep Learning with Python (2nd edition)’\nOpenAI’s, ‘Language Models are Few-Shot Learners’\n",
  "wordCount" : "1412",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/helix.jpg","datePublished": "2024-01-12T00:00:00Z",
  "dateModified": "2024-01-12T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/transformer2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Transformers 2: The Transformer&#39;s Structure
    </h1>
    <div class="post-meta">January 12, 2024&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/helix.jpg" alt="">
        
</figure>
  <div class="post-content"><p>Helix, White City</p>
<h3 id="transformers-2-the-structure-of-a-transformer">Transformers 2: The structure of a Transformer<a hidden class="anchor" aria-hidden="true" href="#transformers-2-the-structure-of-a-transformer">#</a></h3>
<p>The previous <a href="https://t.co/hFufxkFXaQ">post</a> discussed the attention
mechanism that underlies how Transformers work. Continuing the series of
notes on Transformers I’ll now look at the overall structure of
Transformers and how attention is used within them in the context of
language translation. I’ll cover:</p>
<p><strong>1. The architecture of Transformers</strong><br>
<strong>2. How attention is included in the Transformer</strong></p>
<h3 id="1-the-architecture-of-transformers">1. The architecture of Transformers<a hidden class="anchor" aria-hidden="true" href="#1-the-architecture-of-transformers">#</a></h3>
<p><strong>1.1 The overall structure</strong><br>
The diagram below is taken from the original paper that introduced Transformers. The Transformer is split
into two blocks: The encoder and the decoder. On the left-hand side is the encoder where the language we want
to translate from is input. On the right-hand side is the decoder which generates the translation. As shown
in the diagram both encoder and decoder contain attention layers.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/transformers.png" alt="transformers"  />

Figure is from <a href="https://arxiv.org/abs/1706.03762">‘Attention is All You Need’</a>, Vaswani et al.(2017)</p>
<p>To train the model to translate from one language to another, the original text and its corresponding
translation are fed into the model at the bottom through the two embedding layers. The original text is
processed by the encoder and then combined with results of the decoder to make a prediction of what the
translation should be. A softmax layer is used by the decoder to output the probabilities of different words
in the language the input text is translated into. The model weights are adjusted in training to minimise the
errors that the model makes in translation.</p>
<p>If the Transformer was just focussed on generating text in one language, such as in GPT that answers
questions, then the architecture would be simpler  - effectively many repeated layers of the encoder followed
by the linear and softmax layers.</p>
<p><strong>1.2 The numbers that parameterise the Transformer</strong></p>
<p>Within this overall structure, a few key parameters specify the overall size of a Transformer, and are often
used to describe them:</p>
<p><strong>1. The dimensionality of the vectors words are converted into in the embedding stage.</strong><br>
<strong>2. The dimensionality of the query, key and value vectors in the attention layers.</strong><br>
The query and key vectors need to be the same dimensionality, but the value vector can have a different
dimensionality.<br>
<strong>3. The number of heads in multi-head attention which are calculated in parallel.</strong><br>
<strong>4. The number of encoder and decoder layers.</strong> In the diagram the Nx by both blocks which shows that
they could be repeated N times</p>
<h3 id="2-how-attention-is-included-in-the-transformer">2. How Attention is included in the Transformer<a hidden class="anchor" aria-hidden="true" href="#2-how-attention-is-included-in-the-transformer">#</a></h3>
<p>Within the Transformer as shown in the diagram there aree a number of different features that are combined
with the Multi-head attention layers to generate translations.</p>
<p><strong>1.  Outputs (Shifted right)</strong><br>
<strong>2.  The positional encoding which is applied to both the inputs and the outputs</strong><br>
<strong>3.  The Masked Multi-head attention</strong><br>
<strong>4.  The Multi-head attention layer in the encoder that receives inputs from the decoder (encoder-decoder attention)</strong><br>
<strong>5.  The add (the residual connection) and normed layers</strong></p>
<p>We&rsquo;ll cover these in turn:</p>
<p><strong>2.1   Outputs (shifted right)</strong><br>
In training the text we want to translate from is fed into the encoder and the corresponding translations are
fed into the decoder. In the diagram the translated text is labeled as output text as once trained the
generated translations are the outputs.
<img loading="lazy" src="https://johnardavies.github.io/shifted_right.png" alt="shifted"  />

In training, the training text for the language we are translating into has the token [start] added at the
beginning and finishes with the token [end]. The text in the language we are translating from is unchanged.</p>
<p>For the purposes of the example will translate from German to English. On the training stage the text that is
fed into the Encoder are sentences written in German, while at the same time the corresponding sentence
translated into English is fed into the Decoder. For example, if we feed in the German text ‘Geh’ then the
English text we feed in at the same time is [start] ‘Go’ [end]. The Outputs (shifted right) offset means that
when the Transformer receives ‘Geh’ and tries to predict the next word in the translation, then that word is
‘Get’ as the sentence has been shifted right by one token.</p>
<p><strong>2.2  The positional encoding which is applied to both the inputs</strong><br>
As discussed in the first post the words fed into the Transformer are converted into vectors using
embeddings. In addition to the information about the words in the text there is information about their
position in the text. This information can be incorporated by adding a vector of the same dimensionality as
the embedding vector to represents a word’s position in the text its in.
<img loading="lazy" src="https://johnardavies.github.io/pos_encoding.png" alt="posencoding"  />

In the original Transformers paper this was done using an approach based on sine and cosine functions. An
alternative approach is to train another embedding, that maps a vector representing the position of the word in the input text (which is
typically a fixed length) into a vector the size of embedding. This vector is then added to the word
embedding to encode position.</p>
<p><strong>2.3 The Masked Multi-head attention</strong>\</p>
<p>Masked Multi-Head attention, which is used in the decoder,is a refinement to how Multi-head attention is
calculated.
<img loading="lazy" src="https://johnardavies.github.io/masked_multihead.png" alt="masked"  />

In the encoder attention is calculated, as in the first post, between all the words in the piece
of text that’s fed in. However, in the decoder a different approach is used. To generate a prediction, the
decoder operates recursively. It predicts the first word of the translation from the input text. It then
takes the predicted word and the encoded input text to predict the second word. It then takes the first and
second predicted words and the encoded text to produce the third word of the translation and so on. This
means that the decoder, when it computes attention for a token/word should only have access to tokens/words
that appear before the word. If it could access all the input translated text in training then it could just
copy the inputs, but then it would just be memorising the answers rather than learning the language
structure. To address this we apply a mask to the data, which is basically an upper diagonal matrix that sets
the values that come up after a token to NA, depending on the token’s location i.e. first token can calculate
attention using just itself, the second token can calculate attention using itself and the first token. The
third token can calculate attention using itself, the second and first token etc etc.</p>
<p><strong>2.4  The Multi-head attention layer in the decoder that receives inputs from the encoder (encoder-decoder
attention)</strong><br>
In a translation transformer we need to switch from one language to the other. This is done in the part of
the Transformer shown below.
<img loading="lazy" src="https://johnardavies.github.io/Forward.png" alt="forward"  />

To do this we take the key and query vectors that are output from the encoder for the language we are
translating from and pass it to the decoder where we calculate the attention of these vectors with the
corresponding value vectors that the decoder generates for the language we are translating into. As here we
are doing a translation from English to German, writing it in matrix form where the indices G and E represent
that the query and key vectors come from German and the value vectors come from English we calculate the
attention as: <img loading="lazy" src="https://johnardavies.github.io/translate_attention.png" alt="matrix_translate"  />
</p>
<p><strong>2.5  The add (the residual connection) and norm layer</strong>\</p>
<p>In the diagram below we see the original embedding vector being split up into query, key and value vectors to
calculate the Multi-Head attention. After the attention phase the original embedding vector is also added
back into the results of the multi-headed attention and normalised. The addition of the original input is
known as a residual connection. It has been found that doing this has made networks easier to train and
converge on results. In the normalisation you subtract the mean of the values and divide by the standard
deviation.
<img loading="lazy" src="https://johnardavies.github.io/masked_multihead.png" alt="masked"  />

Residual connections and normalisation are also implemented in feed forward network. The feed forward
networks transform the outputs of the previous layer into a new set of values, typically by multiplying by a
weights matrix.
<img loading="lazy" src="https://johnardavies.github.io/feed_forward.png" alt="feedfwd"  />
￼</p>
<p><strong>Next steps</strong></p>
<p>Having covered: </p>
<ol>
<li><a href="https://t.co/hFufxkFXaQ">How attention works</a></li>
<li>How Transformers use attention</li>
</ol>
<p>We now move onto:</p>
<ol start="2">
<li>How Transformers are implemented in code</li>
<li>How Transformers are trained and used to generate text</li>
</ol>
<p>These are the subject of the next notes.</p>
<p><strong>References:</strong></p>
<p><a href="https://arxiv.org/abs/1706.03762/">The original Transformers paper &lsquo;Attention is All You Need&rsquo;,
Vaswani et al. (2017)</a></p>
<p><a href="https://github.com/karpathy/nanoGPT/">The repo for Andrej Karpathy&rsquo;s nanoGPT</a></p>
<p><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s ‘The illustrated transformer’</a></p>
<p>François Chollet’s book ‘Deep Learning with Python (2nd edition)’</p>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">OpenAI&rsquo;s, &lsquo;Language Models are Few-Shot Learners&rsquo;</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
      <li><a href="https://johnardavies.github.io/tags/digital/">digital</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
