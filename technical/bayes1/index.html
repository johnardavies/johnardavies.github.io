<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bayes 1: Introduction to Bayesian inference | John&#39;s Site</title>
<meta name="keywords" content="technical" />
<meta name="description" content="Ilke Sahin&rsquo;s &lsquo;Water Canals (Aquaducts)&rsquo; at Goldsmiths degree show 2024
This note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.
1. Finding the most probable parameters given the data using Bayes’ theorem We want to find the best hypothesis, or model parameter(s) that describes the data. Bayes’ theorem allows us to write down the formula for the probability of model parameters or hypotheses given the data.">
<meta name="author" content="John Davies">
<link rel="canonical" href="https://johnardavies.github.io/technical/bayes1/" />
<link crossorigin="anonymous" href="https://johnardavies.github.io/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css" integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw&#43;W8mWdq36u97PLc=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="https://johnardavies.github.io/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://johnardavies.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://johnardavies.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://johnardavies.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://johnardavies.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://johnardavies.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.88.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-YJ821LVT08', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJ821LVT08"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-YJ821LVT08', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Bayes 1: Introduction to Bayesian inference" />
<meta property="og:description" content="Ilke Sahin&rsquo;s &lsquo;Water Canals (Aquaducts)&rsquo; at Goldsmiths degree show 2024
This note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.
1. Finding the most probable parameters given the data using Bayes’ theorem We want to find the best hypothesis, or model parameter(s) that describes the data. Bayes’ theorem allows us to write down the formula for the probability of model parameters or hypotheses given the data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://johnardavies.github.io/technical/bayes1/" />
<meta property="og:image" content="https://johnardavies.github.io/ilkesahin.jpeg" /><meta property="article:section" content="technical" />
<meta property="article:published_time" content="2024-07-20T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-07-20T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://johnardavies.github.io/ilkesahin.jpeg" />
<meta name="twitter:title" content="Bayes 1: Introduction to Bayesian inference"/>
<meta name="twitter:description" content="Ilke Sahin&rsquo;s &lsquo;Water Canals (Aquaducts)&rsquo; at Goldsmiths degree show 2024
This note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.
1. Finding the most probable parameters given the data using Bayes’ theorem We want to find the best hypothesis, or model parameter(s) that describes the data. Bayes’ theorem allows us to write down the formula for the probability of model parameters or hypotheses given the data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technicals",
      "item": "https://johnardavies.github.io/technical/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bayes 1: Introduction to Bayesian inference",
      "item": "https://johnardavies.github.io/technical/bayes1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bayes 1: Introduction to Bayesian inference",
  "name": "Bayes 1: Introduction to Bayesian inference",
  "description": "Ilke Sahin\u0026rsquo;s \u0026lsquo;Water Canals (Aquaducts)\u0026rsquo; at Goldsmiths degree show 2024\nThis note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.\n1. Finding the most probable parameters given the data using Bayes’ theorem We want to find the best hypothesis, or model parameter(s) that describes the data. Bayes’ theorem allows us to write down the formula for the probability of model parameters or hypotheses given the data.",
  "keywords": [
    "technical"
  ],
  "articleBody": "Ilke Sahin’s ‘Water Canals (Aquaducts)’ at Goldsmiths degree show 2024\nThis note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.\n1. Finding the most probable parameters given the data using Bayes’ theorem We want to find the best hypothesis, or model parameter(s) that describes the data. Bayes’ theorem allows us to write down the formula for the probability of model parameters or hypotheses given the data.\nThis is called the posterior distribution. It consists of a prior, a likelihood, and a marginal likelihood (sometimes called Evidence). The prior gives the assumed probability distribution of model parameters/hypotheses before any data. This can reflect pre-existing beliefs and/or logical/physical constraints. The likelihood gives the probability of the observed data given a set of parameter values/hypotheses. The marginal likelihood is the probability of the data given all possible values the model parameters might take (or all possible hypotheses).\nThe starting point in a Bayesian approach is that there is assumed to be an uncertainty about the model parameters which we capture with a prior distribution and then update the posterior as new data comes in. By contrast, in a frequentist approach, the starting assumption is that there are assumed to be true underlying model parameters and the data generates uncertainty in our estimates. We capture this uncertainty in a frequentist approach in the sampling distribution of the estimates, but the data these distributions are based on is assumed to be generated by fixed model parameters.\nIf the parameter or hypothesis can only take discrete values e.g. does a photo (the data) contain person A (hypothesis 1) or does it not (hypothesis 2). Then the Bayesian formula looks like the below, and the marginal likelihood is a sum over all possible states:\nIf the parameter can take an infinite number of values, for example a parameter specifying a distribution, then the denominator becomes an integral:\nWe often went to find the value of theta that maximises the posterior and so is most probable value given the data. The marginal likelihood does not depend on theta as we integrate over all possible values of it. It therefore represents a normalisation constant which will not affect the optimal value of theta. As a result a lot of Bayesian statistics is about evaluating the likelihood*prior, rather than the full posterior, to estimate the optimal parameter. For a given model the marginal likelihood is fixed, but it can vary between models as they can have different numbers and types of parameters and so different scope to explain the data. It therefore has a role in choosing between different kinds of models.\nTo make the posterior more concrete we now work through a simple example of calculating it based on a Poisson likelihood and a Gamma prior distribution. These functional forms allow the posterior to be completely solved analytically. In many cases this is though not possible and techniques like Markov Chain Monte Carlo (MCMC) have to be used to estimate the posterior.\n2. The Likelihood: The Poisson distribution The Poisson distribution gives the probability that a certain number of types of events x occur in an interval (such as a time period or number of discrete events). Its probability distribution below has both a mean and variance of lambda.\nSumming it from x=0 to infinity and using the Taylor expansion it sums to 1 as a probability distribution should.\nCalculating the expected value:\nThe Poisson is the limit of the Binomial distribution. This is the distribution of x outcomes from n trials where there are two possible states e.g. how many heads from n coin tosses, when we have a large number of trials). Where p+q=1 and p=lambda*n the Binomial distribution tends to the Poisson as sample sizes get large.\nThis Poisson distribution gives the probability of x states as the number of trials gets large. Assuming the values of the observed data are independent the Likelihood function is the product of the Poisson distributions for each observed value. The likelihood of the data then becomes:\n3. The Prior: The Gamma distribution Having got an expression for the likelihood of lambda given the data we now examine a prior for the lambda. A standard prior for a variable with a Poisson likelihood distribution is the Gamma distribution shown below. As the Gamma distribution is defined from 0 to infinity, it allows us to have a probability distribution over the possible values that lambda could take (which cannot be negative). The distribution is specified by two parameters alpha and beta.\nWhere Gamma(alpha) is the Gamma function which generalises the factorial function ! to continuous numbers. It has the functional form:\nThat this Gamma distribution integrates to 1 can be seen by integrating it over x between 0 and infinity. That this Gamma distribution integrates to 1 can be seen by integrating it over x between 0 and infinity.\nwhich is the same as the Gamma function in the denominator, so the whole thing integrates to 1\nThe mean of the Gamma distribution is alpha/beta. Taking the expectation of the distribution:\nNormalising by the Gamma function in the denominator then cancels the Gamma function in the numerator giving E(x)=α/β\n4. Calculating the Posterior distribution analytically With this prior and likelihood, we can substitute into Bayes’ formula and directly calculate the probability of the lambda parameter given the data. In the denominator with the marginal likelihood, we integrate over all values of lambda (here using the variable u) to give the probability of the data conditional on all possible values of lambda. The Gamma prior is specified by two parameters alpha0 and beta0.\nWe will see that has Gamma prior distribution and a Poisson likelihood generates a posterior that is itself a Gamma distribution.\nNow we can see that the posterior also has a Gamma distribution just like the prior and updates the prior with new data while maintaining the same functional form. If we have no data i.e. xi=0 we get back the original prior:\nAs as we get more datapoints we just update this formula, keeping the same Gamma functional form. The original alpha parameter in the posterior is increased by the sum of the subsequent observed datapoints. The beta parameter is increased by the number of new datapoints i.e.\nWhen a prior combined with a given likelihood leads to a posterior distribution of the same functional form it is known as a conjugate prior. Another example of a conjugate prior is that a Normal prior and a Normal likelihood gives a posterior that is itself Normally distributed. For a Binomial likelihood and a Beta prior distribution the posterior will also have a Beta distribution.\nWe can see from the numerical example below that as we add more data the posterior moves away from the prior.\nimport numpy as np import math import matplotlib.pyplot as plot from scipy.special import gamma, factorial def posterior(lambda_val, alpha, beta, data): \"\"\"function that gives the values of the gamma posterior for different values of alpha, beta and data\"\"\" count_no = len(data) sum_val = sum(data) new_beta = count_no + beta new_alpha = alpha + sum_val numerator = ( ((new_beta) ** new_alpha) * np.exp(-lambda_val * (new_beta)) * (lambda_val ** (new_alpha - 1)) ) denominator = gamma(new_alpha - 1) val = numerator / denominator return val l_vals = np.linspace(0, 10, 100) chart_0 = posterior(l_vals, 2, 2, []) chart_1 = posterior(l_vals, 2, 2, list(np.random.poisson(6, 1))) chart_2 = posterior(l_vals, 2, 2, list(np.random.poisson(6, 5))) chart_3 = posterior(l_vals, 2, 2, list(np.random.poisson(6, 10))) fig, axs = plot.subplots(4, figsize=(12, 15)) fig.suptitle('How the posterior changes as we get more data') y_min = min(chart_0.min(), chart_1.min(), chart_2.min(), chart_3.min()) y_max = max(chart_0.max(), chart_1.max(), chart_2.max(), chart_3.max()) axs[0].plot(x_vals, chart_0) axs[0].set_title('Posterior with no data points i.e. Prior') #axs[0].set_ylim(y_min, y_max) axs[0].legend() axs[1].plot(x_vals, chart_1) axs[1].set_title('Posterior with 1 data point') #axs[1].set_ylim(y_min, y_max) axs[1].legend() axs[2].plot(x_vals, chart_2) axs[2].set_title('Posterior with 10 data points') #axs[2].set_ylim(y_min, y_max) axs[2].legend() axs[3].plot(x_vals, chart_3) axs[3].set_title('Posterior with 25 data points') #axs[3].set_ylim(y_min, y_max) axs[3].legend() References: Donovan, T. and Mickey, R. ‘Bayesian Statistics for Beginners’\nEfron, B. and Hastie, T., ‘Computer Age Statistical Inference’\nWilliams, D., ‘A course in probability and Statistics’\n",
  "wordCount" : "1350",
  "inLanguage": "en",
  "image":"https://johnardavies.github.io/ilkesahin.jpeg","datePublished": "2024-07-20T00:00:00Z",
  "dateModified": "2024-07-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "John Davies"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://johnardavies.github.io/technical/bayes1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "John's Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://johnardavies.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://johnardavies.github.io/" accesskey="h" title="John&#39;s Site (Alt + H)">John&#39;s Site</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://johnardavies.github.io/creative_industries" title="creative industries">
                    <span>creative industries</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/other_data_projects" title="other data projects">
                    <span>other data projects</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/talks/" title="talks">
                    <span>talks</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/technical/" title="technical posts">
                    <span>technical posts</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://johnardavies.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Bayes 1: Introduction to Bayesian inference
    </h1>
    <div class="post-meta">July 20, 2024&nbsp;·&nbsp;John Davies
</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://johnardavies.github.io/ilkesahin.jpeg" alt="">
        
</figure>
  <div class="post-content"><p><a href="https://ilkesahin.com/index.html">Ilke Sahin&rsquo;s</a> &lsquo;Water Canals (Aquaducts)&rsquo; at Goldsmiths degree show 2024</p>
<p>This note reviews some basic aspects of Bayesian inference to refresh my knowledge of it and some of the probability distributions it often uses.</p>
<h3 id="1-finding-the-most-probable-parameters-given-the-data-using-bayes-theorem">1. Finding the most probable parameters given the data using Bayes’ theorem<a hidden class="anchor" aria-hidden="true" href="#1-finding-the-most-probable-parameters-given-the-data-using-bayes-theorem">#</a></h3>
<p>We want to find the best hypothesis, or model parameter(s) that describes the data. Bayes’ theorem allows us to write down the formula for the
probability of model parameters or hypotheses given the data.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/bayes1.png" alt="Bayes 1"  />
</p>
<p>This is called the posterior distribution. It consists of a prior, a likelihood, and a marginal likelihood (sometimes called Evidence). The prior
gives the assumed probability distribution of model parameters/hypotheses before any data. This can reflect pre-existing beliefs and/or
logical/physical constraints. The likelihood gives the probability of the observed data given a set of parameter values/hypotheses. The marginal
likelihood is the probability of the data given all possible values the model parameters might take (or all possible hypotheses).</p>
<p>The starting point in a Bayesian approach is that there is assumed to be an uncertainty about the model parameters which we capture with a prior
distribution and then update the posterior as new data comes in. By contrast, in a frequentist approach, the starting assumption is that there are
assumed to be true underlying model parameters and the data generates uncertainty in our estimates. We capture this uncertainty in a frequentist
approach in the sampling distribution of the estimates, but the data these distributions are based on is assumed to be generated by fixed model
parameters.</p>
<p>If the parameter or hypothesis can only take discrete values e.g. does a photo (the data) contain person A (hypothesis 1) or does it not
(hypothesis 2). Then the Bayesian formula looks like the below, and the marginal likelihood is a sum over all possible states:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/bayes2.png" alt="Bayes 2"  />
</p>
<p>If the parameter can take an infinite number of values, for example a parameter specifying a distribution, then the denominator becomes an integral:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/bayes3.png" alt="Bayes 3"  />
</p>
<p>We often went to find the value of theta that maximises the posterior and so is most probable value given the data. The marginal likelihood does not
depend on theta as we integrate over all possible values of it. It therefore represents a normalisation constant which will not affect the optimal
value of theta. As a result a lot of Bayesian statistics is about evaluating the likelihood*prior, rather than the full posterior, to estimate the
optimal parameter. For a given model the marginal likelihood is fixed, but it can vary between models as they can have different numbers and types of
parameters and so different scope to explain the data. It therefore has a role in choosing between different kinds of models.</p>
<p>To make the posterior more concrete we now work through a simple example of calculating it based on a Poisson likelihood
and a Gamma prior distribution. These functional forms allow the posterior to be completely solved analytically. In many cases this is though not
possible and techniques like Markov Chain Monte Carlo (MCMC) have to be used to estimate the posterior.</p>
<h3 id="2-the-likelihood-the-poisson-distribution">2. The Likelihood: The Poisson distribution<a hidden class="anchor" aria-hidden="true" href="#2-the-likelihood-the-poisson-distribution">#</a></h3>
<p>The Poisson distribution gives the probability that a certain number of types of events x occur in an interval (such as a time period or number of
discrete events). Its probability distribution below has both a mean and variance of lambda.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/poisson1.png" alt="poisson 1"  />
</p>
<p>Summing it from x=0 to infinity and using the Taylor expansion it sums to 1 as a probability distribution should.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/poisson2.png" alt="poisson 2"  />
</p>
<p>Calculating the expected value:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/poisson3.png" alt="poisson 3"  />
</p>
<p>The Poisson is the limit of the Binomial distribution. This is the distribution of x outcomes from n trials where there are two
possible states e.g. how many heads from n coin tosses, when we have a large number of trials). Where p+q=1 and p=lambda*n the Binomial distribution tends to the Poisson as sample
sizes get large.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/poisson4.png" alt="poisson 4"  />
</p>
<p>This Poisson distribution gives the probability of x states as the number of trials gets large. Assuming the values of the observed data are
independent the Likelihood function is the product of the Poisson distributions for each observed value. The likelihood of the data then becomes:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/poisson5.png" alt="poisson 5"  />
</p>
<h3 id="3-the-prior-the-gamma-distribution">3. The Prior: The Gamma distribution<a hidden class="anchor" aria-hidden="true" href="#3-the-prior-the-gamma-distribution">#</a></h3>
<p>Having got an expression for the likelihood of lambda given the data we now examine a prior for the lambda. A standard prior for a variable with a
Poisson likelihood distribution is the Gamma distribution shown below. As the Gamma distribution is defined from 0 to infinity, it allows us to have a
probability distribution over the possible values that lambda could take (which cannot be negative). The distribution is specified by two parameters
alpha and beta.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/gamma1.png" alt="gamma 1"  />
</p>
<p>Where Gamma(alpha) is the Gamma function which generalises the factorial function ! to continuous numbers. It has the functional form:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/gamma2.png" alt="gamma 2"  />
</p>
<p>That this Gamma distribution integrates to 1 can be seen by integrating it over x between 0 and infinity. That this Gamma distribution integrates to 1
can be seen by integrating it over x between 0 and infinity.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/gamma3.png" alt="gamma 3"  />
</p>
<p>which is the same as the Gamma function in the denominator, so the whole thing integrates to 1</p>
<p>The mean of the Gamma distribution is alpha/beta. Taking the expectation of the distribution:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/gamma4.png" alt="gamma 4"  />
</p>
<p>Normalising by the Gamma function in the denominator then cancels the Gamma function in the numerator giving E(x)=α/β</p>
<h3 id="4-calculating-the-posterior-distribution-analytically">4. Calculating the Posterior distribution analytically<a hidden class="anchor" aria-hidden="true" href="#4-calculating-the-posterior-distribution-analytically">#</a></h3>
<p>With this prior and likelihood, we can substitute into Bayes’ formula and directly calculate the probability of the lambda parameter given the data.
In the denominator with the marginal likelihood, we integrate over all values of lambda (here using the variable u) to give the probability of the
data conditional on all possible values of lambda. The Gamma prior is specified by two parameters alpha0 and beta0.</p>
<p>We will see that has Gamma prior distribution and a Poisson likelihood generates a posterior that is itself a Gamma distribution.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/posterior1.png" alt="posterior 1"  />
</p>
<p>Now we can see that the posterior also has a Gamma distribution just like the prior and updates the prior with new data while maintaining the same
functional form. If we have no data i.e. xi=0 we get back the original prior:</p>
<p><img loading="lazy" src="https://johnardavies.github.io/posterior2.png" alt="posterior 2"  />
</p>
<p>As as we get more datapoints we just update this formula, keeping the same Gamma functional form. The original alpha parameter in the posterior is
increased by the sum of the subsequent observed datapoints. The beta parameter is increased by the number of new datapoints i.e.</p>
<p><img loading="lazy" src="https://johnardavies.github.io/posterior3.png" alt="posterior 3"  />
</p>
<p>When a prior combined with a given likelihood leads to a posterior distribution of the same functional form it is known as a conjugate prior. Another
example of a conjugate prior is that a Normal prior and a Normal likelihood gives a posterior that is itself Normally distributed. For a Binomial
likelihood and a Beta prior distribution the posterior will also have a Beta distribution.</p>
<p>We can see from the numerical example below that as we add more data the posterior moves away from the prior.</p>
<pre tabindex="0"><code>import numpy as np
import math
import matplotlib.pyplot as plot 
from scipy.special import gamma, factorial

def posterior(lambda_val, alpha, beta, data):
    &quot;&quot;&quot;function that gives the values of the gamma posterior for different values of alpha, beta and data&quot;&quot;&quot;

    count_no = len(data)
    sum_val = sum(data)
    new_beta = count_no + beta
    new_alpha = alpha + sum_val

    numerator = (
        ((new_beta) ** new_alpha)
        * np.exp(-lambda_val * (new_beta))
        * (lambda_val ** (new_alpha - 1))
    )
    denominator = gamma(new_alpha - 1)

    val = numerator / denominator

    return val

l_vals = np.linspace(0, 10, 100)

chart_0 = posterior(l_vals, 2, 2, [])
chart_1 = posterior(l_vals, 2, 2, list(np.random.poisson(6, 1)))
chart_2 = posterior(l_vals, 2, 2, list(np.random.poisson(6, 5)))
chart_3 = posterior(l_vals, 2, 2, list(np.random.poisson(6, 10)))

fig, axs = plot.subplots(4,  figsize=(12, 15))
fig.suptitle('How the posterior changes as we get more data')


y_min = min(chart_0.min(), chart_1.min(), chart_2.min(), chart_3.min())
y_max = max(chart_0.max(), chart_1.max(), chart_2.max(), chart_3.max())

axs[0].plot(x_vals, chart_0)
axs[0].set_title('Posterior with no data points i.e. Prior')
#axs[0].set_ylim(y_min, y_max)
axs[0].legend()

axs[1].plot(x_vals, chart_1)
axs[1].set_title('Posterior with 1 data point')
#axs[1].set_ylim(y_min, y_max)
axs[1].legend()

axs[2].plot(x_vals, chart_2)
axs[2].set_title('Posterior with 10 data points')
#axs[2].set_ylim(y_min, y_max)
axs[2].legend()

axs[3].plot(x_vals, chart_3)
axs[3].set_title('Posterior with 25 data points')
#axs[3].set_ylim(y_min, y_max)
axs[3].legend()
</code></pre><p><img loading="lazy" src="https://johnardavies.github.io/example_bayes_1.png" alt="example_bayes_1"  />
</p>
<h3 id="references">References:<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<p>Donovan, T. and Mickey, R. ‘Bayesian Statistics for Beginners’</p>
<p>Efron, B. and Hastie, T., ‘Computer Age Statistical Inference’</p>
<p>Williams, D., ‘A course in probability and Statistics’</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://johnardavies.github.io/tags/technical/">technical</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://johnardavies.github.io/">John&#39;s Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
